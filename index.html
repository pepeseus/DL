<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Visual Deception Detection for Financial Security: Homoglyph and Deepfake Identification</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    div.abstract {
      margin: 2em 2em 2em 2em;
      text-align: left;
      font-size: 85%;
    }
    div.abstract-title {
      font-weight: bold;
      text-align: center;
      padding: 0;
      margin-bottom: 0.5em;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Visual Deception Detection for Financial Security:
Homoglyph and Deepfake Identification</h1>
<div class="abstract">
<div class="abstract-title">Abstract</div>
<p>Visually-deceptive text, manifesting through homoglyph attacks, poses
a growing threat across cybersecurity, financial systems, and identity
verification. While existing string detection methods are easily
scalable, they fail to capture the nuanced perceptual similarities
between textual inputs. This paper introduces Visually-Aligned Text
Embeddings (VA-TE), a contrastive learning framework that uses
curriculum learning to train a lightweight projection head on top of a
pretrained vision-language model (VLM) text encoder, translating
semantic embeddings into representations aligned with human visual
perception. Crucially, this method operates directly on text, enabling
faster, lighter-weight detection than prior image-based approaches that
require rendering and training on millions of images. Evaluated on a
large-scale homoglyph benchmark, VA-TE attains competitive performance
(AUC = 0.95) using a SigLIP encoder backbone, approaching the
performance of state-of-the-art (SOTA) image-based methods while
offering substantial gains in scalability and deployment efficiency.
Further, fusing these visually-aligned embeddings with complementary
string-similarity features yields SOTA results (AUC = 0.98),
underscoring the value of multi-modal signals for robust spoof
detection. Taken together, VA-TE establishes a self-supervised procedure
for converting semantic text features into visually grounded
representations applicable across perceptual string-matching tasks.</p>
</div>
</header>
<p><span> lang = en-US, pdfversion = 1.7, pdfstandard = a-2b,
</span></p>
<h1 class="unnumbered" id="acknowledgments">Acknowledgments</h1>
<p>I would like to express my deepest gratitude to my thesis advisor,
Dr. Amar Gupta, for his unwavering guidance, encouragement, and support
throughout the course of this work. From the initial stages of
formulating ideas to the final steps of refining this thesis, Dr.
Gupta’s insight and mentorship were essential. His leadership has shaped
not only the quality of this research but also my development as a
researcher and thinker.</p>
<p>I would also like to thank the other members of our research team who
contributed to various aspects of this project. I am especially grateful
to Sean-Winston Luo, who worked closely with me on Section 1 of this
thesis (VA-TE) and whose collaboration, feedback, and persistence were
invaluable. I also extend my thanks to Zhurui Sheng and Ricardo
Carrillo, whose partnership was central to building and refining the
deepfake detection system, both technically and conceptually. Their
dedication and teamwork made the process both productive and rewarding.
I am similarly grateful to Dr. Rafael Palacios for his thoughtful input
throughout the past year. His guidance in drafting manuscripts and
communicating our findings across data and science contexts greatly
strengthened the clarity and impact of this work.</p>
<p>I would like to thank our sponsors at Itaú Unibanco—Miguel Dos Santos
Wanderley, Fernando Beserra, Jair Carvalho, and Lucas Orosco—for their
support, engagement, and collaboration. Their feedback and domain
expertise were key in shaping the direction and grounding of this
research.</p>
<p>Finally, I would like to thank my family for their steadfast support.
To my sister, Ana, whose companionship and encouragement during our time
living together in Boston sustained me through the challenges of
graduate school. Without her, I would not have been able to gain
admission to the program, let alone complete this thesis. I am also
deeply grateful to my parents, Christopher and Aleksandra, for inspiring
my curiosity, nurturing my love for learning, and serving as the guiding
light throughout my academic journey. Their belief in me has been my
greatest source of strength.</p>
<h1 id="introduction">Introduction</h1>
<h2 id="motivation">Motivation</h2>
<p>The proliferation of digital content has brought with it an
escalating challenge: visual deception. This threat manifests critically
in financial systems, where adversaries exploit visual similarities to
bypass security measures. Two problems exemplify this challenge. First,
in homoglyph attacks, adversaries create visually similar account names
to deceive users into trusting fraudulent accounts. These spoofed names
look nearly identical to legitimate companies, fooling human observers,
while evading detection systems that rely on text-based string matching
rather than visual appearance. Second, AI-generated synthetic media,
including deepfakes, threaten biometric verification and identity
authentication systems that rely on facial recognition. These deception
techniques span multiple areas and types of fraud, yet share a common
strategy: exploiting the gap between visual appearance and underlying
representation to deceive both humans and automated systems. Given the
scale and reach of these threats, robust and accurate detection systems
are essential to maintain the integrity of modern financial
infrastructure, identity verification platforms, and information
ecosystems that depend on content authenticity. Traditional detection
methods have approached these challenges by identifying specific
artifacts or patterns characteristic of deceptive content. In the text
domain, string matching algorithms rely on character-level metrics such
as edit distance and token overlap, treating homoglyphs as a text
problem when the deception operates visually. In the audio-video domain,
deepfake detectors learn to recognize generation artifacts left by
specific models. However, this artifact-driven paradigm suffers from a
critical limitation: adversaries continuously evolve their techniques,
rendering artifact-based detectors brittle. A detector trained to
recognize artifacts from one generative model often fails
catastrophically when confronted with outputs from newer, unseen models.
As generative capabilities advance, the cat-and-mouse game of artifact
detection becomes increasingly unsustainable. This thesis proposes a
paradigm shift: rather than chasing the ever-changing artifacts of
deceptive content, the approach instead learns the stable, intrinsic
properties that characterize authentic content. This reframing
transforms detection from a reactive pursuit of adversarial signatures
to a proactive learning of what genuine content looks like. By focusing
on the commonalities shared by real images or the natural alignment
between text semantics and visual appearance, the trained detectors are
able to generalize across deception methods.</p>
<h2 id="thesis-overview">Thesis Overview</h2>
<p>This thesis addresses visual deception for two distinct tasks,
developing specialized approaches for each. These two distinct tasks are
delineated in the following chapters.</p>
<h3 id="part-i-textual-visual-deception-chapter-2">Part I: Textual
Visual Deception (Chapter 2)</h3>
<p>Chapter 2 tackles homoglyph attacks, where adversaries substitute
visually similar characters to evade text-based spoof detection. A naive
visual solution might treat text as images, rendering strings and
applying computer vision models, but this proves impractical at scale
due to the computational overhead and memory requirements of image
processing for every account registration or transaction. Instead, the
approach introduces Visually-Aligned Text Embeddings (VA-TE), which
exploits a key insight: Vision-Language Models (VLMs) pretrained on
image-text pairs learn text encoders that implicitly capture visual
characteristics of text appearance. By fine-tuning the VLM text encoder
directly, VA-TE encodes visual properties from text itself, without
rendering images, enabling fast, memory-efficient inference at
deployment scale. Through contrastive learning with curriculum-based
hard negative mining, VA-TE learns to embed visually similar strings
close together in representation space, even when they are distant
according to character-based string metrics. Combined with complementary
string-based features in an ensemble, VA-TE achieves 98% average
precision, matching or exceeding vision-based methods while offering
substantial advantages in speed, memory footprint, and deployability for
real-world systems.</p>
<h3 id="part-ii-image-level-visual-deception-chapter-3">Part II:
Image-Level Visual Deception (Chapter 3)</h3>
<p>In an era of rapidly evolving generative models, existing detectors
overfit to artifacts specific to training-set generative models and fail
on unseen model families. A disentangled representation learning
framework is proposed that aims to separate authenticity-relevant
features from identity and content information within pretrained
audio-visual embeddings. Specifically, dual projection heads with
orthogonality constraints learn complementary subspaces: an authenticity
head trained via variance minimization to capture properties shared
across real samples, and an identity head trained via prototypical
contrastive learning to encode content-specific information. A temporal
transformer classifier then operates on the authenticity embeddings to
produce frame-level predictions. The approach is evaluated on
AVDeepfake1M++  <span class="citation" data-cites="cai2025av"></span>
and ShareVeo3  <span class="citation"
data-cites="wang2024vidprom"></span>, with out-of-distribution
generalization assessed on a novel test set of 150 videos (11,000+
segments) collected from OpenAI’s Sora 2. Preliminary results reveal
challenges with representation collapse during multi-objective
optimization, providing diagnostic insights into the difficulties of
disentangled learning for this task and motivating specific directions
for future work.<br />
While these two problems differ substantially in their technical
domains, from text embeddings to image forensics, they share a unifying
methodological philosophy: learning stable representations of authentic
content, enabling more robust detection systems.</p>
<h2 id="contributions">Contributions</h2>
<p>This thesis presents a unified framework for visual deception
detection, addressing two complementary challenges: homoglyph-based
textual fraud and AI-generated media. The primary contributions are as
follows:</p>
<ol>
<li><p><strong>Visually-Aligned Text Embeddings (VA-TE) for Homoglyph
Detection.</strong> A novel approach is introduced that leverages
vision-language model text encoders to capture visual properties of
Unicode characters without requiring explicit image rendering. This
method achieves competitive performance with traditional visual
approaches while offering superior scalability.         </p></li>
<li><p><strong>Disentangled Representation Learning for Deepfake
Detection.</strong> A dual-projection architecture with orthogonality
constraints is proposed to learn semantically meaningful and
generalizable features for distinguishing authentic from synthetic video
content. The approach separates authenticity-relevant features from
generator-specific artifacts, with the goal of improving
out-of-distribution generalization to unseen generation methods.       
 </p></li>
<li><p><strong>Novel Evaluation Dataset.</strong> A new
out-of-distribution test set is collected and annotated, comprising 150
videos (11,000+ segments) generated by OpenAI’s Sora 2, providing a
challenging benchmark for evaluating generalization to state-of-the-art
generation methods not contained in the training datasets.       
 </p></li>
<li><p><strong>Gradient-Balanced Multi-Objective Optimization.</strong>
Gradient normalization techniques <span class="citation"
data-cites="chen2018gradnorm"></span> are adapted to balance competing
objectives in representation learning, demonstrating effectiveness in
preventing loss collapse in disentangled learning scenarios.</p></li>
</ol>
<p>Together, these contributions advance the broader goal of detecting
visual deception across modalities, from character-level substitution
attacks to sophisticated AI-generated media-based deceptions.</p>
<h1
id="visually-aligned-text-embeddings-for-homoglyph-detection">Visually-Aligned
Text Embeddings for Homoglyph Detection</h1>
<h2 id="introduction-1">Introduction</h2>
<p>Research exploring visual similarity of text has remained
underexplored, despite vast applications within cybersecurity <span
class="citation" data-cites="10.1145/1124772.1124861"></span>, website
infrastructure <span class="citation"
data-cites="linari2009typo liu2016towards"></span>, bank check
processing <span class="citation"
data-cites="palacios2002automatic"></span>, and other growing digital
fields. Financial institutions need to deploy systems protecting against
scammers, who subtly manipulate legitimate company names to create
visually-similar “spoofs” that trick customers and merchants into
approving fraudulent transactions. Businesses centered around user
accounts must create software designed to prevent username
impersonations <span class="citation"
data-cites="10.1145/2815675.2815699"></span>, while financial technology
companies require systems to ensure precise digital check processing.
Each of these cases demands high accuracy, minimizing false negatives
that prevent malicious visually-similar text from scraping through,
while avoiding false positives that cause significant customer
inconvenience.</p>
<p>A common spoofing tactic is the use of <em>homoglyphs</em>,
characters from different writing systems that look identical but have
distinct Unicode code points. For instance, the Latin letter “a”
(U+0061) and the Cyrillic “a” (U+0430) appear the same to readers but
not to computers. Similarly, attackers may substitute the letter “O”
with the digit “0,” or replace “e” with accented forms such as “é.” Such
substitutions exploit human perception while bypassing naive string
matching, making homoglyph detection essential for spoof prevention.</p>
<p>Despite this problem’s relevance, existing text similarity research
focuses on semantic relationships <span class="citation"
data-cites="mikolov2013efficient Radford2021 devlin-etal-2019-bert electronics12143126"></span>
—aligning words and phrases based on meaning rather than visual
appearance. Methods for detecting visually-similar text have lagged
behind industry needs, with most companies using string matching
techniques to fulfill various mission-critical detection tasks. These
methods include string similarity metrics like Levenshtein distance
<span class="citation" data-cites="lcvenshtcin1966binary"></span>, which
define decision boundaries based on the number of edits required to
match two names. Fuzzy matching techniques are also used to assess
relative similarity, where names with high similarity scores are flagged
as potentially fraudulent. Typically, newly registered text, including
account registrations and usernames, is iteratively checked against a
reference dataset, with higher similarity indicating visually-similar
text.</p>
<p>String-based similarity detection methods face a major challenge:
they fail to capture the perceptual nuances of string modifications, as
these are intentionally designed to be visually subtle to evade
downstream detection. Methods in current use can detect minor textual
alterations, but often create false positives when string lengths become
smaller <span class="citation"
data-cites="kalyanathaya2019fuzzy"></span>. This necessitates the
development of more sophisticated methods that align with human-like
perception of visual string similarity, specifically a scalable and
deployable implementation capable of training accurate models without
using large quantities of image-based data. In practice, attackers blend
visual similarity with textual manipulation patterns. A robust detection
system must therefore capture both dimensions of deception—perceptual
similarity and structural variation—to defend against a spectrum of
spoofing attacks.</p>
<p>Visually-Aligned Text Embeddings (VA-TE) are proposed, a contrastive
learning <span class="citation" data-cites="Hadsell2006"></span>
framework that expands upon pre-trained vision-language models <span
class="citation" data-cites="Li2025"></span> capable of encoding the
visual characteristics of text. Through building on models that generate
semantically rich embeddings directly from text, this approach captures
subtle, visually deceptive name variations, achieving significantly
higher separability than string-based methods, improving ROC-AUC from
0.81 <span class="citation" data-cites="Woodbridge2018"></span> to 0.95,
while also offering deployment gains compared to existing machine
learning approaches <span class="citation"
data-cites="Woodbridge2018 R2020"></span>. To fulfill this task, a
lightweight projector <span class="citation"
data-cites="Chen2020"></span> is fine-tuned on top of these pre-trained
embeddings to enhance the system’s ability to distinguish between
legitimate names and visually-similar spoofs.</p>
<p>Contrastive learning <span class="citation"
data-cites="Hadsell2006"></span> is used, experimenting with various
contrastive objectives <span class="citation"
data-cites="Schroff2015 Oord2018 Khosla2020"></span> to optimize
similarity in the target embedding space. Additionally, curriculum
learning <span class="citation" data-cites="Bengio2009"></span> is
employed to progressively introduce more challenging negative examples
during training, enabling the model to learn nuanced decision boundaries
that improve sensitivity to string variations. To evaluate the
effectiveness of VA-TE, spoof detection is framed as the primary
evaluation task, measuring the quality of the learned embeddings through
their ability to distinguish between legitimate company names and
visually deceptive spoof counterparts.</p>
<p>The paper is structured as follows: Section <a
href="#sec:related-work" data-reference-type="ref"
data-reference="sec:related-work">2.2</a> reviews relevant literature,
spanning traditional string similarity methods, contrastive learning
techniques, and vision–language models. Section <a href="#sec:methods"
data-reference-type="ref" data-reference="sec:methods">2.3</a>
delineates the proposed method, including the architectural design and
the training objectives. Section <a href="#sec:experiments"
data-reference-type="ref" data-reference="sec:experiments">2.4</a>
describes the datasets, evaluation protocols, and baseline comparisons,
while Section <a href="#sec:results" data-reference-type="ref"
data-reference="sec:results">3.5</a> presents experimental results.
Section <a href="#sec:vate-future" data-reference-type="ref"
data-reference="sec:vate-future">2.6.3</a> discusses avenues for future
work, and Section <a href="#sec:discussion" data-reference-type="ref"
data-reference="sec:discussion">2.6</a> concludes with a summary of the
findings.</p>
<h2 id="sec:related-work">Related Works</h2>
<h3 id="string-matching-methods">String Matching Methods</h3>
<p>Despite challenges with accuracy and implementation <span
class="citation" data-cites="kalyanathaya2019fuzzy"></span>,
string-based similarity methods act as fast, computationally efficient
proxies for evaluating visual similarity between strings, demonstrating
value as components in similarity detection systems <span
class="citation" data-cites="Damerau1964"></span>. These methods fall
into two main categories: edit distance metrics and fuzzy matching
methods, each capturing different aspects of textual manipulation
patterns commonly exploited in spoofing attacks. While individually
limited, their distinct strengths and failure modes make them
well-suited for ensemble-based approaches.</p>
<h4 id="levenshtein-distance-metrics">Levenshtein Distance Metrics</h4>
<p>Levenshtein distance <span class="citation"
data-cites="lcvenshtcin1966binary"></span> measures the minimum number
of single-character edits (insertions, deletions, and substitutions)
required to transform one string into another. This metric excels at
detecting minor manipulations in longer strings. However, the edit
distance frequently exhibits systematic failures that limit its
standalone effectiveness. For short strings, legitimate variations and
unrelated names can trigger false positives due to the metric’s
insensitivity to string length. A single character difference between
genuinely distinct entities can produce a low edit distance that
incorrectly suggests spoofing. Conversely, sophisticated spoofing
attacks on longer strings can involve many character edits while
maintaining visual similarity, resulting in high edit distances that
evade detection. This can be seen in Table <a
href="#tab:edit_distance_failure" data-reference-type="ref"
data-reference="tab:edit_distance_failure">[tab:edit_distance_failure]</a>.</p>
<p>Despite these limitations, Levenshtein distance achieves reasonable
performance as a standalone classifier, with reported ROC-AUC of 0.81
(see Section <a href="#sec: evaluation-metrics"
data-reference-type="ref"
data-reference="sec: evaluation-metrics">2.4.6</a> for metric selection
rationale) on domain name spoofing tasks <span class="citation"
data-cites="Woodbridge2018"></span>.</p>
<h4 id="fuzzy-matching-methods">Fuzzy Matching Methods</h4>
<p>Percentage-based similarity metrics, including Token Sort Ratio,
Partial Ratio, and Token Set Ratio<a href="#fn1" class="footnote-ref"
id="fnref1" role="doc-noteref"><sup>1</sup></a>, calculate similarity
based on token overlap and string length normalization. These methods
address edit distance’s length sensitivity by producing normalized
similarity scores, making them particularly effective for longer strings
where proportional similarity is more meaningful than absolute character
differences. </p>
<p>While commonly used by financial institutions, fuzzy matching methods
exhibit failure patterns that limit their standalone effectiveness. When
strings share common words or substantial token overlap, percentage
metrics yield high similarity scores that create false positives for
genuinely distinct entities. Conversely, well-designed spoofs that
appear visually similar but are textually dissimilar, particularly in
short strings, often evade detection entirely, resulting in false
negatives. Examples are shown below in Table <a
href="#tab:token_set_ratio_failure" data-reference-type="ref"
data-reference="tab:token_set_ratio_failure">[tab:token_set_ratio_failure]</a>.</p>
<p>Fuzzy matching achieves superior standalone performance compared to
editing distance, with a reported ROC-AUC of 0.86 on domain spoofing
tasks <span class="citation" data-cites="Woodbridge2018"></span>,
reflecting its improved handling of string length variations.</p>
<h4 id="complementary-strengths-and-ensemble-potential">Complementary
Strengths and Ensemble Potential</h4>
<p>An analysis of failure conditions reveals that edit distance and
fuzzy matching offer complementary strengths for spoof detection.
Levenshtein distance performs well on short strings with character-level
manipulations, where fuzzy matching often fails. Conversely, for longer
strings with proportionally similar tokens, fuzzy matching excels while
edit distance struggles. These patterns suggest that combining both
metrics in an ensemble approach could capture a wider range of spoofing
behaviors than either method alone.</p>
<p>Despite these strengths, both techniques fundamentally fail to
account for the perceptual subtleties of visual similarity, an essential
aspect of effective spoofing as illustrated by the misclassifications in
Tables  <a href="#tab:edit_distance_failure" data-reference-type="ref"
data-reference="tab:edit_distance_failure">[tab:edit_distance_failure]</a>
and  <a href="#tab:token_set_ratio_failure" data-reference-type="ref"
data-reference="tab:token_set_ratio_failure">[tab:token_set_ratio_failure]</a>.
This limitation motivates the development of more advanced methods that
embed visual characteristics into their representations to improve
similarity analysis and spoof detection.</p>
<h3 id="a-visual-non-ml-approach-hitzone-mapping">A Visual Non-ML
Approach: HitZone Mapping</h3>
<p>HitZone Mapping <span class="citation" data-cites="8367634"></span>
assesses visual similarity by dividing characters into grids and
measuring the degree of overlap between corresponding squares. Each
character is mapped onto a grid with varying degrees of granularity,
allowing for detection of minor visual differences and providing an
interpretable framework for spoof detection. While HitZone maps have
been successfully applied at the character level, they have not been
widely explored in the context of word-level similarity detection.</p>
<p>When comparing different length strings, words must be carefully
aligned so hitboxes match-up on a character-to-character basis. This
task would require a dynamic programming approach that has not yet been
formally developed.</p>
<h3 id="embeddings-for-similarity-measurement">Embeddings for Similarity
Measurement</h3>
<p>Unlike the previously mentioned methods, embeddings provide an
efficient and continuous framework for similarity assessment, most
commonly cosine similarity <span class="citation"
data-cites="mikolov2013efficient bengio2003neural"></span>. Learning a
task-specific embedding space enables effective thresholding and
ranking, both of which are essential for real-time retrieval and
large-scale systems <span class="citation"
data-cites="gordo2017end"></span>.</p>
<p>Accordingly, embeddings have been used for similarity assessment
across many different tasks. Within computer vision, they power systems
for facial recognition <span class="citation"
data-cites="Schroff2015"></span> and image classification <span
class="citation" data-cites="he2016deep"></span>, both of which require
precise ranking of visually-similar features and items. Extending even
to symbols, embeddings have enabled tasks like sign language recognition
<span class="citation" data-cites="9054076"></span> and handwritten text
retrieval <span class="citation"
data-cites="rao2016optical 7814078"></span>, where visual patterns
rather than strict symbolic sequences define similarity. In text-based
tasks, embeddings have achieved widespread usage in word semantic
settings that measure meaning-based closeness <span class="citation"
data-cites="mikolov2013efficient devlin-etal-2019-bert"></span>. The
overwhelming literature outlining the success of embeddings in
similarity analysis <span class="citation" data-cites="9933854"></span>
motivates the usage of embeddings for visual text similarity
analysis.</p>
<h3 id="contrastive-learning">Contrastive Learning</h3>
<p>Contrastive learning <span class="citation"
data-cites="Hadsell2006"></span> has been effectively used to learn
task-specific embeddings requiring fine-grained distinction<span
class="citation" data-cites="jaiswal2020survey"></span>, including face
forgery detection<span class="citation" data-cites="Sun2022"></span>,
text-image alignment<span class="citation"
data-cites="Radford2021"></span>, and self-supervised representation
learning<span class="citation" data-cites="Chen2020"></span>.</p>
<p>In particular, SimCLR <span class="citation"
data-cites="Chen2020"></span> presents a simple yet powerful framework
for contrastive learning of visual representations, using data
augmentation to generate positive pairs, a neural network encoder to
compute embeddings, and a projection head to map these embeddings into a
space where contrastive loss is applied. This framework motivates the
methodological approach presented in this thesis.</p>
<p>Additionally, prior work has applied contrastive learning frameworks,
specifically Siamese convolutional neural networks <span
class="citation" data-cites="Bromley1993"></span>, to the problem of
homoglyph detection by learning visually-aligned representations of text
strings <span class="citation"
data-cites="Woodbridge2018 R2020"></span>. These studies demonstrate the
ability of contrastive loss for distinguishing visually similar strings,
underscoring the relevance of contrastive learning for spoof detection
tasks.</p>
<p>In contrast to classification-based training, which learns embeddings
incidentally while optimizing for label prediction <span
class="citation" data-cites="9025448"></span>, contrastive learning
explicitly shapes the embedding space to align with meaningful notions
of similarity by bringing similar (positive) samples closer and pushing
dissimilar (negative) samples apart <span class="citation"
data-cites="Hadsell2006 hermans2017defense"></span>. The goal is to
learn a feature space in which similar inputs are mapped to nearby
points, while dissimilar inputs are separated by a significant margin,
using distance metrics like cosine distance. </p>
<p>In the literature, a range of loss functions have been developed to
implement this learning objective:</p>
<ul>
<li><p><strong>Contrastive Loss:</strong> <span class="citation"
data-cites="Hadsell2006"></span> This formulation penalizes positive
pairs that are far apart and negative pairs that are too close,
typically including a margin hyperparameter to encourage strong
separation without collapsing the embedding space. <span
class="math display">\[\begin{equation}
    \mathcal{L}_{\mathrm{contrastive}}
    = y\,\|z_i-z_j\|^{2}
    + (1-y)\,[\,m-\|z_i-z_j\|\,]_+^{2}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(y = 1\)</span> for positive pairs
and <span class="math inline">\(y = 0\)</span> for negative pairs, and
<span class="math inline">\(m\)</span> is the margin.</p></li>
<li><p><strong>Triplet Loss:</strong> <span class="citation"
data-cites="Schroff2015"></span> Extends the contrastive formulation to
anchor-positive-negative triplets, ensuring the anchor is closer to the
positive than the negative by a margin. <span
class="math display">\[\begin{equation}
    \mathcal{L}_{\text{triplet}} = \max\left(0, \| \mathbf{z}_a -
\mathbf{z}_p \|^2 - \| \mathbf{z}_a - \mathbf{z}_n \|^2 + \alpha \right)
\end{equation}\]</span> where <span
class="math inline">\(\mathbf{z}_a\)</span>, <span
class="math inline">\(\mathbf{z}_p\)</span>, and <span
class="math inline">\(\mathbf{z}_n\)</span> are the anchor, positive,
and negative embeddings, and <span class="math inline">\(\alpha\)</span>
is the margin.</p></li>
<li><p><strong>InfoNCE:</strong> <span class="citation"
data-cites="Oord2018"></span> Commonly used in self-supervised learning
frameworks, InfoNCE generalizes the contrastive objective to a
multi-negative classification task, maximizing the similarity of the
anchor-positive pair relative to a set of negatives and encouraging
discriminative representation learning in large batches. <span
class="math display">\[\begin{equation}
    \mathcal{L}_{\text{InfoNCE}} = -\log \left( \frac{\exp(\mathbf{z}_i
\cdot \mathbf{z}_p / \tau)}{\sum_{a \in A(i)} \exp(\mathbf{z}_i \cdot
\mathbf{z}_a / \tau)} \right)
\end{equation}\]</span> where <span class="math inline">\(\tau\)</span>
is the temperature and <span class="math inline">\(A(i)\)</span>
includes one positive and multiple negatives.</p></li>
<li><p><strong>Supervised Contrastive Loss (SupCon):</strong> <span
class="citation" data-cites="Khosla2020"></span> Extends InfoNCE to
leverage batches of positive samples. This is particularly beneficial in
spoof detection where multiple spoof variants may exist for a single
legitimate entity. <span class="math display">\[\begin{equation}
    \mathcal{L}_{\mathrm{sup}}
    = -\sum_{i\in I}\frac{1}{|P(i)|}\sum_{p\in P(i)}
    \log\!\tfrac{\exp(z_i^{\top} z_p/\tau)}
       {\sum_{a\in A(i)} \exp(z_i^{\top} z_a/\tau)}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(P(i)\)</span> is the set of
positives for anchor <span class="math inline">\(i\)</span>, excluding
itself.</p></li>
</ul>
<p>A critical component of effective contrastive learning is the use of
hard negatives <span class="citation"
data-cites="Robinson2021 hermans2017defense"></span> - negative samples
that are close to positives in the initial embedding space before
applying contrastive learning. By challenging the model to distinguish
between such near-positives, hard negatives refine decision boundaries
and encourage the learning of robust, discriminative representations.
This consideration naturally motivates the use of curriculum learning
strategies, described in the following section.</p>
<h3 id="curriculum-learning">Curriculum Learning</h3>
<p>Curriculum learning <span class="citation"
data-cites="Bengio2009"></span> is a training paradigm that structures
the learning process by gradually increasing the difficulty of training
examples over time, mimicking the way humans learn progressively from
simpler to more complex tasks. Originally introduced in natural language
processing and computer vision tasks, curriculum learning has since been
applied to deep metric learning <span class="citation"
data-cites="huang2020curricularface"></span> and contrastive
representation learning <span class="citation"
data-cites="chu2021cuco"></span>, where it has been shown to improve
convergence, generalization, and robustness of the learned embedding
spaces.</p>
<p>In contrastive learning specifically, where models are trained to
bring positive pairs closer and push negative pairs apart in the
embedding space, the selection and ordering of training examples,
particularly negative samples, plays a critical role in shaping model
performance. A naïve strategy that samples negatives uniformly at random
may lead to uninformative or overly trivial contrastive signals <span
class="citation" data-cites="Wang2022"></span>, which in turn limit the
model’s ability to develop nuanced decision boundaries. This is
especially problematic in tasks like homoglyph detection, where many
adversarial examples are intentionally designed to be visually
deceptive, requiring the model to capture subtle typographic cues.</p>
<p>To address this, recent studies in curriculum-based contrastive
learning propose incorporating progressively harder negative samples
over the course of training <span class="citation"
data-cites="Robinson2021"></span>. This structured training helps the
model refine its embedding space to capture more fine distinctions. This
concept is closely related to hard negative mining <span
class="citation" data-cites="Schroff2015 hermans2017defense"></span>, a
well-established technique in deep metric learning, where negatives that
are difficult to distinguish from positives are prioritized to
accelerate learning and improve discriminative capacity.</p>
<p>Several curriculum learning strategies have been explored in prior
work:</p>
<ul>
<li><p><strong>Curriculum Learning</strong> <span class="citation"
data-cites="Bengio2009"></span> introduces samples incrementally based
on a difficulty metric (e.g., loss, similarity score), allowing the
model to focus first on easier instances and later adapt to harder
ones.</p></li>
<li><p><strong>Self-Paced Curriculum Learning (SPL)</strong> <span
class="citation" data-cites="Jiang2015"></span> ranks training samples
based on their contribution to model loss, ensuring continued focus on
examples where the model struggles most.</p></li>
<li><p><strong>Bandit-Based Adaptive Sampling</strong> <span
class="citation" data-cites="Graves2017"></span> applies multi-armed
bandit frameworks to dynamically allocate sampling effort across
difficulty tiers, selecting the most informative samples based on
learning progress.</p></li>
</ul>
<p>These strategies have demonstrated improvements across various
domains, including image retrieval <span class="citation"
data-cites="gong2016multi"></span>, face verification <span
class="citation" data-cites="huang2020curricularface"></span>, and
few-shot classification <span class="citation"
data-cites="wei-etal-2021-shot"></span>. However, their use in
vision-language pretraining and visually-aligned text representation
learning remains unexplored. In this work, the approach builds on these
principles and extends curriculum learning into the realm of textual
visual-similarity analysis by integrating it with contrastive learning
over pre-trained embeddings.</p>
<p>By leveraging curriculum learning to shape the training trajectory of
hard negative examples, a pretrained text encoder—originally optimized
for semantic alignment—is repurposed to instead encode visual similarity
between text strings.</p>
<h3 id="vision-language-models">Vision-Language Models</h3>
<p>Vision-Language Models (VLMs) <span class="citation"
data-cites="Li2025"></span> have recently emerged as powerful tools for
learning joint representations of images and text, finding applicability
for image-text matching, image-to-text generation, and zero-shot
classification <span class="citation" data-cites="Zhang2024"></span>.
Trained on large datasets, typically of image-caption pairs <span
class="citation" data-cites="Radford2021"></span>, these models align
semantic information in a shared embedding space, making them effective
at recognizing meaning across both modalities. While most VLMs are
trained for semantic alignment, prior work has shown that visual
features of text can emerge in their internal representations <span
class="citation" data-cites="goh2021multimodal"></span>, and that
semantic similarity in text often correlates with visual similarity
<span class="citation"
data-cites="BrustClemens-AlexanderandDenzler2019"></span>. This
motivates the investigation into whether such models, when enhanced with
projection layers <span class="citation" data-cites="Chen2020"></span>,
can be effectively adapted for VA-TE. The following examples illustrate
cases where semantic similarity aligns with visual similarity (Table <a
href="#tab:strong_visual_semantic" data-reference-type="ref"
data-reference="tab:strong_visual_semantic">[tab:strong_visual_semantic]</a>).</p>
<p>Alongside visual-semantic correlation, many VLMs are trained on
images that contain naturally occurring text <span class="citation"
data-cites="Radford2021"></span>, like street signs, logos, and item
labels, that are paired with text that describe them. This indirect
exposure to textual appearance in visual contexts allows the model to
learn association between word meaning and visual appearance <span
class="citation" data-cites="goh2021multimodal"></span>. These two
factors motivate the usage of VLMs for detecting <em>perceptual</em>
similarity of text.</p>
<p>To determine the model-specific approach, VLMs are categorized into
four architectural types: contrastive-based, VLMs with masking
objectives, generative-based, and VLMs from pretrained backbones<span
class="citation" data-cites="Bordes2024"></span>.</p>
<ul>
<li><p><strong>Contrastive-based VLMs</strong> (CLIP  <span
class="citation" data-cites="Radford2021"></span>, OpenCLIP  <span
class="citation" data-cites="Cherti2023"></span>, SigLip  <span
class="citation" data-cites="Zhai2023"></span>, ALIGN  <span
class="citation" data-cites="jia2021scaling"></span>) use separate
encoders for image and text modalities, aligning their outputs through a
contrastive loss function.</p></li>
<li><p><strong>VLMs with masking objectives</strong> (FLAVA  <span
class="citation" data-cites="singh2022flava"></span>) process image and
text together through a single transformer with early fusion and
cross-modal attention, making them especially effective for tasks that
require understanding detailed interactions between different
modalities.</p></li>
<li><p><strong>Generative-based VLMs</strong> (BLIP  <span
class="citation" data-cites="li2022blip"></span>, BLIP-2  <span
class="citation" data-cites="li2023blip"></span>, CoCa  <span
class="citation" data-cites="Yu2022"></span>) combine a vision encoder
with a generative text decoder. CoCa, in particular, fuses together
dual-encoder retrieval with captioning capabilities.</p></li>
<li><p><strong>VLMs from Pretrained Backbones</strong> (LLaVA  <span
class="citation" data-cites="liu2023visual"></span> and GPT-4V  <span
class="citation" data-cites="openai2023gpt4v"></span>) integrate visual
inputs through adaptors or embedded vision tokens into a pre-trained
LLM. They are typically more suited for semantic understanding
tasks.</p></li>
</ul>
<p>For identifying visually-similar text, VLMs capable of encoding text
directly are most desirable. Leveraging text encoders avoids the usage
of images for visual embeddings, reducing memory overhead, training
complexity, and improving system-wide deployment. A baseline test is
conducted to confirm the efficacy of contrastive-based models and select
a pretrained VLM.</p>
<h2 id="sec:methods">Methods</h2>
<h3 id="model-architecture-and-learning-objectives">Model Architecture
and Learning Objectives</h3>
<p>The proposed framework, <strong>VA-TE</strong>, builds upon the power
of pretrained VLM embeddings to encode visual characteristics of text
<span class="citation" data-cites="goh2021multimodal"></span>, rather
than just semantic meaning. However, using a pretrained VLM without
additional fine-tuning is insufficient, as the model faces problems when
semantic-visual correlation fails. See Table <a
href="#tab:weak_visual_semantic" data-reference-type="ref"
data-reference="tab:weak_visual_semantic">[tab:weak_visual_semantic]</a>
for reference.</p>
<p>To refine the input embedding space, training is performed using four
contrastive objectives: pairwise loss, triplet loss, InfoNCE, and
Supervised Contrastive Loss (SupCon), each implemented based on original
formulations in the literature. The only notable deviation is the use of
cosine similarity in the triplet loss instead of the traditional
Euclidean distance—a change that yields equivalent results in practice
due to prior embedding normalization.</p>
<p>Supporting these contrastive objectives, VA-TE follows a simple yet
effective architecture (Fig. <a href="#fig:vate_architecture"
data-reference-type="ref"
data-reference="fig:vate_architecture">2.1</a>). Each input string is
first passed through a frozen text encoder from a pretrained
vision-language model, which produces a high-dimensional semantic
embedding. These embeddings are then fed into a lightweight projection
head, consisting of two shared linear layers with ReLU activation,
designed to map semantic representations into a space that better
reflects visual similarity. During training, the architecture operates
as a Siamese network, receiving inputs and optimizing distances based on
label relationships, essentially pulling positive pairs closer together,
while negative pairs are pushed apart. A prediction is made by computing
the cosine similarity score between 2 input names, and classifying as a
spoof if the score exceeds a predefined threshold.</p>
<figure id="fig:vate_architecture" data-latex-placement="t">
<img src="VATE/jovan1.png" />
<figcaption>Overview of the VA-TE training architecture with light
linear projector head.</figcaption>
</figure>
<h3 id="curriculum-learning-1">Curriculum Learning</h3>
<p>To strengthen the discriminative capability of the model, three
curriculum learning strategies are employed. Datasets of three
difficulties: easy, medium, and hard, are used, based on the
separability of positives and negatives in the input (semantic)
embedding space.</p>
<ul>
<li><p><strong>Manual Approach:</strong> a fixed-schedule curriculum
where the training process is divided into three equal stages by epoch,
each stage increasing in example difficulty. This structured progression
allows the model to first reinforce easily separable relationships in
the input embedding space, before being challenged with hard negatives
that closely resemble positives—forcing the model to learn fine-grained
distinctions between visually-similar text.</p></li>
<li><p><strong>Automated Approach:</strong> instead of set stages, each
epoch uses a training set with a mix of easy, medium, and hard samples.
The ratio of samples from each difficulty level is determined by a
scheduling function which dynamically adjusts these ratios as training
progresses. The curriculum starts with a high proportion of easy samples
and gradually shifts toward more hard negatives. This method follows a
predefined curriculum, but provides smoother transitions than the manual
approach, enabling more stable learning and potentially more effective
adaptations to challenging examples. This scheduling strategy is defined
by (<a href="#eq:cos_schedule" data-reference-type="ref"
data-reference="eq:cos_schedule">[eq:cos_schedule]</a>) and (<a
href="#eq:difficulty_weights" data-reference-type="ref"
data-reference="eq:difficulty_weights">[eq:difficulty_weights]</a>) and
illustrated in Fig. <a href="#fig:automated_curriculum"
data-reference-type="ref"
data-reference="fig:automated_curriculum">2.2</a>.</p>
<p><span class="math display">\[\begin{equation}
        t = \frac{\text{current epoch}}{\text{total \# of epochs}},
\quad c(t) = \cos(\pi t)
        \label{eq:cos_schedule}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
        \begin{bmatrix}
            \mathrm{easy}(t)\\[2pt]
            \mathrm{medium}(t)\\[2pt]
            \mathrm{hard}(t)
        \end{bmatrix}
        =
        \begin{bmatrix}
            \frac{1}{2}(1 + c(t))(1 - t)\\[4pt]
            \frac{1}{2}\left[1 - (1 - 2t)c(t)\right]\\[4pt]
            \frac{1}{2}(1 - c(t))t
        \end{bmatrix}
        \label{eq:difficulty_weights}
\end{equation}\]</span></p>
<figure id="fig:automated_curriculum" data-latex-placement="h">
<img src="VATE/jovan2.png" />
<figcaption>Automated curriculum scheduling function. Easy samples
dominate early epochs, medium samples peak midway, and hard samples
increase toward training end</figcaption>
</figure></li>
<li><p><strong>Adaptive Bandit-Based Approach:</strong> frames
curriculum learning as a multi-armed bandit problem, where each
difficulty level represents an arm of the bandit. At each epoch, the
trainer balances exploration and exploitation by following an
epsilon-greedy policy, selecting which dataset to use either at random
or by estimated reduction in loss. This adaptive strategy allows the
training process to dynamically focus on the most beneficial dataset,
rather than adhering to a predefined schedule.</p></li>
</ul>
<h3 id="ensemble-with-textual-features">Ensemble with Textual
Features</h3>
<p>Hard negatives pose a major challenge in spoof detection because they
appear close to genuine company names in the VLM’s input (semantic)
embedding space. Although contrastive objectives aim to separate these
examples, hard negatives often remain near the anchor, especially when
they share structural traits the model deems semantically relevant.
Consequently, non-spoof names are frequently misclassified, hard
negatives exhibit high cosine similarity despite differing in
string-based metrics (Table  <a href="#tab:metric_profiles"
data-reference-type="ref"
data-reference="tab:metric_profiles">[tab:metric_profiles]</a>),
exposing a key limitation of utilizing embedding-based methods alone:
their reliance on transforming semantic similarity into visual-alignment
can limit homoglyph detection accuracy.</p>
<p>Table <a href="#tab:weak_visual_semantic" data-reference-type="ref"
data-reference="tab:weak_visual_semantic">[tab:weak_visual_semantic]</a>
presents two representative examples where VLMs struggle due to
conflicting visual and semantic cues. The first example, <em>Google</em>
vs. <em>Goggle</em>, illustrates a <strong>hard positive</strong>.
Although the two names are visually similar, they lie far apart in
semantic space. As a result, even with fine-tuning, the VLM may fail to
bring them sufficiently close in the embedding space to correctly
classify the pair as a spoof, resulting in a false negative. In
contrast, the second example, <em>Doggy</em> vs. <em>Puppy</em>,
represents a <strong>hard negative</strong>. Here, the names are
semantically similar but visually distinct. Because they are already
close in semantic space, the model may fail to separate them adequately,
leading to a false positive—incorrectly identifying a non-spoof as a
spoof. These cases underscore the difficulty of spoof detection when
visual and semantic signals diverge, motivating the addition of
non-embedding string methods that integrate strictly textual cues. This
divergence becomes especially apparent in Table <a
href="#tab:metric_profiles" data-reference-type="ref"
data-reference="tab:metric_profiles">[tab:metric_profiles]</a>.</p>
<p><span id="tab:metric_profiles"
data-label="tab:metric_profiles"></span></p>
<div id="tab:pair_dataset_summary">
<table>
<caption>Average Levenshtein distance and token set ratio across spoof
difficulty levels. Spoofs show lower distances and higher
ratios.</caption>
<thead>
<tr>
<th style="text-align: left;"><strong>Dataset</strong></th>
<th style="text-align: center;"><strong>Spoof</strong></th>
<th style="text-align: center;"><strong>Levenshtein
Distance</strong></th>
<th style="text-align: center;"><strong>Token Set Ratio</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Easy</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">9.57</td>
<td style="text-align: center;">21.15</td>
</tr>
<tr>
<td style="text-align: left;">Easy</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">1.77</td>
<td style="text-align: center;">82.06</td>
</tr>
<tr>
<td style="text-align: left;">Medium</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">3.54</td>
<td style="text-align: center;">68.12</td>
</tr>
<tr>
<td style="text-align: left;">Medium</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">2.13</td>
<td style="text-align: center;">77.36</td>
</tr>
<tr>
<td style="text-align: left;">Hard</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">6.18</td>
<td style="text-align: center;">48.57</td>
</tr>
<tr>
<td style="text-align: left;">Hard</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">1.78</td>
<td style="text-align: center;">82.07</td>
</tr>
</tbody>
</table>
</div>
<p><span id="tab:pair_dataset_summary"
data-label="tab:pair_dataset_summary"></span></p>
<p>Table <a href="#tab:pair_dataset_summary" data-reference-type="ref"
data-reference="tab:pair_dataset_summary">2.1</a> shows that spoof and
non-spoof pairs differ consistently across all difficulty levels: spoofs
have lower Levenshtein distances and higher token set ratios. This
distinction is especially pronounced in the hard subset, where embedding
models struggle. The spoofs average 1.78 in Levenshtein distance and
82.07 in token set ratio, while non-spoofs average 6.18 and 48.57,
respectively. These results suggest that string-based metrics remain
effective at separating spoof types that evade visual-semantic
detection. Thus, to address the limitations of embedding models on
visually deceptive cases, an ensemble approach is adopted that
integrates string similarity with embedding-based representations,
leveraging complementary strengths to improve robustness.</p>
<p>Given a pair of strings <span class="math inline">\((x_1,
x_2)\)</span>, the following are computed: (i) <span
class="math inline">\(s_{\text{VA-TE}}=\cos(\hat{z}(x_1),\hat{z}(x_2))\)</span>
where <span class="math inline">\(\hat{z}(\cdot)\)</span> are
L2-normalized VA-TE embeddings; (ii) <span
class="math inline">\(r_{\text{TSR}}\in[0,1]\)</span>, the Token Set
Ratio (normalized to <span class="math inline">\([0,1]\)</span>)
capturing token overlap similarity; and (iii) <span
class="math inline">\(d_{\text{Lev}}\in\mathbb{N}\)</span>, the
Levenshtein edit distance.<a href="#fn2" class="footnote-ref"
id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>A Gradient Boosting Classifier <span
class="math inline">\(g(\cdot)\)</span> is trained on the feature vector
<span class="math inline">\(\phi(x_1,x_2)=[s_{\text{VA-TE}},\;
r_{\text{TSR}},\; d_{\text{Lev}}]\)</span> to predict <span
class="math inline">\(p(\text{spoof}\mid
x_1,x_2)=g(\phi(x_1,x_2))\)</span>. Unless noted otherwise, <span
class="math inline">\(n_{\text{estimators}} = 100\)</span>, <span
class="math inline">\(\text{max\_depth} = 6\)</span>, <span
class="math inline">\(\text{learning\_rate} = 0.1\)</span>, <span
class="math inline">\(\text{random\_state} = 42\)</span> are used.
Features are left unscaled (tree-based models are scale-invariant).
<span class="math inline">\(g\)</span> is fit on the training split
only, the operating threshold <span class="math inline">\(\tau\)</span>
is selected on the validation split via Youden’s <span
class="math inline">\(J\)</span>, and metrics are reported on the
held-out test set, ensuring data leakage does not occur.</p>
<p>Through this approach, <span
class="math inline">\(s_{\text{VA-TE}}\)</span> encodes perceptual
similarity learned from contrastive fine-tuning, while <span
class="math inline">\(r_{\text{TSR}}\)</span> and <span
class="math inline">\(d_{\text{Lev}}\)</span> are complementary,
length-aware and edit-aware textual cues. A learned fusion lets the
model weight each cue differently across regimes (string length, token
overlap, character-level attacks), yielding a more robust homoglyph
detector.</p>
<h3 id="parameter-tuning-and-implementation-details">Parameter Tuning
and Implementation Details</h3>
<p>To effectively train the VA-TE model, several hyperparameters that
influence overall model performance and convergence stability are
fine-tuned. These include learning rate, batch size, and internal layer
size (for linear projection layers), as well as contrastive
loss-specific temperature or margin parameters:</p>
<ul>
<li><p><strong>Margin</strong>, used specifically for pairwise and
triplet losses, defines the minimum required separation between negative
and positive samples. This way, the model learns separation between
difficult data points.</p></li>
<li><p><strong>Temperature</strong>, relevant only for InfoNCE and
SupCon loss, scales similarity scores before softmax normalization.
Lower temperatures produce sharper distributions with high contrast
between positives and negatives, while higher temperatures produce less
separation between contrasting data.</p></li>
</ul>
<p>To determine optimal hyperparameters, <strong>Optuna</strong> <a
href="#fn3" class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a>, an optimization library that
explores different regions of the hyperparameter space through
probabilistic modeling, is used.</p>
<p>Aside from accurate and efficient training, the implementation was
designed specifically for scalability and deployment within practical
systems. Other systems <span class="citation"
data-cites="Woodbridge2018 R2020"></span> require training from scratch
on artificially-created images of text, running into dataset creation
issues with inconsistent text sizing, along with scalability problems.
These issues are bypassed by building on pretrained VLM text encoders
that don’t require image inputs. These advantages make the model more
deployable in applications that involve text-based visual-similarity
analysis. Through use of a lightweight MLP projector and
curriculum-based contrastive learning, the implementation lowers costs
while simultaneously improving performance beyond other existing
methods.</p>
<h2 id="sec:experiments">Experiments</h2>
<h3 id="evaluation-task">Evaluation Task</h3>
<p>To evaluate the effectiveness of VA-TE, homoglyph detection of
visually deceptive website domains <span class="citation"
data-cites="Woodbridge2018"></span> is framed as the core downstream
task. Within this setting, attackers register domains that closely
resemble those of legitimate sites, swapping, deleting, or inserting
characters to create visually similar and deceptive alternatives. At its
core, this evaluation allows determination of whether the model’s
learned embeddings are effective for textual visual similarity
analysis.</p>
<h3 id="dataset">Dataset</h3>
<figure id="fig:triplet-original" data-latex-placement="h">
<img src="VATE/jovan3.png" />
<figcaption>Separability of the triplet dataset without any
modifications.</figcaption>
</figure>
<figure id="fig:triplet-no-com" data-latex-placement="h">
<img src="VATE/jovan4.png" />
<figcaption>Separability of the triplet dataset after “.com”
modification.</figcaption>
</figure>
<p>The publicly available dataset introduced by Woodbridge et al <span
class="citation" data-cites="Woodbridge2018"></span> is used, having
been constructed to support visual similarity learning for homoglyph
detection in both domain and process names. The focus is on the domain
name dataset, and training and evaluation are performed using this
dataset to benchmark the ensemble approach against prior works.</p>
<p>This original dataset was designed for contrastive learning using a
Siamese CNN, where string pairs are rendered as grayscale images and
labeled according to whether they represent a spoofing attack. The
authors construct spoofed domains using predetermined homoglyph
substitutions—targeted character swaps involving visually similar ASCII
and Unicode characters. These manipulations yield domain pairs that are
lexically distinct yet visually deceptive, reflecting the adversarial
nature of homoglyph attacks.</p>
<p>The dataset is divided into three disjoint subsets—training (976,122
examples), validation (51,380 examples), and testing (256,886
examples)—for a total of 1,284,388 labeled pairs. Each example is a
tuple containing a real domain name, a candidate spoof or unrelated
domain, and a binary spoof label. This dataset is adapted to later
generate triplets, along with InfoNCE- and SupCon-compatible
examples.</p>
<p>At inference time, the trained model takes in two domain names,
generates their embeddings using the projector head, and computes their
cosine similarity, which is then used to decide whether the input is a
homoglyph attack.</p>
<h4 id="dataset-manipulations">Dataset Manipulations</h4>
<p>Woodbridge et al. <span class="citation"
data-cites="Woodbridge2018"></span> express domain names in the format
<code>example-domain.com</code>, introducing a constant “.com” suffix.
This fixed token acts as a background signal in the input, possibly
reducing the discriminative capabilities of the VLM’s pooled embedding
and degrading performance. Based on insights from separability analysis,
“.com” is removed from all domain names to mitigate this effect (Table
<a href="#tab:com_removal_example" data-reference-type="ref"
data-reference="tab:com_removal_example">2.2</a>).</p>
<div id="tab:com_removal_example">
<table>
<caption>Example domain transformation after removing
<code>.com</code>.</caption>
<thead>
<tr>
<th style="text-align: left;"><strong>Original</strong></th>
<th style="text-align: center;"><strong>Removed</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><code>example-domain.com</code></td>
<td style="text-align: center;"><code>example-domain</code></td>
</tr>
</tbody>
</table>
</div>
<p>Analysis indicates greater separability between the positive and
negative pairs after removing <code>.com</code>.</p>
<div id="tab:com_removal_metrics">
<table>
<caption>Removing <code>.com</code> on separability of anchor–positive
and anchor–negative cosine similarities.</caption>
<thead>
<tr>
<th style="text-align: left;"><strong>Dataset</strong></th>
<th style="text-align: center;"><strong>KL Divergence</strong></th>
<th style="text-align: center;"><strong>JS Distance</strong></th>
<th style="text-align: center;"><strong>Wasserstein</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Original Dataset</td>
<td style="text-align: center;">0.1205</td>
<td style="text-align: center;">0.0347</td>
<td style="text-align: center;">0.0568</td>
</tr>
<tr>
<td style="text-align: left;"><code>.com</code> Removed</td>
<td style="text-align: center;">0.1442</td>
<td style="text-align: center;">0.0394</td>
<td style="text-align: center;">0.0893</td>
</tr>
</tbody>
</table>
</div>
<figure id="fig:easy-medium-hard-separability">
<img src="VATE/jovan5.png" />
<figcaption>Similarity distribution between anchor–negative and
anchor–positive pairs across easy, medium, and hard
datasets.</figcaption>
</figure>
<p>Visually, Figure <a href="#fig:triplet-original"
data-reference-type="ref" data-reference="fig:triplet-original">2.3</a>
and Figure <a href="#fig:triplet-no-com" data-reference-type="ref"
data-reference="fig:triplet-no-com">2.4</a> reveal a shift in the cosine
similarity distribution of anchor-negative and anchor-positive examples
after removing “.com,” with negative embeddings spreading further from
the positive set. This shift is quantitatively supported by all three
distributional metrics in Table <a href="#tab:com_removal_metrics"
data-reference-type="ref"
data-reference="tab:com_removal_metrics">2.3</a>. Wasserstein distance
increased from 0.0568 to 0.0893 (+57%), indicating greater separation
between anchor–positive and anchor–negative similarity distributions. KL
divergence rose from 0.1205 to 0.1442 (+19.7%), while Jensen–Shannon
distance grew from 0.0347 to 0.0394 (+13.5%). Together, these increases
suggest improved contrast between positives and negatives post-removal,
although the drop in overall similarity values may necessitate
adjustment of decision thresholds.</p>
<p>For the curriculum learning experiments, datasets of varying
difficulty are created by altering the negative sampling strategy.
Difficulty is defined based on the prevalence of hard negatives, samples
that are highly similar to the anchor in the input (semantic) embedding
space. To quantify this, the cosine similarity distributions of
anchor-negative and anchor-positive pairs are compared (see Fig. <a
href="#fig:easy-medium-hard-separability" data-reference-type="ref"
data-reference="fig:easy-medium-hard-separability">2.5</a>), using the
latter as a reference to assess the prevalence of hard negatives.</p>
<p>To construct an easy dataset, random negative examples are sampled,
which tend to be more distinct from their respective anchors and thus
yield a higher baseline separability than the original (medium
difficulty) training dataset. For a hard dataset, hard negatives are
mined, resulting in a much higher average anchor-negative cosine
similarity. Training on these hard negatives forces the model to learn
more subtle perceptual distinctions, improving robustness against highly
deceptive homoglyph attacks.</p>
<div id="tab:mean-similarities-com-removed">
<table>
<caption>Mean similarity values for anchor–negative and anchor–positive
pairs after removing <code>.com</code>, across dataset difficulty
levels.</caption>
<thead>
<tr>
<th style="text-align: left;"><strong>Difficulty</strong></th>
<th style="text-align: center;"><strong>Mean Negative</strong></th>
<th style="text-align: center;"><strong>Mean Positive</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Easy</td>
<td style="text-align: center;">0.4701</td>
<td style="text-align: center;">0.6825</td>
</tr>
<tr>
<td style="text-align: left;">Medium</td>
<td style="text-align: center;">0.5932</td>
<td style="text-align: center;">0.6825</td>
</tr>
<tr>
<td style="text-align: left;">Hard</td>
<td style="text-align: center;">0.7658</td>
<td style="text-align: center;">0.6825</td>
</tr>
</tbody>
</table>
</div>
<p>After removing “.com” from all domain names, mean negative similarity
increases with dataset difficulty—0.4701 for the easy set, 0.5932 for
the medium set, and 0.7658 for the hard set (Table <a
href="#tab:mean-similarities-com-removed" data-reference-type="ref"
data-reference="tab:mean-similarities-com-removed">2.4</a>). This trend
reflects the growing challenge of distinguishing anchor–negative pairs
as difficulty rises, with negatives in the hard set exhibiting
similarity scores closer to those of positives—even surpassing them in
some cases.</p>
<h3 id="baseline-experiments">Baseline Experiments</h3>
<h4 id="vision-language-models-1">Vision-Language Models</h4>
<p>Before applying training methods, several pre-trained VLMs are
evaluated to determine which could serve most effectively as the
backbone for VA-TE. Though these models were initially trained to
capture semantic similarity across image-text pairs, the hypothesis is
that their internal representation may already encode relevant
perceptual information, particularly regarding the visual appearance of
text.</p>
<p>Four prominent VLMs are tested: CLIP, SigLIP, CoCa, and FLAVA. These
models were selected to represent a range of architectures and initial
training datasets. It is hypothesized that dual-encoder designs, because
they encode text and images separately, would be best suited to create
VA-TE. For this reason, two different dual-encoder VLMs are tested: CLIP
and SigLIP.</p>
<ul>
<li><p><strong>CLIP (Contrastive Language-Image Pretraining)</strong>
<span class="citation" data-cites="Radford2021"></span> uses contrastive
loss to train on 400 million image-text pairs, minimizing the distance
between matched pairs, and vice versa. Trained with softmax-normalized
dot-product similarity and cross-entropy loss, CLIP excels at high-level
semantic consistency. It uses a standard Transformer for text encoding
purposes.</p></li>
<li><p><strong>SigLIP (Sigmoid Loss of Language-Image
Pretraining)</strong><span class="citation"
data-cites="Zhai2023"></span> is an adaptation of the CLIP framework
that replaces softmax-based contrastive loss with a sigmoid-based
pairwise loss, training the model using a binary cross-entropy over all
pairwise matches within a batch. In contrast to CLIP’s softmax-based
objective, which emphasizes relative similarity across a batch, SigLIP
evaluates the relatedness of each image-text pair independently using a
sigmoid-based loss. This pairwise formulation allows the model to learn
finer-grained distinctions, which may be particularly helpful for
preserving perceptual detail relevant to visual-similarity
analysis.</p></li>
</ul>
<p>Contrastive-based models are intuitively best-positioned for
visual-similarity analysis. We verify this by testing select models from
other architectures, namely CoCa and FLAVA.</p>
<ul>
<li><p><strong>CoCa (Contrastive Captioners)</strong><span
class="citation" data-cites="Yu2022"></span> is a hybrid encoder-decoder
VLM that blend contrastive pretraining with image captioning objectives,
introducing a captioning loss where the model learns to generate the
associated text from the visual input.</p></li>
<li><p><strong>FLAVA (Foundational Language and Vision
Alignment)</strong><span class="citation" data-cites="Singh2022"></span>
, a fusion-encoder model, is designed to jointly learn representations
from both visual and textual modalities using a transformer backbone.
Performing early fusion, concatenating visual and language tokens then
processing them together, FLAVA excels at understanding modality
interactions.</p></li>
</ul>
<p>Each model was evaluated in its frozen, zero-shot form, using its
text encoder to generate embeddings for domain names, before being
compared to each other using cosine similarity. With this baseline test,
the evaluation determines how well each model’s native embeddings map
visually-aligned text, without any additional fine-tuning. Selection
criteria include ROC AUC, as well as accuracy, recall, and precision at
Youden’s <span class="math inline">\(J\)</span>.</p>
<p>SigLIP yielded the best baseline results as shown in Table <a
href="#tab:vlm" data-reference-type="ref"
data-reference="tab:vlm">[tab:vlm]</a>, indicating its effectiveness for
creating VA-TE. Accordingly, the remaining experiments have been
conducted using SigLIP embeddings as input.</p>
<p><span id="tab:vlm" data-label="tab:vlm"></span></p>
<h3
id="fine-tuning-experiments-contrastive-loss-and-curriculum-learning">Fine-Tuning
Experiments: Contrastive Loss and Curriculum Learning</h3>
<p>The SigLIP text encoder is built upon by training two lightweight
linear projection layers to map inputs into the target embedding space.
To evaluate the effectiveness of different learning strategies, all
pairwise combinations of four contrastive loss functions with each
curriculum learning approach are tested. As a control, each loss
function is also fine-tuned without curriculum learning using only
original training data from <span class="citation"
data-cites="Woodbridge2018"></span>, isolating the contribution of
curriculum design.</p>
<h3 id="ensemble">Ensemble</h3>
<p>After identifying the best VA-TE configuration (contrastive objective
+ curriculum strategy) using the validation set, the model is frozen and
similarity scores <span class="math inline">\(s_{\text{VA-TE}}\)</span>
are computed on the training, validation, and test splits. A Gradient
Boosting Classifier is then trained using three input features: <span
class="math inline">\(s_{\text{VA-TE}}\)</span>, Token Set Ratio, and
Levenshtein distance. The decision threshold is tuned on the validation
set, and test performance is evaluated via ROC-AUC. To assess the
benefit of ensembling, ablation results are also reported for using
VA-TE alone and string-based features alone.</p>
<h3 id="sec: evaluation-metrics">Evaluation Metrics</h3>
<p>The primary evaluation metric is ROC-AUC, a threshold independent
value which enables direct comparison with previous works <span
class="citation" data-cites="Woodbridge2018 R2020"></span> and provides
a robust summary of the model’s ability to distinguish visually similar
text. Accuracy, Precision, and Recall are additionally reported to offer
a more detailed view of performance under practical performance
metrics.</p>
<h2 id="sec:results">Results</h2>
<h3 id="baseline-performance">Baseline Performance</h3>
<p>To assess the effectiveness of traditional string matching
approaches, Levenshtein Distance and Token Set Ratio were evaluated on
the spoof detection task. As shown in Table <a
href="#tab:baseline_unsupervised" data-reference-type="ref"
data-reference="tab:baseline_unsupervised">[tab:baseline_unsupervised]</a>,
Token Set Ratio slightly outperforms Levenshtein Distance in ROC-AUC
(0.8350 vs. 0.8137), indicating the capture of a slightly larger range
of visually-similar text. However, these results are not viable compared
to existing methods, despite scalability advantages. This motivates the
usage of VLMs for better learned representations of visual
similarity.</p>
<p><span id="tab:baseline_unsupervised"
data-label="tab:baseline_unsupervised"></span></p>
<h3 id="va-te-performance">VA-TE Performance</h3>
<p>Across 20 different VA-TE training configurations, the interaction
between contrastive loss functions and curriculum learning strategies
was explored. As shown in Table <a href="#tab:curriculum_short"
data-reference-type="ref"
data-reference="tab:curriculum_short">[tab:curriculum_short]</a>, the
pairwise contrastive loss consistently achieves the highest ROC-AUC
scores, reaching 0.95 across nearly all curriculum variations. For
triplet loss, scores hovered around 0.91–0.92 regardless of curriculum.
InfoNCE and SupCon performed competitively (0.92–0.94), with Automated
Curriculum slightly boosting SupCon to 0.94. These results indicate that
pairwise loss is robust to sampling difficulty, triplet remains weaker
overall, while InfoNCE and SupCon show modest gains from curriculum
learning.</p>
<p><span id="tab:curriculum_short"
data-label="tab:curriculum_short"></span></p>
<p>Taken together, these findings underscore VA-TE’s competitive
performance and practical deployability compared to SOTA alternatives.
Nevertheless, vision-language models alone are limited in capturing the
textual perturbations common in homoglyph attacks. This motivates an
ensemble approach, which corroborates VA-TE similarity scores with
Levenshtein distance and fuzzy string matching.</p>
<h3 id="ensembles">Ensembles</h3>
<p>To evaluate the benefits of combining vision-language and
string-based features, an ensemble model was constructed using the best
VA-TE configuration alongside traditional string similarity metrics. As
detailed in Table <a href="#tab:ensemble_results"
data-reference-type="ref" data-reference="tab:ensemble_results">2.5</a>,
the ensemble achieved a ROC-AUC of 0.98, outperforming both the VA-TE
model alone (0.95) and the string-based metrics alone (0.89). This
significant improvement highlights the complementary strengths of
visual-semantic embeddings and token-level string comparisons,
especially in detecting visually deceptive text where either modality
alone may be insufficient.</p>
<div id="tab:ensemble_results">
<table>
<caption><em>ROC-AUC for Ensemble Strategies</em>. “String Metrics”
refers to the combination of Edit Distance and Token Set
Ratio.</caption>
<thead>
<tr>
<th style="text-align: left;"><strong>Ensemble Strategy</strong></th>
<th style="text-align: center;"><strong>ROC AUC</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">String Metrics</td>
<td style="text-align: center;">0.89</td>
</tr>
<tr>
<td style="text-align: left;">VA-TE + String Metrics</td>
<td style="text-align: center;">0.98</td>
</tr>
</tbody>
</table>
</div>
<p><span id="tab:ensemble_results"
data-label="tab:ensemble_results"></span></p>
<h3 id="comparison-to-state-of-the-art">Comparison to
State-of-the-Art</h3>
<p>Compared to prior SOTA methods, the VA-TE framework alone achieves
highly competitive results while maintaining a lightweight and scalable
architecture. As shown in Table <a href="#tab:sota_results"
data-reference-type="ref" data-reference="tab:sota_results">2.6</a>, the
VA-TE ensemble attains SOTA results with a ROC-AUC of 0.98, surpassing
traditional models that rely heavily on domain-specific heuristics or
large-scale neural architectures. Furthermore, while other deep learning
approaches require substantial training time and preprocessing, VA-TE
benefits from compact linear projection heads over pretrained encoders,
enabling easier deployment beneficial for real-world applications that
demand both accuracy and efficiency. These results demonstrate that a
semantically grounded representation can be effectively translated into
a visually aligned embedding space, enabling high performance on
downstream tasks.</p>
<div id="tab:sota_results">
<table>
<caption><em>Comparison of proposed methods to state-of-the-art models
on the ROC-AUC metric.</em></caption>
<thead>
<tr>
<th style="text-align: left;"><strong>Method</strong></th>
<th style="text-align: center;"><strong>ROC AUC</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>VA-TE</strong></td>
<td style="text-align: center;">0.95</td>
</tr>
<tr>
<td style="text-align: left;"><strong>VA-TE + String
Metrics</strong></td>
<td style="text-align: center;">0.98</td>
</tr>
<tr>
<td style="text-align: left;">Siamese-GRU <span class="citation"
data-cites="R2020"></span></td>
<td style="text-align: center;">0.98</td>
</tr>
<tr>
<td style="text-align: left;">Siamese-LSTM <span class="citation"
data-cites="R2020"></span></td>
<td style="text-align: center;">0.97</td>
</tr>
<tr>
<td style="text-align: left;">Siamese-CNN <span class="citation"
data-cites="Woodbridge2018"></span></td>
<td style="text-align: center;">0.97</td>
</tr>
<tr>
<td style="text-align: left;">Siamese-CNN <span class="citation"
data-cites="R2020"></span></td>
<td style="text-align: center;">0.93</td>
</tr>
</tbody>
</table>
</div>
<p><span id="tab:sota_results" data-label="tab:sota_results"></span></p>
<h2 id="sec:discussion">Discussion</h2>
<h3 id="performance-analysis">Performance Analysis</h3>
<p>Experiments highlight important insights into the efficacy of VA-TE
and the role of different training configurations. </p>
<p>Across all settings, pairwise contrastive loss consistently achieved
the strongest performance, reaching a ROC-AUC of 0.95. This result is
consistent with the SigLIP encoder’s original pairwise training
objective <span class="citation" data-cites="Zhai2023"></span> and
avoids the margin sensitivity issues that hinder alternative loss
formulations. In particular, triplet loss plateaued around 0.91-0.92,
underperforming pairwise. This is likely due to difficulties in margin
tuning and limited batch sizes restricting effective negative sampling.
InfoNCE and SupCon achieved competitive results, but showed greater
dependence on curriculum learning to achieve optimal performance.</p>
<p>The impact of curriculum learning varied according to the loss
function used. For pairwise and triplet losses, curriculum learning
yielded negligible improvements, suggesting that these objectives are
relatively robust to changes in negative sampling difficulty. In
contrast, InfoNCE and SupCon gained modest but consistent benefits from
Automated Curriculum, while manual and bandit-based approaches showed
little added value. Across the four loss functions, the Automated
Curriculum was consistently the strongest performer, matching the best
result or achieving the top score outright, providing smoother
transitions and more stable convergence than alternative curricula (as
seen in Table  <a href="#tab:curriculum_short" data-reference-type="ref"
data-reference="tab:curriculum_short">[tab:curriculum_short]</a>).</p>
<p>When compared against prior benchmarks, VA-TE achieved its best
standalone performance with pairwise loss and automated curriculum
(ROC-AUC = 0.95). More importantly, when fused with string-based
similarity metrics such as Levenshtein distance and Token Set Ratio, the
ensemble attained state-of-the-art performance with a ROC-AUC of 0.98.
This surpasses or matches the strongest image-based approaches,
including Siamese-CNN and Siamese-GRU models <span class="citation"
data-cites="Woodbridge2018 R2020"></span>. Unlike those methods, VA-TE
operates directly on text rather than rendered images, providing
substantial advantages in scalability, memory efficiency, and deployment
simplicity while maintaining competitive accuracy. These findings
underscore both the robustness of pairwise contrastive learning in
visually-aligned text embeddings and the complementary value of
combining embedding-based and string-based features.</p>
<h3 id="advantages-of-va-te">Advantages of VA-TE</h3>
<h4 id="scalability-benefits-over-image-based-approaches">Scalability
Benefits over Image-Based Approaches</h4>
<p>VA-TE offers substantial advantages over existing image-based
homoglyph detection methods in terms of computational efficiency and
memory requirements. Previous state-of-the-art approaches <span
class="citation" data-cites="Woodbridge2018 R2020"></span> require
rendering each string as a binary image before training and inference.
Specifically, Woodbridge et al. <span class="citation"
data-cites="Woodbridge2018"></span> render strings as 150<span
class="math inline">\(\times\)</span>12 pixel grayscale images. The
associated datasets contain 1,284,388 labeled pairs, requiring the
generation and storage of over 2.5 million images. In contrast, VA-TE
operates directly on text strings, eliminating image rendering entirely
and reducing memory overhead by orders of magnitude.</p>
<p>Beyond storage considerations, image-based methods introduce
additional computational overhead during both training and deployment.
Each inference requires: 1) String-to-image rendering using specific
fonts and sizing parameters, 2) Image preprocessing and normalization,
and 3) Forward pass through a convolutional neural network designed for
image input. VA-TE eliminates steps 1 and 2 entirely, requiring only a
lightweight forward pass through the frozen VLM text encoder followed by
the projection head. This streamlined pipeline enables real-time
deployment in production environments where latency and computational
resources are critical constraints.</p>
<h4 id="deployment-simplicity-and-robustness">Deployment Simplicity and
Robustness</h4>
<p>VA-TE’s text-only design significantly simplifies deployment by
avoiding the rendering used by image-based methods, which depend on
parameters like font choice, size, anti-aliasing, and background color.
Inconsistencies in these parameters can degrade model performance and
require pipeline maintenance. By relying on pretrained VLM text
encoders, VA-TE avoids these issues entirely, enabling consistent
performance across environments without graphics libraries or image
processing, reducing the risk of deployment failures.</p>
<p>Additionally, image-based approaches impose artificial constraints on
input string length due to fixed image dimensions. CNN-based models
often require truncating or padding strings to fit a predetermined image
size (e.g., the 25-character limit in <span class="citation"
data-cites="Woodbridge2018"></span>), introducing a source of
information loss or distortion. By contrast, VA-TE can process strings
of arbitrary length limited only by the underlying VLM’s context window,
typically accommodating hundreds of characters.</p>
<p>Furthermore, VA-TE leverages the robustness of large-scale pretrained
models. SigLIP and similar VLMs are trained on massive, diverse datasets
and demonstrate strong generalization capabilities. By building on these
established representations rather than training from scratch on
synthetically generated images, VA-TE inherits the robustness and
stability of the underlying foundation model.</p>
<h3 id="sec:vate-future">Future Work</h3>
<h4 id="application-to-different-text-domains">Application to Different
Text Domains</h4>
<p>Extending beyond English, visually deceptive text persists in other,
more complex writing systems. Detecting homoglyphs in languages such as
Chinese, Arabic, or Cyrillic would require more fine-grained character
distinctions and introduces additional challenges, including font-based
homographs and diacritic variations.</p>
<p>Alternatively, VA-TE can serve as a verification layer for Optical
Character Recognition (OCR) pipelines, particularly in document
digitization. Recent literature has shown that OCR systems are
susceptible to misclassifying visually similar characters—particularly
in difficult-to-distinguish languages like Arabic <span class="citation"
data-cites="app13074584"></span>. Embedding-based homoglyph detection
could identify and automatically correct these OCR-induced
misclassifications, providing vast applications in security-critical
contexts, including bank check processing, passport scanning, and
historical document transcription.</p>
<h4 id="enhancing-text-embeddings-via-perceptual-decoding">Enhancing
Text Embeddings via Perceptual Decoding</h4>
<p>While the current approach leverages the joint text-image embedding
space of pretrained VLMs, it relies exclusively on the <em>text
encoder</em> to generate representations of input strings. These
embeddings, though semantically rich and convenient for scalable
deployment, are aligned with the final-layer outputs of the VLM’s text
encoder and therefore may not capture perceptual features necessary for
fine-grained homoglyph detection.  </p>
<p>To address this limitation, an architectural extension is proposed
that grounds text embeddings in low- and mid-level visual features
extracted from rendered images of the input text. Specifically, a
lightweight decoder <span class="math inline">\(f_d:
\mathbb{R}^{d_{text}} \rightarrow \mathbb{R}^{d_{vis}}\)</span> could be
introduced, trained to map text embeddings to multi-scale image
features, extracted from shallow, intermediate, and deeps layers of the
VLM’s vision encoder. During training, this decoder learns to
reconstruct visual features from text embeddings alone, encouraging the
text representations to encode subtle typographic details. Crucially,
once trained, the decoder becomes part of the permanent inference
pipeline, transforming text embeddings into visually-grounded
representations without requiring image inputs at deployment time.
Training can be approached in two ways:</p>
<ol>
<li><p><strong>Preprocessing module:</strong> Pre-train the encoder to
enhance the text embedding space prior to standard contrastive
training.</p></li>
<li><p><strong>Joint Optimization:</strong> Simultaneously train the
decoder and contrastive objectives, allowing the text encoder to adapt
its representations for both visual reconstruction and similarity
learning.</p></li>
</ol>
<p>This formulation offers a promising path for improving visual
discrimination in spoof detection while preserving the efficiency and
deployment simplicity of text-only inference.</p>
<h4 id="enhanced-model-architectures-and-training-strategies">Enhanced
Model Architectures and Training Strategies</h4>
<p>Future architectures could benefit from delayed weight update
strategies, where gradients are accumulated over multiple batches before
parameter updates  <span class="citation"
data-cites="goyal2017accurate"></span>. Given the relatively lightweight
projection head (two linear layers) and the noisy nature of contrastive
learning objectives  <span class="citation"
data-cites="Chen2020"></span>, gradient accumulation could provide more
stable learning signals and improved global optimization trajectories
 <span class="citation" data-cites="hoffer2017train"></span>. This
approach would be particularly valuable when computational constraints
limit batch sizes, allowing the model to approximate the benefits of
larger-batch training without additional memory overhead.</p>
<h1
id="learning-what-is-real-intrinsic-authenticity-detection-for-generalization-across-deepfake-methods">Learning
What Is Real: Intrinsic Authenticity Detection for Generalization Across
Deepfake Methods</h1>
<h2 id="introduction-2">Introduction</h2>
<p>Deepfake generation technology has been advancing at an unprecedented
pace, posing significant threats to systems dependent on identity
verification and public trust. Early deepfakes relied on simple
face-swapping techniques using Generative Adversarial Networks (GANs),
but modern diffusion models are now able to produce synthetic media with
photorealistic quality that is increasingly difficult to distinguish
from authentic content. This rapid evolution in generation capabilities
and methods has exposed critical vulnerabilities in existing detection
approaches.</p>
<p>Current deepfake detection methods face two fundamental limitations.
First, many approaches train exclusively on synthetic data generated by
a single model, learning to identify artifacts specific to that
generation method [cite]. While this strategy achieves reasonable
performance on data from within the training distribution, it
drastically underperforms when confronted with novel generation
techniques. The accelerating pace of advancement in this field poses a
critical weakness for identifying authentic media. Second, alternative
approaches attempt to leverage embeddings from various modalities,
namely audio and video features, to distinguish real from synthetic
media (cite). However, these learned representations predominantly
encode content-related information rather than the subtle artifacts and
inconsistencies that reliably indicate synthetic generation.</p>
<p>These limitations motivate the approach: disentangling
authenticity-related features from those encoding identity and content
within pretrained representations, isolating the signals most relevant
for deepfake detection. By explicitly separating these feature types,
the goal is to reduce the influence of extraneous information, which may
be similar across both real and fake media, while amplifying
detection-relevant signals that generalize across generation methods.
The central hypothesis is that disentanglement will enable learned
representations to cluster authentic and synthetic media into distinct
regions of a latent space, achieving robust generalization to previously
unseen generation techniques. This approach addresses both the
overfitting problem of technique-specific detectors and the
content-encoding limitation of existing representation-based methods,
offering a more principled path toward reliable, generalizable deepfake
detection.</p>
<h2 id="related-works">Related Works</h2>
<h3 id="deepfake-generation-so-far">Deepfake Generation So Far</h3>
<p>Deepfake generation has progressed rapidly from early face-swapping
pipelines toward highly expressive multimodal synthesis systems. Initial
methods relied on autoencoders and Generative Adversarial
Network(GAN)-based architectures <span class="citation"
data-cites="goodfellow2020a"></span>, which learned identity mappings
through reconstruction losses and produced artifacts that were often
detectable using pixel-level or frequency-based heuristics. More recent
advances leverage diffusion models and large-scale text-to-video
architectures, such as those powering modern systems like Sora, which
model long-range temporal coherence and fine-grained photorealism<span
class="citation" data-cites="babaei2025a"></span>. These approaches
significantly reduce the low-level inconsistencies that earlier
detectors relied on, making generation artifacts less stable and less
predictable. As generative frameworks diverge in architecture, training
data, and inference strategies, the space of possible deepfake
signatures becomes more variable. This shift has made artifact-based
detection fundamentally brittle: cues that signal synthesis for one
model are often fundamentally different from those from another model.
Understanding how the diversity of generative techniques influences
downstream embedding structure is therefore essential for developing
detection systems that generalize beyond the specific artifacts of a
single generator.</p>
<h3 id="deepfake-detection-approaches">Deepfake Detection
Approaches</h3>
<p>Existing deepfake detection research spans supervised,
self-supervised, and multimodal strategies, yet most approaches share a
central assumption: the fake distribution is sufficiently stable that
learned discriminative boundaries will transfer to new content.
Artifact-based methods seek inconsistencies in eye blinking, head
motion, frequency spectra, or compression patterns, and while effective
within their training distribution, they generalize poorly to newer or
higher-fidelity generators <span class="citation"
data-cites="matern2019a frank2020a li2018a"></span> Embedding-based
approaches attempt to bypass brittle pixel-level cues by leveraging
audio, visual, or joint audio-visual representations. Video models
capture identity and motion patterns, while audio models encode speaker
characteristics, prosody, and phonetic structure. However, these
embeddings are optimized primarily for recognition tasks—such as speech
transcription, speaker identification, or scene classification—rather
than for modeling authenticity. As a result, they tend to encode content
and identity far more strongly than generative irregularities, and so
simple ensembles have been proved ineffective for this task <span
class="citation" data-cites="khalid2021evaluation"></span>. Because
deepfake models increasingly preserve high-level content while altering
low-level generative processes, relying on embeddings not explicitly
disentangled for authenticity introduces confounding factors that may
obscure the subtle and cross-model-consistent cues necessary for
generalization. This limitation motivates approaches that explicitly
separate authenticity-related signals from identity and semantic
content.</p>
<h3 id="disentangled-representation-learning">Disentangled
Representation Learning</h3>
<p>REVISED</p>
<p>Disentangled representation learning seeks to separate latent factors
of variation into orthogonal or minimally correlated subspaces, enabling
models to isolate specific causal or semantic dimensions. Classical
approaches include <span class="math inline">\(\beta\)</span>-VAE  <span
class="citation" data-cites="higgins2017beta"></span> frameworks,
contrastive learning variants[NEED CITATION], and supervised
disentanglement [NEED CITATION] using attribute labels. More recently,
multi-task learning and orthogonality-constrained projection methods
have been used to prevent one objective from dominating representation
space, as demonstrated in gradient-balancing strategies such as GradNorm
 <span class="citation" data-cites="chen2018gradnorm"></span>. For
deepfake detection, disentanglement is appealing because authenticity
should represent a low-variance, shared property across all real
samples, while identity and content vary widely across individuals and
contexts  <span class="citation" data-cites="liang2025a"></span>.
Separating these components could allow detectors to operate on features
that generalize across generative models without collapsing under
distributional shifts. However, little prior work has explored
disentanglement explicitly for deepfake detection, and existing
embedding refinement techniques primarily target robustness or semantic
alignment [NEED CITATION HERE]. This gap highlights the need for
systematic evaluation of whether disentangling identity from
authenticity can meaningfully improve generalization, particularly for
unseen generators where artifact-based cues are unreliable.</p>
<h2 id="methods">Methods</h2>
<p>This section presents the methodology for learning authenticity-based
representations that disentangle real-content structure from identity
and semantic information. The framework builds on pretrained audio and
visual embeddings and introduces projection heads, orthogonality
constraints, and multi-objective regularization to isolate factors most
relevant for deepfake detection and cross-model generalization.</p>
<h3
id="overview-disentangled-representation-learning-framework">Overview:
Disentangled Representation Learning Framework</h3>
<p>The core hypothesis of this work is that authentic audio–visual
content lies on a relatively stable, low-variance manifold, whereas
deepfakes, regardless of the generative model used, deviate from this
structure. Conventional embeddings emphasize identity and
content-relevant features because they are trained for tasks like speech
recognition, speaker identification, or image classification. As a
result, these embeddings often conflate identity information with
authenticity information. To address this, the proposed framework learns
two complementary subspaces:</p>
<p><span class="math inline">\(\textbf{Authenticity Subspace}\)</span>:
Captures stable characteristics that real samples share and fake samples
disrupt.</p>
<p><span class="math inline">\(\textbf{Identity/Content
Subspace}\)</span>: Preserves information that varies between speakers,
scenes, or utterances and should not influence authenticity
decisions.</p>
<p>Given an embedding from a pretrained model <span
class="math inline">\(z\)</span> [REFER TO CORRESPONDING SECTION], the
framework maps this representation into to projected vectors</p>
<p><span class="math display">\[z^{\text{auth}} =
f_{\text{auth}}(z)\]</span></p>
<p><span class="math display">\[z^{\text{id}} =
f_{\text{id}}(z)\]</span></p>
<p>where <span class="math inline">\(f_{auth}\)</span> and <span
class="math inline">\(f_{id}\)</span> are lightweight, two-layer
projection heads. These heads are trained jointly to enforce structural
separation between authenticity and identity components while
maintaining useful representational geometry. This is motivated by
recent work showing separation of complementary factor aids
generalization in media forensics  <span class="citation"
data-cites="liang2025a"></span>.</p>
<p>The learning framework is built around three objectives: (1) an
authenticity-consistency objective, (2) an identity discrimination
objective, and (3) an orthogonality constraint between subspaces. A
loss-balancing mechanism regulates their interaction to prevent collapse
and ensure stability across embedding types.</p>
<h3 id="sec:representation-learning">Training Objectives</h3>
<h5 id="orthogonality-constraint">Orthogonality Constraint</h5>
<p>Even with complementary objectives, the two projection heads may
drift toward encoding correlated information. To enforce explicit
disentanglement, the framework imposes an orthogonality regularizer:</p>
<p><span class="math display">\[\begin{equation}
    \mathcal{L}_{\text{orth}} = \frac{1}{N^2} \sum_{i=1}^{N}
\sum_{j=1}^{N}
    \left| \text{sim}\left(z_i^{\text{id}}, z_j^{\text{auth}}\right)
\right|
\label{eq:orth}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\text{sim}(\cdot, \cdot)\)</span>
denotes cosine similarity. Penalizing cross-subspace dot products
encourages the two projections to lie in approximately orthogonal
directions within representation space. This decorrelation is critical
for generalization: a model that disentangles these factors should not
memorize identity-specific deepfake patterns (e.g., “person A is always
fake in the training set”) but instead learn identity-agnostic
authenticity cues.</p>
<h5 id="identity-learning-via-prototypical-contrastive-loss">Identity
Learning via Prototypical Contrastive Loss</h5>
<p><br />
To prevent the authenticity projection from inadvertently absorbing
identity-related information, a complementary objective ensures that
identity remains encoded separately. This mirrors the intuition that two
real samples of different individuals should not appear artificially
similar in authenticity space simply because the variance penalty
compresses them. [PREVIEW]For a "content group" (augmentations of the
same source video or frames from the same video), we compute the
prototype as the mean embedding.</p>
<p><span class="math display">\[c_k = \frac{1}{|G_k|}\sum_{i \in
G_k}z_i^{\text{id}}\]</span></p>
<p>where <span class="math inline">\(G_k\)</span> is the set of samples
belonging to a content group <span class="math inline">\(k\)</span>.
Then, the identity projection head is trained with a prototypical
contrastive objective:</p>
<p><span class="math display">\[\begin{equation}
\label{eq:proto_loss}
    \mathcal{L}_{\text{proto}} = -\frac{1}{N} \sum_{i=1}^{N} \log \left[
\frac{\exp\left(-d(z^{\text{id}}_i, c_k) / \tau\right)}{\sum_{j}
\exp\left(-d(z^{\text{id}}_i, c_j) / \tau\right)} \right]
\end{equation}\]</span> where <span
class="math inline">\(d(a,b)\)</span> is Euclidean distance, <span
class="math inline">\(\tau\)</span> is a temperature hyperparameter, and
the sum is over all prototypes in the batch. In addition, for each</p>
<p>where positive pairs share speaker or identity labels and negatives
differ. This ensures that <span class="math inline">\(z_i\)</span>
preserves rich, discriminative variation and prevents the authenticity
head from learning content-based shortcuts that do not generalize.
Together, the authenticity and identity objectives form a disentangled
representation in which each subspace captures distinct, minimally
interacting factors.</p>
<h5 id="authenticity-learning-via-variance-minimization">Authenticity
Learning via Variance Minimization</h5>
<p>The authenticity head <span
class="math inline">\(f_{\text{auth}}\)</span> is trained using an
anomaly-detection principle: only real videos are used during
optimization, encouraging their embeddings to form a compact cluster in
the <span class="math inline">\(z^{\text{auth}}\)</span> space. Real
samples share consistent authenticity cues (e.g., sensor noise and
photometric coherence), whereas fake videos exhibit heterogeneous
artifacts that depend on the generation method. Minimizing variance
among real embeddings is therefore intended to form a compact "real
manifold" that fake samples naturally deviate from.</p>
<p>[NEED CITATION]However, naïve variance minimization introduces a
critical failure mode: the objective cannot distinguish a meaningful
compact cluster from a degenerate collapsed solution in which all
embeddings map to a single point. When combined with the prototypical
identity loss, this collapse becomes even more likely, as both
objectives can be jointly minimized by destroying class
separability.</p>
<p>To prevent this, a <em>variance floor regularizer</em> is added:
<span class="math display">\[\begin{equation}
\label{eq:variance_loss_reg}
\mathcal{L}_{\text{var}} =
\frac{1}{N_{\text{real}}} \sum_{i : y_i = 1}
\left\| z^{\text{auth}}_i - \mu_{\text{real}} \right\|^2
+ \lambda_{\text{reg}}
\left[
\left( \max(0, \tau - \sigma^2) \right)^2
+ 5 \cdot \max(0, \tau - \sigma^2)
\right].
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(\tau\)</span> sets the minimum
allowable variance. The quadratic term stabilizes gradients near the
threshold, while the linear term provides stronger penalties when
variance collapses. This ensures real embeddings remain tightly
clustered yet sufficiently spread to preserve structure and allow fake
samples to diverge.</p>
<h5 id="joint-optimization">Joint Optimization</h5>
<p>The total loss for the representation learning stage combines three
objectives: <span class="math display">\[\begin{equation}
\label{eq:total_loss}
    \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{proto}} +
\mathcal{L}_{\text{var}} + \lambda_{\text{orth}}
\mathcal{L}_{\text{orth}}
\end{equation}\]</span> where <span
class="math inline">\(\lambda_{\text{orth}}\)</span> weights the
orthogonality constraint. Each loss is normalized by its initial value
to ensure similar magnitudes across objectives, then equal unit weights
are applied (1.0 for prototype, 1.0 for variance, <span
class="math inline">\(\lambda_{\text{orth}} = 0.1\)</span> for
orthogonality). This normalization strategy proved more stable than
gradient-based balancing methods <span class="citation"
data-cites="chen2018gradnorm"></span> in preliminary experiments, as it
avoids the interaction between adaptive weight scheduling and variance
floor regularization.</p>
<p>[METHODS SECTION: 1244 WORDS AT THE MOMENT]</p>
<h2 id="experiments">Experiments</h2>
<h3 id="sec:datasets">Datasets</h3>
<p>The method is trained and evaluated on three complementary
datasets—AVDeepfake-1M++ <span class="citation"
data-cites="cai2025av"></span>, ShareVeo3  <span class="citation"
data-cites="wang2024vidprom"></span>, and Sora2 <span class="citation"
data-cites="sora2_2025"></span>—chosen to cover a broad spectrum of
deepfake generation techniques. AVDeepfake-1M++ contains large-scale
face-swap and voice-cloning manipulations applied to real source videos,
while ShareVeo3 and OpenAI’s Sora2 represent fully synthetic,
text-to-video diffusion models that generate entire videos from scratch.
ShareVeo3 contributes additional diversity during training, and Sora2 is
reserved exclusively for out-of-distribution evaluation, enabling
assessment of generalization to a state-of-the-art generator unseen
during training.</p>
<p>All videos are segmented into 0.15-second clips, from which
pretrained encoders extract audio and video embeddings that are stored
in a serverless PostgreSQL database. This granularity balances temporal
resolution with computational efficiency. From AVDeepfake-1M++, the
training subset includes 241 source videos and their augmented variants,
producing 231,889 embeddings. The dataset provides two structural
advantages: multiple perturbation-based augmentations per source,
supporting identity learning, and partial fake injections with
segment-level authenticity labels. After preprocessing, the subset
contains 225,843 real, 2,394 fully fake, and 3,652 partial fake
embeddings. Balanced batching and content-group definitions (all
augmentations of the same source) support prototypical contrastive
learning. ShareVeo3 contributes 63,184 fully fake embeddings from 1,460
diffusion-generated videos. Although lacking augmentations, temporal
continuity within each video forms content groups for identity modeling.
Sora2 provides 11,317 fully synthetic embeddings for strict
out-of-distribution evaluation. Its diverse human and non-human content
enables testing whether the model captures intrinsic properties of
authentic video rather than artifacts specific to a manipulation
technique.</p>
<h3 id="sec:embedding-selection">Embedding Selection</h3>
<p>Before training the disentanglement model, a systematic evaluation is
conducted to identify which pretrained audio and video embeddings
provide the strongest baseline separability between real and fake
samples. Candidate embeddings are evaluated using 5-fold
cross-validation on a balanced subset of AVDeepfake1M++, training simple
logistic regression classifiers on each embedding type independently.
This analysis informs the choice of input representations for the full
pipeline.</p>
<h4 id="audio-embeddings">Audio Embeddings</h4>
<p>Four audio embedding approaches are evaluated: OpenL3 <span
class="citation" data-cites="cramer2019openl3"></span>, HuBERT <span
class="citation" data-cites="hsu2021hubert"></span>, Wav2Vec2 <span
class="citation" data-cites="baevski2020wav2vec"></span>, and
Mel-Frequency Cepstral Coefficients (MFCC). Table <a
href="#tab:audio-embed" data-reference-type="ref"
data-reference="tab:audio-embed">3.1</a> reports classification
performance for each embedding type.</p>
<div id="tab:audio-embed">
<table>
<caption>Audio embedding comparison for deepfake detection. Results
averaged over 5-fold cross-validation.</caption>
<thead>
<tr>
<th style="text-align: left;"><strong>Embedding</strong></th>
<th style="text-align: center;"><strong>AUROC</strong></th>
<th style="text-align: center;"><strong>Accuracy</strong></th>
<th style="text-align: center;"><strong>Recall</strong></th>
<th style="text-align: center;"><strong>FPR</strong></th>
<th style="text-align: center;"><strong>F1</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">OpenL3</td>
<td style="text-align: center;"><strong>0.976</strong></td>
<td style="text-align: center;"><strong>93.84%</strong></td>
<td style="text-align: center;"><strong>93.78%</strong></td>
<td style="text-align: center;"><strong>6.12%</strong></td>
<td style="text-align: center;"><strong>0.931</strong></td>
</tr>
<tr>
<td style="text-align: left;">HuBERT</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">90.75%</td>
<td style="text-align: center;">87.08%</td>
<td style="text-align: center;">6.31%</td>
<td style="text-align: center;">0.893</td>
</tr>
<tr>
<td style="text-align: left;">Wav2Vec2</td>
<td style="text-align: center;">0.776</td>
<td style="text-align: center;">74.92%</td>
<td style="text-align: center;">65.31%</td>
<td style="text-align: center;">17.40%</td>
<td style="text-align: center;">0.698</td>
</tr>
<tr>
<td style="text-align: left;">MFCC</td>
<td style="text-align: center;">0.699</td>
<td style="text-align: center;">66.10%</td>
<td style="text-align: center;">54.31%</td>
<td style="text-align: center;">24.47%</td>
<td style="text-align: center;">0.587</td>
</tr>
</tbody>
</table>
</div>
<p>OpenL3 achieves the strongest performance across all metrics, with an
AUROC of 0.976 and F1 score of 0.931. However, HuBERT performs
competitively, achieving 0.958 AUROC with a comparable false positive
rate (6.31% vs 6.12%).</p>
<p>To further characterize the discriminative power of each embedding,
distributional divergence metrics between real and fake samples are
computed. Table <a href="#tab:audio-divergence"
data-reference-type="ref" data-reference="tab:audio-divergence">3.2</a>
reports KL divergence for each embedding type.</p>
<div id="tab:audio-divergence">
<table>
<caption>Distribution divergence between real and fake samples for audio
embeddings. Higher values indicate greater separability.</caption>
<thead>
<tr>
<th style="text-align: left;"><strong>Embedding</strong></th>
<th style="text-align: center;"><strong>KL Divergence</strong></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">OpenL3</td>
<td style="text-align: center;"><strong>7.57</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">HuBERT</td>
<td style="text-align: center;">6.34</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Wav2Vec2</td>
<td style="text-align: center;">1.28</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">MFCC</td>
<td style="text-align: center;">1.21</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
</div>
<p>The divergence metric corroborats the classification results: OpenL3
and HuBERT exhibit substantially higher distributional separation
between classes. This suggests that OpenL3 and HuBERT encode features
that naturally distinguish authentic from synthetic audio, making them
suitable candidates for downstream representation learning.</p>
<p>Based on these results, both OpenL3 and HuBERT are selected as audio
embeddings. While OpenL3 achieves marginally higher performance, HuBERT
captures complementary information (Pearson correlation <span
class="math inline">\(r = 0.82\)</span> between their prediction
scores), motivating their combined use.</p>
<h4 id="video-embeddings">Video Embeddings</h4>
<p>Five video embedding approaches are evaluated spanning face
recognition models and video understanding architectures: SENet <span
class="citation" data-cites="hu2018senet"></span>, Marlin <span
class="citation" data-cites="cai2022marlin"></span>, ArcFace <span
class="citation" data-cites="deng2019arcface"></span>, FaceNet <span
class="citation" data-cites="schroff2015facenet"></span>, and
MagFace <span class="citation" data-cites="meng2021magface"></span>.
Table <a href="#tab:video-embed" data-reference-type="ref"
data-reference="tab:video-embed">3.3</a> reports classification
performance.</p>
<div id="tab:video-embed">
<table>
<caption>Video embedding comparison for deepfake detection. Results
averaged over 5-fold cross-validation.</caption>
<thead>
<tr>
<th style="text-align: left;"><strong>Embedding</strong></th>
<th style="text-align: center;"><strong>AUROC</strong></th>
<th style="text-align: center;"><strong>Accuracy</strong></th>
<th style="text-align: center;"><strong>Recall</strong></th>
<th style="text-align: center;"><strong>FPR</strong></th>
<th style="text-align: center;"><strong>F1</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SENet</td>
<td style="text-align: center;"><strong>0.914</strong></td>
<td style="text-align: center;"><strong>87.03%</strong></td>
<td style="text-align: center;"><strong>88.04%</strong></td>
<td style="text-align: center;"><strong>13.77%</strong></td>
<td style="text-align: center;"><strong>0.858</strong></td>
</tr>
<tr>
<td style="text-align: left;">Marlin</td>
<td style="text-align: center;">0.802</td>
<td style="text-align: center;">74.28%</td>
<td style="text-align: center;">66.27%</td>
<td style="text-align: center;">19.31%</td>
<td style="text-align: center;">0.696</td>
</tr>
<tr>
<td style="text-align: left;">ArcFace</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">67.16%</td>
<td style="text-align: center;">78.47%</td>
<td style="text-align: center;">41.87%</td>
<td style="text-align: center;">0.680</td>
</tr>
<tr>
<td style="text-align: left;">FaceNet</td>
<td style="text-align: center;">0.711</td>
<td style="text-align: center;">66.42%</td>
<td style="text-align: center;">69.86%</td>
<td style="text-align: center;">36.33%</td>
<td style="text-align: center;">0.649</td>
</tr>
<tr>
<td style="text-align: left;">MagFace</td>
<td style="text-align: center;">0.561</td>
<td style="text-align: center;">52.50%</td>
<td style="text-align: center;">79.43%</td>
<td style="text-align: center;">69.02%</td>
<td style="text-align: center;">0.598</td>
</tr>
</tbody>
</table>
</div>
<p>SENet substantially outperforms all other video embeddings, achieving
an AUROC of 0.914. Notably, the face recognition embeddings (ArcFace,
FaceNet, MagFace) perform poorly for deepfake detection, which is
expected as these models are optimized for identity discrimination
rather than authenticity detection, and they may encode
identity-specific features that are preserved across real and fake
versions of the same subject.</p>
<p>Table <a href="#tab:video-divergence" data-reference-type="ref"
data-reference="tab:video-divergence">3.4</a> reports distributional
divergence metrics for video embeddings.</p>
<div id="tab:video-divergence">
<table>
<caption>Distribution divergence between real and fake samples for video
embeddings. Higher values indicate greater separability.</caption>
<thead>
<tr>
<th style="text-align: left;"><strong>Embedding</strong></th>
<th style="text-align: center;"><strong>KL Divergence</strong></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SENet</td>
<td style="text-align: center;">2.69</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Marlin</td>
<td style="text-align: center;"><strong>3.03</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">FaceNet</td>
<td style="text-align: center;">2.03</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ArcFace</td>
<td style="text-align: center;">1.09</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">MagFace</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
</div>
<p>While Marlin achieves the highest KL divergence (3.03), SENet has
substantially higher classification performance, so it is selected as
the video embedding.</p>
<h3 id="sec:repr-quality">Representation Quality Analysis</h3>
<p>To evaluate the quality of the learned authenticity representations
without relying on a downstream classifier, we employ a suite of
unsupervised metrics that assess the geometric structure and intrinsic
separability of the embedding space.</p>
<h4 id="clustering-metric-adjusted-mutual-information-ami">Clustering
Metric: Adjusted Mutual Information (AMI)</h4>
<p>K-means clustering is applied to the authenticity embeddings <span
class="math inline">\(z^{\text{auth}}\)</span>, and AMI is computed to
measure agreement between cluster assignments and real/fake labels,
corrected for chance. High AMI values indicate that real and fake
samples naturally separate into distinct clusters, suggesting that
authenticity is encoded as an intrinsic property of the embedding
space.</p>
<h4 id="distribution-metric-kl-divergence">Distribution Metric: KL
Divergence</h4>
<p>Real and fake embeddings are projected onto the
distance-to-real-centroid statistic, and KL divergence between the
resulting distributions is computed. Higher KL divergence indicates
greater distributional separation and reflects the effectiveness of the
variance minimization objective in creating a distinguishable real
manifold.</p>
<p><span class="math display">\[\begin{equation}
D_{\text{KL}}(P \| Q) = \int p(x) \log \frac{p(x)}{q(x)} \, dx
\end{equation}\]</span></p>
<h4 id="separation-metric-mean-cosine-similarity">Separation Metric:
Mean Cosine Similarity</h4>
<p>We compute cosine similarity between each embedding and both the real
and fake centroids. Key statistics include real-to-real, fake-to-real,
real-to-fake, and fake-to-fake similarities. The separation gap—defined
as the difference between real-to-real and fake-to-real
similarity—quantifies how well real samples form a compact cluster from
which fake samples deviate. For each sample <span
class="math inline">\(i\)</span> with embedding <span
class="math inline">\(z_i^{\text{auth}}\)</span> and centroid <span
class="math inline">\(\mu_c\)</span>, the cosine similarity is: <span
class="math display">\[\begin{equation}
\text{sim}(z_i^{\text{auth}}, \mu_c) = \frac{z_i^{\text{auth}} \cdot
\mu_c}{\|z_i^{\text{auth}}\| \|\mu_c\|}
\end{equation}\]</span></p>
<h4 id="local-content-group-metrics">Local Content-Group Metrics</h4>
<p>To assess disentanglement consistency within individual videos
(content groups), we evaluate two metrics: (i) intra-group cosine
similarity in <span class="math inline">\(z^{\text{id}}\)</span>,
measuring whether identity features remain invariant across
augmentations, and (ii) intra-group variance in <span
class="math inline">\(z^{\text{auth}}\)</span> for real and fake
augmentations, with the variance ratio capturing the relative
consistency of authenticity cues.</p>
<h4 id="regularization-strategy-comparison">Regularization Strategy
Comparison</h4>
<p>Three variance-floor regularization regimes (conservative, moderate,
aggressive) are compared using this complete metric suite to evaluate
collapse prevention, real–fake separability, and training stability.</p>
<h3 id="sec:regularization-strategies">Regularization Strategy
Comparison</h3>
<p>To systematically evaluate the effectiveness of variance floor
regularization and determine optimal hyperparameter settings, three
training paradigms are designed that span the spectrum from weak to
strong regularization enforcement. These paradigms test whether the
regularization mechanism successfully prevents the representation
collapse observed in preliminary experiments (Section <a
href="#sec:interpretation" data-reference-type="ref"
data-reference="sec:interpretation">3.6.1</a>), and which hyperparameter
configuration achieves the best balance between preventing collapse and
maintaining real/fake separability.</p>
<h4 id="training-paradigms">Training Paradigms</h4>
<p>Three regularization strategies are defined by varying the variance
floor threshold <span class="math inline">\(\tau\)</span> and
regularization weight <span
class="math inline">\(\lambda_{\text{reg}}\)</span> in Equation <a
href="#eq:variance_loss_reg" data-reference-type="ref"
data-reference="eq:variance_loss_reg">[eq:variance_loss_reg]</a>:</p>
<h5 id="conservative-regularization">Conservative Regularization</h5>
<p>(<span class="math inline">\(\tau = 0.1\)</span>, <span
class="math inline">\(\lambda_{\text{reg}} = 1.0\)</span>) allows
aggressive compression of real embeddings with minimal resistance
against collapse. This weak enforcement strategy tests whether light
regularization suffices to prevent the degenerate behavior where all
embeddings collapse to a single point despite satisfying the variance
objective. This approach is hypothesized to still exhibit collapse, as
the low variance floor and weak penalty provide insufficient constraint
on the optimization.</p>
<h5 id="moderate-regularization">Moderate Regularization</h5>
<p>(<span class="math inline">\(\tau = 0.2\)</span>, <span
class="math inline">\(\lambda_{\text{reg}} = 2.0\)</span>) enforces a
moderate variance floor with moderate penalty strength. The higher
threshold (<span class="math inline">\(\tau = 0.2\)</span> vs. 0.1)
requires real embeddings to maintain more spread, while the doubled
regularization weight (<span class="math inline">\(\lambda_{\text{reg}}
= 2.0\)</span>) increases the cost of violating this constraint. This
balanced approach is hypothesized to prevent collapse while allowing
sufficient compression for effective anomaly detection, striking an
optimal trade-off between the competing objectives.</p>
<h5 id="aggressive-regularization">Aggressive Regularization</h5>
<p>(<span class="math inline">\(\tau = 0.5\)</span>, <span
class="math inline">\(\lambda_{\text{reg}} = 5.0\)</span>) imposes a
high variance floor with severe penalties for violations. While this
should reliably prevent collapse, it risks overregularization—forcing
real embeddings to remain spread even when tighter clustering would
improve separation from fake samples. The concern is that aggressive
regularization could artificially inflate the real manifold, reducing
the margin between real and fake distributions and potentially harming
out-of-distribution generalization.</p>
<h4 id="evaluation-protocol">Evaluation Protocol</h4>
<p>Each paradigm is trained for 50 epochs on identical data splits
(AVDeepfake-1M++ and ShareVeo3 as described in Section <a
href="#sec:datasets" data-reference-type="ref"
data-reference="sec:datasets">3.4.1</a>), using the loss formulation in
Equation <a href="#eq:total_loss" data-reference-type="ref"
data-reference="eq:total_loss">[eq:total_loss]</a> with
paradigm-specific <span class="math inline">\(\tau\)</span> and <span
class="math inline">\(\lambda_{\text{reg}}\)</span> values. All other
hyperparameters remain fixed across paradigms: batch size of 256,
learning rate of <span class="math inline">\(1 \times 10^{-4}\)</span>
with cosine annealing, and balanced batching to ensure equal
representation of real and fake samples.</p>
<p>Evaluation occurs at each epoch, where the full suite of unsupervised
representation quality metrics described in Section <a
href="#sec:repr-quality" data-reference-type="ref"
data-reference="sec:repr-quality">3.4.3</a> is computed on 2 evaluation
sets: (1) Mixed AVDeepfake-1M++ and  ShareVeo3 validation split
(in-distribution real and fake) and (2) Sora2 (out-of-distribution
synthetic).</p>
<p>Paradigms are compared on three criteria:</p>
<ol>
<li><p><strong>Collapse Prevention:</strong> Does the approach prevent
the representation collapse observed in preliminary experiments? This is
measured through Wasserstein distance between real and fake
distributions (should remain <span class="math inline">\(&gt;
0.1\)</span>), variance trajectory of real embeddings over training
(should stabilize above <span class="math inline">\(\tau\)</span>), and
mean distance to real centroid (should not approach zero for both
classes).</p></li>
<li><p><strong>Class Separability:</strong> Does the approach maintain
meaningful separation between real and fake samples? This is assessed
through AMI and ARI scores (should increase or remain stable),
silhouette coefficient with ground truth labels (should be positive),
and KL/JS divergence between real and fake distributions (should
increase).</p></li>
<li><p><strong>Training Stability:</strong> What are the training
dynamics and gradient flow characteristics? Loss curves for <span
class="math inline">\(\mathcal{L}_{\text{proto}}\)</span>, <span
class="math inline">\(\mathcal{L}_{\text{var}}\)</span>, and <span
class="math inline">\(\mathcal{L}_{\text{orth}}\)</span> are tracked to
verify all objectives decrease without plateauing or oscillating, and
whether the variance loss remains active (non-zero gradient) throughout
training is monitored.</p></li>
</ol>
<p>The central research questions addressed are: (1) Does variance floor
regularization successfully prevent collapse while maintaining
separability? (2) Which regularization strength
(conservative/moderate/aggressive) achieves optimal performance? (3)
What insights does this provide for multi-objective optimization in
representation learning, particularly for anomaly detection frameworks
that rely on variance minimization?</p>
<h2 id="sec:results">Results</h2>
<p>The analysis is structured per embedding model (HuBERT, OpenL3,
SENet) and explores how varying the regularization strength
(Conservative, Moderate, Aggressive) impacts the learned
representations. For each model, the <strong>In-Distribution (ID)
Analysis</strong> on the training data is first presented, followed by
the <strong>Out-of-Distribution (OOD) Analysis</strong> on
Sora2-generated content to evaluate generalization.</p>
<h3 id="sec:hubert-repr-results">HuBERT Audio Embeddings</h3>
<h5 id="in-distribution-analysis">In-Distribution Analysis</h5>
<p>Table <a href="#tab:repr-id-comparison" data-reference-type="ref"
data-reference="tab:repr-id-comparison">[tab:repr-id-comparison]</a>
presents geometric and clustering metrics on the validation set.</p>
<p>All projected schemes exhibit <strong>representation
collapse</strong>, with Wasserstein distance dropping <span
class="math inline">\(&gt;99\%\)</span> (from <span
class="math inline">\(0.533\)</span> to <span
class="math inline">\(\approx0.004\)</span>). However, the schemes
differ in their preservation of authenticity structure.</p>
<p>The <strong>Aggressive</strong> scheme (<span
class="math inline">\(\tau=0.5\)</span>) outperforms the baseline,
improving AMI by <span class="math inline">\(8\%\)</span> (<span
class="math inline">\(0.120\)</span>) and ARI by <span
class="math inline">\(35\%\)</span> (<span
class="math inline">\(0.089\)</span>). Unlike the baseline, it achieves
a positive Separation Gap (<span
class="math inline">\(+0.0010\)</span>), confirming correct centroid
alignment despite collapse. Conversely, <strong>Conservative</strong>
and <strong>Moderate</strong> schemes degrade label alignment. Notably,
Moderate fails to achieve a positive Separation Gap (<span
class="math inline">\(-0.0023\)</span>), indicating incorrect centroid
orientation.</p>
<p>Results confirm the <strong>Silhouette Paradox</strong>: K-means
Silhouette peaks with the most collapsed representations (Conservative:
<span class="math inline">\(0.575\)</span>), inversely to label
alignment. This indicates that collapse creates tight clusters unrelated
to authenticity. Crucially, the Aggressive scheme maintains <span
class="math inline">\(5\times\)</span> higher intra-group variance
(Real: <span class="math inline">\(0.212\)</span>) than Conservative,
suggesting the higher variance floor successfully resists complete
collapse.</p>
<figure id="fig:global-hubert-comparison" data-latex-placement="t!">
<div class="minipage">
<img
src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/senet/baseline/global_tsne_all_datasets_senet_baseline.png" />
</div>
<div class="minipage">
<img
src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/hubert/conservative/global_tsne_all_datasets_hubert_conservative.png" />
</div>
<div class="minipage">
<img
src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/hubert/moderate/global_tsne_all_datasets_hubert_moderate.png" />
</div>
<div class="minipage">
<img
src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/hubert/aggressive/global_tsne_all_datasets_hubert_aggressive.png" />
</div>
<figcaption>Global t-SNE comparison. The Original baseline (a) shows
broad dispersion. The Aggressive scheme (d) maintains the most spread
among projected schemes while achieving better authenticity
alignment.</figcaption>
</figure>
<figure id="fig:per-video-hubert-comparison" data-latex-placement="t!">
<div class="minipage">
<img
src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/hubert/baseline/exp1_hubert_baseline_gdg4mUSwuhI_00002_seg14.png"
style="width:90.0%" />
</div>
<div class="minipage">
<img
src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/hubert/conservative/exp1_hubert_conservative_gdg4mUSwuhI_00002_seg14.png"
style="width:90.0%" />
</div>
<div class="minipage">
<img
src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/hubert/moderate/exp1_hubert_moderate_gdg4mUSwuhI_00002_seg14.png"
style="width:90.0%" />
</div>
<div class="minipage">
<img
src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/hubert/aggressive/exp1_hubert_aggressive_gdg4mUSwuhI_00002_seg14.png"
style="width:90.0%" />
</div>
<figcaption>Per-video analysis: (left) PC1/PC2 scatter, (center)
authenticity score scatter, (right) cosine similarity to source. The
distance-to-source metric (right) demonstrates improved differentiation
between real (green) and fake (red) augmentations under stronger
regularization.</figcaption>
</figure>
<h5 id="out-of-distribution-analysis">Out-of-Distribution Analysis</h5>
<p>Table <a href="#tab:repr-ood-comparison" data-reference-type="ref"
data-reference="tab:repr-ood-comparison">3.5</a> evaluates
generalization by comparing embeddings of in-distribution real samples
(AVDeepfake1M++) against out-of-distribution synthetic samples from
Sora2.</p>
<div id="tab:repr-ood-comparison">
<table>
<caption>Out-of-Distribution Representation Metrics Comparison (HuBERT).
Metrics compare ID Real samples against OOD Sora2 samples. <span
class="math inline">\(\uparrow\)</span> indicates higher is better for
generalization.</caption>
<thead>
<tr>
<th style="text-align: left;"><strong>Metric</strong></th>
<th style="text-align: center;"><strong>Original</strong></th>
<th style="text-align: center;"><strong>Conservative</strong></th>
<th style="text-align: center;"><strong>Moderate</strong></th>
<th style="text-align: center;"><strong>Aggressive</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="5" style="text-align: left;"><em>Clustering
Metrics</em></td>
</tr>
<tr>
<td style="text-align: left;">AMI <span
class="math inline">\(\uparrow\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(0.016\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(0.017\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(0.017\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(\mathbf{0.018}\)</span></td>
</tr>
<tr>
<td style="text-align: left;">ARI <span
class="math inline">\(\uparrow\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(0.004\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(-0.001\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(-0.001\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(\mathbf{0.007}\)</span></td>
</tr>
<tr>
<td style="text-align: left;">Silhouette (GT) <span
class="math inline">\(\uparrow\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(-0.025\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(-0.002\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(-0.009\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(\mathbf{+0.001}\)</span></td>
</tr>
<tr>
<td colspan="5" style="text-align: left;"><em>Distribution and
Separation Metrics</em></td>
</tr>
<tr>
<td style="text-align: left;">Separation Gap <span
class="math inline">\(\uparrow\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(-0.020\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(\mathbf{+0.0001}\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(-0.0022\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(+0.00005\)</span></td>
</tr>
<tr>
<td style="text-align: left;">Wasserstein Distance <span
class="math inline">\(\uparrow\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(\mathbf{0.219}\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(0.001\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(0.004\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(0.002\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>The OOD analysis reveals that the <strong>Aggressive</strong> scheme
achieves the best generalization to unseen Sora2 content across all
clustering metrics. It is the only scheme to achieve positive values for
both ARI (<span class="math inline">\(+0.007\)</span> vs. <span
class="math inline">\(+0.004\)</span> for Original) and ground-truth
Silhouette (<span class="math inline">\(+0.001\)</span> vs. <span
class="math inline">\(-0.025\)</span> for Original). This represents a
qualitative shift: while the Original embeddings and
Conservative/Moderate schemes produce negative Silhouette scores
(indicating that Sora2 samples are geometrically closer to real samples
than to each other), the Aggressive scheme produces positive Silhouette,
suggesting meaningful geometric separation between ID real and OOD
synthetic content.</p>
<p>The Separation Gap results reveal an important nuance. Both
Conservative (<span class="math inline">\(+0.0001\)</span>) and
Aggressive (<span class="math inline">\(+0.00005\)</span>) achieve
positive gaps, indicating correct centroid orientation where Sora2
content is positioned farther from the real centroid than real samples.
However, the Moderate scheme fails to generalize, producing a negative
Separation Gap (<span class="math inline">\(-0.0022\)</span>) that
mirrors its in-distribution failure. This suggests that the Moderate
configuration (<span class="math inline">\(\tau=0.2\)</span>, <span
class="math inline">\(\lambda_{\text{reg}}=2.0\)</span>) occupies an
unstable region of the hyperparameter space where neither sufficient
collapse prevention nor effective centroid alignment is achieved.</p>
<p>Despite these improvements in geometric structure, all schemes suffer
from severe Wasserstein collapse in the OOD setting (<span
class="math inline">\(0.219 \rightarrow 0.001\)</span>–<span
class="math inline">\(0.004\)</span>), indicating that the
distributional spread necessary for robust anomaly detection is not
preserved. The positive Separation Gaps and improved Silhouette scores
suggest that the <em>direction</em> of OOD separation is learned
correctly, but the <em>magnitude</em> of separation remains insufficient
for high-confidence detection of novel generation methods.</p>
<p>The Aggressive scheme’s superior OOD performance, combined with its
best-in-class ID metrics, suggests that higher variance floors are
essential for both preserving authenticity structure and enabling
generalization. However, the persistent Wasserstein collapse across all
schemes indicates that variance floor regularization alone is
insufficient to fully prevent the degenerate optimization behavior,
motivating the alternative loss formulations discussed in Section <a
href="#sec:interpretation" data-reference-type="ref"
data-reference="sec:interpretation">3.6.1</a>.</p>
<h3 id="sec:openl3-repr-results">OpenL3 Audio Embeddings</h3>
<h5 id="in-distribution-analysis-1">In-Distribution Analysis</h5>
<p>OpenL3 embeddings exhibit markedly different baseline characteristics
compared to HuBERT. The Original OpenL3 embeddings achieve substantially
higher ground-truth Silhouette (<span
class="math inline">\(0.321\)</span> vs. HuBERT’s <span
class="math inline">\(0.033\)</span>) and a positive baseline Separation
Gap (<span class="math inline">\(+0.0096\)</span> vs. HuBERT’s <span
class="math inline">\(-0.013\)</span>), indicating that OpenL3 already
encodes authenticity-relevant structure prior to disentanglement
training. The extremely high K-means Silhouette (<span
class="math inline">\(0.937\)</span>) suggests that OpenL3 embeddings
naturally form tight clusters, though the low AMI/ARI indicate these
clusters do not align with authenticity labels.</p>
<p>Despite these favorable baseline properties, all three regularization
schemes induce severe <strong>representation collapse</strong>. The
Wasserstein distance drops from <span
class="math inline">\(3.413\)</span> to <span
class="math inline">\(0.010\)</span>–<span
class="math inline">\(0.018\)</span> (<span
class="math inline">\(\downarrow 99.5\%\)</span>), and the ground-truth
Silhouette decreases from <span class="math inline">\(0.321\)</span> to
<span class="math inline">\(0.029\)</span>–<span
class="math inline">\(0.059\)</span> (<span
class="math inline">\(\downarrow 82\)</span>–<span
class="math inline">\(91\%\)</span>). This collapse is more dramatic
than observed with HuBERT, likely because OpenL3’s higher initial
variance provides more room for compression.</p>
<p>The <strong>Moderate</strong> scheme (<span
class="math inline">\(\tau=0.2\)</span>, <span
class="math inline">\(\lambda_{\text{reg}}=2.0\)</span>) achieves the
best label-alignment metrics, with AMI of <span
class="math inline">\(0.081\)</span> and ARI of <span
class="math inline">\(0.088\)</span>—representing <span
class="math inline">\(238\%\)</span> and <span
class="math inline">\(120\%\)</span> improvements over the Original
baseline respectively. Moderate also achieves the highest positive
Separation Gap (<span class="math inline">\(+0.0102\)</span>), slightly
exceeding even the Original (<span
class="math inline">\(+0.0096\)</span>). This contrasts with HuBERT,
where the Aggressive scheme performed best, suggesting that optimal
regularization strength is embedding-dependent.</p>
<p>The <strong>Aggressive</strong> scheme (<span
class="math inline">\(\tau=0.5\)</span>, <span
class="math inline">\(\lambda_{\text{reg}}=5.0\)</span>) shows degraded
performance relative to Moderate, with AMI dropping to <span
class="math inline">\(0.042\)</span> and ARI to <span
class="math inline">\(0.031\)</span>—both below even the Conservative
scheme. However, Aggressive maintains the highest preserved variance
(Real: <span class="math inline">\(0.215\)</span>, Fake: <span
class="math inline">\(0.315\)</span>) and K-means Silhouette (<span
class="math inline">\(0.535\)</span>), suggesting that excessive
variance regularization prevents the model from learning discriminative
structure. The <strong>Conservative</strong> scheme fails to achieve a
positive Separation Gap (<span class="math inline">\(-0.0001\)</span>),
indicating incorrect centroid orientation despite acceptable
label-alignment metrics.</p>
<figure id="fig:openl3-global-comparison" data-latex-placement="h">
<div class="minipage">
<img
src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/openl3/baseline/global_tsne_all_datasets_openl3_baseline.png" />
</div>
<div class="minipage">
<img
src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/openl3/conservative/global_tsne_all_datasets_openl3_conservative.png" />
</div>
<div class="minipage">
<img
src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/openl3/moderate/global_tsne_all_datasets_openl3_moderate.png" />
</div>
<div class="minipage">
<img
src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/openl3/aggressive/global_tsne_all_datasets_openl3_aggressive.png" />
</div>
<figcaption>Visualization of the global embedding space for OpenL3 using
t-SNE. The Original baseline (a) exhibits tighter clustering than HuBERT
due to OpenL3’s higher intrinsic similarity structure (K-means
Silhouette <span class="math inline">\(0.937\)</span>). The projected
schemes (b–d) are expected to show progressive collapse, with Moderate
(c) achieving the best balance between compression and authenticity
separation.</figcaption>
</figure>
<figure id="fig:openl3-per-video-comparison" data-latex-placement="h">
<div class="minipage">
<img
src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/openl3/baseline/exp1_openl3_baseline_gdg4mUSwuhI_00002_seg14.png"
style="width:90.0%" />
</div>
<div class="minipage">
<img
src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/openl3/conservative/exp1_openl3_conservative_gdg4mUSwuhI_00002_seg14.png"
style="width:90.0%" />
</div>
<div class="minipage">
<img
src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/openl3/moderate/exp1_openl3_moderate_gdg4mUSwuhI_00002_seg14.png"
style="width:90.0%" />
</div>
<div class="minipage">
<img
src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/openl3/aggressive/exp1_openl3_aggressive_gdg4mUSwuhI_00002_seg14.png"
style="width:90.0%" />
</div>
<figcaption>Per-video analysis for the projected OpenL3 embeddings
across regularization schemes. Given OpenL3’s superior baseline
separation, the per-video plots should show clearer differentiation
between real and fake augmentations in the Original baseline compared to
HuBERT. The Moderate scheme (c) is expected to maintain this
differentiation while the Conservative (b) and Aggressive (d) schemes
may show degraded structure.</figcaption>
</figure>
<h5 id="out-of-distribution-analysis-1">Out-of-Distribution
Analysis</h5>
<div id="tab:repr-ood-comparison-openl3">
<table>
<caption>Out-of-Distribution Representation Metrics Comparison (OpenL3).
Metrics compare ID Real samples against OOD Sora2 samples. <span
class="math inline">\(\uparrow\)</span> indicates higher is better for
generalization.</caption>
<thead>
<tr>
<th style="text-align: left;"><strong>Metric</strong></th>
<th style="text-align: center;"><strong>Original</strong></th>
<th style="text-align: center;"><strong>Conservative</strong></th>
<th style="text-align: center;"><strong>Moderate</strong></th>
<th style="text-align: center;"><strong>Aggressive</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="5" style="text-align: left;"><em>Clustering
Metrics</em></td>
</tr>
<tr>
<td style="text-align: left;">AMI <span
class="math inline">\(\uparrow\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(0.005\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(\mathbf{0.019}\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(\mathbf{0.020}\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(0.010\)</span></td>
</tr>
<tr>
<td style="text-align: left;">ARI <span
class="math inline">\(\uparrow\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(-0.020\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(0.015\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(\mathbf{0.021}\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(0.006\)</span></td>
</tr>
<tr>
<td style="text-align: left;">Silhouette (GT) <span
class="math inline">\(\uparrow\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(\mathbf{+0.029}\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(-0.048\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(-0.040\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(-0.036\)</span></td>
</tr>
<tr>
<td colspan="5" style="text-align: left;"><em>Distribution and
Separation Metrics</em></td>
</tr>
<tr>
<td style="text-align: left;">Separation Gap <span
class="math inline">\(\uparrow\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(\mathbf{+0.0002}\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(-0.0055\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(-0.0110\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(-0.0273\)</span></td>
</tr>
<tr>
<td style="text-align: left;">Wasserstein Distance <span
class="math inline">\(\uparrow\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(\mathbf{0.808}\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(0.017\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(0.023\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(0.028\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>The OOD analysis reveals a critical limitation of the disentanglement
framework when applied to OpenL3 embeddings: <strong>all three
regularization schemes degrade OOD generalization</strong> relative to
the Original baseline. The Original OpenL3 embeddings achieve a positive
Separation Gap (<span class="math inline">\(+0.0002\)</span>) and
positive ground-truth Silhouette (<span
class="math inline">\(+0.029\)</span>) on Sora2 content, indicating that
untrained OpenL3 embeddings already possess some capacity to distinguish
novel synthetic audio from authentic content.</p>
<p>After disentanglement training, all schemes produce <strong>negative
Separation Gaps</strong> (Conservative: <span
class="math inline">\(-0.0055\)</span>, Moderate: <span
class="math inline">\(-0.0110\)</span>, Aggressive: <span
class="math inline">\(-0.0273\)</span>), indicating that Sora2 content
is now positioned <em>closer</em> to the real centroid than real
samples—the opposite of the intended behavior. This inversion worsens
monotonically with regularization strength, suggesting that stronger
variance floors exacerbate the OOD failure mode.</p>
<p>The ground-truth Silhouette scores similarly degrade from <span
class="math inline">\(+0.029\)</span> (Original) to <span
class="math inline">\(-0.036\)</span> to <span
class="math inline">\(-0.048\)</span> across the projected schemes.
Notably, while the label-alignment metrics (AMI, ARI) improve for the
projected schemes relative to Original, this improvement reflects better
clustering of ID content rather than improved OOD detection. The
Moderate scheme achieves the best AMI (<span
class="math inline">\(0.020\)</span>) and ARI (<span
class="math inline">\(0.021\)</span>), but its negative Separation Gap
indicates this clustering does not generalize to unseen generation
methods.</p>
<p>This pattern contrasts sharply with HuBERT, where the Conservative
and Aggressive schemes achieved positive OOD Separation Gaps despite
similar collapse patterns. The difference may stem from OpenL3’s
stronger baseline OOD performance: because OpenL3 already encodes some
generalization-relevant structure, the disentanglement training
<em>overwrites</em> this information with ID-specific features that fail
to transfer. HuBERT’s weaker baseline may paradoxically benefit from
disentanglement training by providing a “blank slate” that can learn
more generalizable representations.</p>
<p>These results suggest that disentanglement training on OpenL3
embeddings optimizes for ID separation at the cost of OOD
generalization. Future work should explore regularization strategies
that explicitly preserve baseline OOD structure while improving ID
discrimination, such as knowledge distillation from the Original
embeddings or OOD-aware training objectives.</p>
<h3 id="sec:senet-repr-results">SENet Visual Embeddings</h3>
<h5 id="in-distribution-analysis-2">In-Distribution Analysis</h5>
<p>Table <a href="#tab:repr-id-comparison-senet"
data-reference-type="ref"
data-reference="tab:repr-id-comparison-senet">[tab:repr-id-comparison-senet]</a>
reports the core geometric and clustering metrics for SENet visual
embeddings on the combined AVDeepfake1M++ and ShareVeo3 validation
set.</p>
<p>SENet visual embeddings exhibit fundamentally different baseline
characteristics compared to both audio embedding types. The Original
SENet embeddings possess a substantially higher Wasserstein distance
(<span class="math inline">\(6.759\)</span> vs. HuBERT’s <span
class="math inline">\(0.533\)</span> and OpenL3’s <span
class="math inline">\(3.413\)</span>) and a strongly positive Separation
Gap (<span class="math inline">\(+0.081\)</span>), indicating that
untrained SENet embeddings already encode significant
authenticity-relevant structure with correctly-oriented class centroids.
The high intra-group variances (Real: <span
class="math inline">\(3659.8\)</span>, Fake: <span
class="math inline">\(5010.7\)</span>) reflect the greater
dimensionality and heterogeneity of visual features compared to
audio.</p>
<p>All three regularization schemes induce severe <strong>representation
collapse</strong>, with Wasserstein distance dropping from <span
class="math inline">\(6.759\)</span> to <span
class="math inline">\(0.003\)</span>–<span
class="math inline">\(0.015\)</span> (<span
class="math inline">\(\downarrow 99.8\%\)</span>). More critically, all
schemes <strong>destroy the baseline’s positive Separation Gap</strong>,
producing negative values (Conservative: <span
class="math inline">\(-0.0048\)</span>, Moderate: <span
class="math inline">\(-0.0062\)</span>, Aggressive: <span
class="math inline">\(-0.0015\)</span>). This represents a qualitative
failure: the disentanglement training <em>inverts</em> the centroid
structure, positioning fake samples closer to the real centroid than
real samples—the opposite of the intended behavior and worse than the
untrained baseline.</p>
<p>Despite this centroid inversion, the <strong>Aggressive</strong>
scheme (<span class="math inline">\(\tau=0.5\)</span>, <span
class="math inline">\(\lambda_{\text{reg}}=5.0\)</span>) achieves
remarkable improvements in label-alignment metrics. AMI increases from
<span class="math inline">\(0.141\)</span> to <span
class="math inline">\(0.146\)</span> (<span
class="math inline">\(\uparrow 4\%\)</span>) and ARI improves
dramatically from <span class="math inline">\(0.053\)</span> to <span
class="math inline">\(0.158\)</span> (<span
class="math inline">\(\uparrow 198\%\)</span>). This paradoxical
result—improved clustering despite inverted centroids—suggests that the
Aggressive scheme learns discriminative structure that is orthogonal to
the prototype-based separation objective. The projected embeddings may
capture authenticity-relevant features that enable clustering without
conforming to the intended centroid geometry.</p>
<p>The Conservative and Moderate schemes show degraded AMI (<span
class="math inline">\(0.088\)</span> and <span
class="math inline">\(0.073\)</span> respectively) compared to both the
Original baseline and the Aggressive scheme, while achieving
intermediate ARI values (<span class="math inline">\(0.089\)</span> and
<span class="math inline">\(0.085\)</span>). The inverse relationship
between regularization strength and K-means Silhouette (Conservative:
<span class="math inline">\(0.516\)</span>, Moderate: <span
class="math inline">\(0.452\)</span>, Aggressive: <span
class="math inline">\(0.400\)</span>) mirrors the pattern observed in
HuBERT and OpenL3, confirming that weaker regularization produces
tighter but less authenticity-aligned clusters.</p>
<figure id="fig:senet-global-comparison" data-latex-placement="h">
<div class="minipage">
<img
src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/senet/baseline/global_tsne_all_datasets_senet_baseline.png" />
</div>
<div class="minipage">
<img
src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/senet/conservative/global_tsne_all_datasets_senet_conservative.png" />
</div>
<div class="minipage">
<img
src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/senet/moderate/global_tsne_all_datasets_senet_moderate.png" />
</div>
<div class="minipage">
<img
src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/senet/aggressive/global_tsne_all_datasets_senet_aggressive.png" />
</div>
<figcaption>Visualization of the global embedding space for SENet using
t-SNE. The Original baseline (a) exhibits the highest spread among all
embedding types due to SENet’s high-dimensional visual features. The
projected schemes (b–d) are expected to show dramatic collapse, with
Aggressive (d) maintaining slightly more spread while achieving the best
label-alignment despite inverted centroid structure.</figcaption>
</figure>
<figure id="fig:senet-per-video-comparison" data-latex-placement="h">
<div class="minipage">
<img
src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/senet/baseline/exp1_senet_baseline_gdg4mUSwuhI_00002_seg14.png"
style="width:90.0%" />
</div>
<div class="minipage">
<img
src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/senet/conservative/exp1_senet_conservative_gdg4mUSwuhI_00002_seg14.png"
style="width:90.0%" />
</div>
<div class="minipage">
<img
src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/senet/moderate/exp1_senet_moderate_gdg4mUSwuhI_00002_seg14.png"
style="width:90.0%" />
</div>
<div class="minipage">
<img
src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/senet/aggressive/exp1_senet_aggressive_gdg4mUSwuhI_00002_seg14.png"
style="width:90.0%" />
</div>
<figcaption>Per-video analysis for the projected SENet embeddings across
regularization schemes. Given SENet’s strong baseline separation (<span
class="math inline">\(+0.081\)</span> Separation Gap), the Original
baseline (a) should show clear differentiation between real and fake
augmentations in the distance-to-source bar chart. The projected schemes
(b–d) may show reduced or inverted differentiation, reflecting the
negative Separation Gaps observed in the quantitative
metrics.</figcaption>
</figure>
<h5 id="out-of-distribution-analysis-2">Out-of-Distribution
Analysis</h5>
<div id="tab:repr-ood-comparison-senet">
<table>
<caption>Out-of-Distribution Representation Metrics Comparison (SENet).
Metrics compare ID Real samples against OOD Sora2 samples. <span
class="math inline">\(\uparrow\)</span> indicates higher is better for
generalization.</caption>
<thead>
<tr>
<th style="text-align: left;"><strong>Metric</strong></th>
<th style="text-align: center;"><strong>Original</strong></th>
<th style="text-align: center;"><strong>Conservative</strong></th>
<th style="text-align: center;"><strong>Moderate</strong></th>
<th style="text-align: center;"><strong>Aggressive</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="5" style="text-align: left;"><em>Clustering
Metrics</em></td>
</tr>
<tr>
<td style="text-align: left;">AMI <span
class="math inline">\(\uparrow\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(\mathbf{0.039}\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(0.023\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(0.015\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(0.029\)</span></td>
</tr>
<tr>
<td style="text-align: left;">ARI <span
class="math inline">\(\uparrow\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(-0.010\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(0.023\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(0.020\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(\mathbf{0.043}\)</span></td>
</tr>
<tr>
<td style="text-align: left;">Silhouette (GT) <span
class="math inline">\(\uparrow\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(\mathbf{+0.115}\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(-0.050\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(-0.030\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(-0.015\)</span></td>
</tr>
<tr>
<td colspan="5" style="text-align: left;"><em>Distribution and
Separation Metrics</em></td>
</tr>
<tr>
<td style="text-align: left;">Separation Gap <span
class="math inline">\(\uparrow\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(\mathbf{+0.074}\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(-0.0047\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(-0.0067\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(-0.0029\)</span></td>
</tr>
<tr>
<td style="text-align: left;">Wasserstein Distance <span
class="math inline">\(\uparrow\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(\mathbf{7.012}\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(0.015\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(0.014\)</span></td>
<td style="text-align: center;"><span
class="math inline">\(0.004\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>The OOD analysis reveals that SENet exhibits the strongest baseline
generalization to Sora2 content among all embedding types, but this
generalization is <strong>completely destroyed</strong> by
disentanglement training. The Original SENet embeddings achieve a
Separation Gap of <span class="math inline">\(+0.074\)</span> and
ground-truth Silhouette of <span class="math inline">\(+0.115\)</span>
on Sora2 content—substantially higher than both HuBERT (Gap: <span
class="math inline">\(-0.020\)</span>, Silhouette: <span
class="math inline">\(-0.025\)</span>) and OpenL3 (Gap: <span
class="math inline">\(+0.0002\)</span>, Silhouette: <span
class="math inline">\(+0.029\)</span>). This suggests that visual
features naturally encode artifacts that distinguish Sora2-generated
video from authentic content, consistent with Sora2’s primary function
as a video generation model.</p>
<p>All three regularization schemes invert the OOD Separation Gap to
negative values (Conservative: <span
class="math inline">\(-0.0047\)</span>, Moderate: <span
class="math inline">\(-0.0067\)</span>, Aggressive: <span
class="math inline">\(-0.0029\)</span>), indicating that Sora2 content
is now positioned <em>closer</em> to the real centroid than real
samples. The ground-truth Silhouette scores similarly degrade from <span
class="math inline">\(+0.115\)</span> to negative values ranging from
<span class="math inline">\(-0.015\)</span> to <span
class="math inline">\(-0.050\)</span>. This pattern mirrors the ID
results, confirming that the centroid inversion observed in-distribution
extends to OOD content.</p>
<p>The Aggressive scheme achieves the best OOD ARI (<span
class="math inline">\(0.043\)</span>) among projected schemes—and
notably exceeds even the Original baseline (<span
class="math inline">\(-0.010\)</span>)—suggesting that despite incorrect
centroid orientation, the learned representations contain
OOD-discriminative structure recoverable by clustering. However, the
negative Separation Gap indicates this structure does not conform to the
intended anomaly detection framework where novel synthetic content
should be positioned farther from the real prototype.</p>
<p>The severity of OOD degradation for SENet (<span
class="math inline">\(+0.074 \rightarrow -0.0029\)</span> to <span
class="math inline">\(-0.0067\)</span>) exceeds that observed for OpenL3
(<span class="math inline">\(+0.0002 \rightarrow -0.0055\)</span> to
<span class="math inline">\(-0.0273\)</span>), suggesting that visual
embeddings are particularly susceptible to losing generalization
capacity during disentanglement training. This may reflect the higher
initial quality of SENet’s OOD structure: embeddings with stronger
baseline generalization have more to lose when subjected to ID-focused
optimization. These results motivate future work on regularization
strategies that explicitly preserve OOD separation during training, such
as incorporating Sora2 or other held-out synthetic content into the
training objective as negative examples.</p>
<h2 id="discussion">Discussion</h2>
<h3 id="sec:interpretation">Interpretation of Results</h3>
<p>The experiments across HuBERT, OpenL3, and SENet embeddings reveal
systematic patterns in how variance-based disentanglement interacts with
different pretrained representations. While all three embedding types
exhibit representation collapse under the disentanglement framework, the
nature of this collapse and its impact on detection performance varies
substantially across modalities and regularization strengths.</p>
<h4 id="cross-embedding-comparison">Cross-Embedding Comparison</h4>
<p>The three embedding types exhibit fundamentally different baseline
characteristics that shape their response to disentanglement
training:</p>
<h5 id="hubert-audio-embeddings.">HuBERT Audio Embeddings.</h5>
<p>HuBERT begins with the weakest baseline structure: negative
Separation Gap (<span class="math inline">\(-0.013\)</span>), low
ground-truth Silhouette (<span class="math inline">\(0.033\)</span>),
and moderate distributional spread (Wasserstein <span
class="math inline">\(0.533\)</span>). This “blank slate” property
proves advantageous—the Aggressive regularization scheme achieves the
best overall results, improving AMI from <span
class="math inline">\(0.111\)</span> to <span
class="math inline">\(0.120\)</span> (<span
class="math inline">\(\uparrow 8\%\)</span>) and ARI from <span
class="math inline">\(0.066\)</span> to <span
class="math inline">\(0.089\)</span> (<span
class="math inline">\(\uparrow 35\%\)</span>). Critically, both
Conservative and Aggressive schemes achieve positive Separation Gaps on
OOD Sora2 content (<span class="math inline">\(+0.0001\)</span> and
<span class="math inline">\(+0.00005\)</span> respectively),
demonstrating transfer of learned authenticity structure to unseen
generation methods.</p>
<h5 id="openl3-audio-embeddings.">OpenL3 Audio Embeddings.</h5>
<p>OpenL3 exhibits strong baseline structure: positive Separation Gap
(<span class="math inline">\(+0.0096\)</span>), high ground-truth
Silhouette (<span class="math inline">\(0.321\)</span>), and substantial
distributional spread (Wasserstein <span
class="math inline">\(3.413\)</span>). The Moderate scheme achieves best
in-distribution performance (AMI <span
class="math inline">\(0.081\)</span>, ARI <span
class="math inline">\(0.088\)</span>—improvements of <span
class="math inline">\(238\%\)</span> and <span
class="math inline">\(120\%\)</span> over baseline). However, all three
regularization schemes <em>degrade</em> OOD generalization, with
Separation Gaps inverting from <span
class="math inline">\(+0.0002\)</span> (Original) to negative values
ranging from <span class="math inline">\(-0.0055\)</span> to <span
class="math inline">\(-0.0273\)</span>. This suggests that OpenL3’s
baseline OOD structure is overwritten by ID-specific features during
training.</p>
<h5 id="senet-visual-embeddings.">SENet Visual Embeddings.</h5>
<p>SENet demonstrates the strongest baseline generalization: Separation
Gap of <span class="math inline">\(+0.081\)</span> and ground-truth
Silhouette of <span class="math inline">\(+0.115\)</span> on Sora2
content—substantially exceeding both audio embeddings. The Aggressive
scheme achieves remarkable label-alignment improvements (ARI: <span
class="math inline">\(0.053 \rightarrow 0.158\)</span>, <span
class="math inline">\(\uparrow 198\%\)</span>), yet all schemes invert
the Separation Gap to negative values. This represents a qualitative
failure: the disentanglement training destroys SENet’s inherent capacity
to distinguish novel synthetic content.</p>
<h4 id="the-collapse-separation-trade-off">The Collapse-Separation
Trade-off</h4>
<p>All embedding types exhibit severe representation collapse, with
Wasserstein distances decreasing by <span
class="math inline">\(99\%\)</span> or more across all regularization
schemes (Table <a href="#tab:collapse-summary" data-reference-type="ref"
data-reference="tab:collapse-summary">3.8</a>). However, the
relationship between collapse and separation varies systematically with
regularization strength.</p>
<div id="tab:collapse-summary">
<table>
<caption>Wasserstein distance collapse across embedding types and
regularization schemes. All schemes show <span
class="math inline">\(&gt;99\%\)</span> reduction from
baseline.</caption>
<thead>
<tr>
<th style="text-align: left;"><strong>Embedding</strong></th>
<th style="text-align: center;"><strong>Original</strong></th>
<th style="text-align: center;"><strong>Conservative</strong></th>
<th style="text-align: center;"><strong>Moderate</strong></th>
<th style="text-align: center;"><strong>Aggressive</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">HuBERT</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">0.005</td>
<td style="text-align: center;">0.004</td>
</tr>
<tr>
<td style="text-align: left;">OpenL3</td>
<td style="text-align: center;">3.413</td>
<td style="text-align: center;">0.010</td>
<td style="text-align: center;">0.018</td>
<td style="text-align: center;">0.012</td>
</tr>
<tr>
<td style="text-align: left;">SENet</td>
<td style="text-align: center;">6.759</td>
<td style="text-align: center;">0.015</td>
<td style="text-align: center;">0.013</td>
<td style="text-align: center;">0.003</td>
</tr>
</tbody>
</table>
</div>
<p>The key finding is that collapse and separation are partially
decoupled: collapsed representations can still maintain
correctly-oriented centroid structure. For HuBERT, the transition from
negative to positive Separation Gap (Conservative: <span
class="math inline">\(+0.0008\)</span>, Aggressive: <span
class="math inline">\(+0.0010\)</span>) occurs despite <span
class="math inline">\(99.5\%\)</span> Wasserstein collapse. The variance
minimization objective successfully learns the <em>direction</em> of
authenticity separation but fails to preserve the <em>magnitude</em> of
distributional spread necessary for robust discrimination.</p>
<h4 id="the-silhouette-paradox">The Silhouette Paradox</h4>
<p>A consistent pattern emerges across all embedding types: K-means
Silhouette scores <em>increase</em> with collapse while label-alignment
metrics often <em>decrease</em>. Table <a href="#tab:silhouette-paradox"
data-reference-type="ref"
data-reference="tab:silhouette-paradox">3.9</a> illustrates this
divergence.</p>
<div id="tab:silhouette-paradox">
<table>
<caption>The Silhouette Paradox: K-means Silhouette increases while
label-alignment metrics (AMI/ARI) show variable response. HuBERT data
shown; similar patterns observed for OpenL3 and SENet.</caption>
<thead>
<tr>
<th style="text-align: left;"><strong>Metric</strong></th>
<th style="text-align: center;"><strong>Original</strong></th>
<th style="text-align: center;"><strong>Conservative</strong></th>
<th style="text-align: center;"><strong>Moderate</strong></th>
<th style="text-align: center;"><strong>Aggressive</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">K-means Silhouette <span
class="math inline">\(\uparrow\)</span></td>
<td style="text-align: center;">0.274</td>
<td style="text-align: center;">0.575</td>
<td style="text-align: center;">0.502</td>
<td style="text-align: center;">0.425</td>
</tr>
<tr>
<td style="text-align: left;">Ground-truth Silhouette</td>
<td style="text-align: center;">0.033</td>
<td style="text-align: center;">0.066</td>
<td style="text-align: center;">0.054</td>
<td style="text-align: center;">0.062</td>
</tr>
<tr>
<td style="text-align: left;">AMI</td>
<td style="text-align: center;">0.111</td>
<td style="text-align: center;">0.104</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;"><strong>0.120</strong></td>
</tr>
<tr>
<td style="text-align: left;">ARI</td>
<td style="text-align: center;">0.066</td>
<td style="text-align: center;">0.033</td>
<td style="text-align: center;">0.032</td>
<td style="text-align: center;"><strong>0.089</strong></td>
</tr>
</tbody>
</table>
</div>
<p>This divergence occurs because representation collapse concentrates
all embeddings into a compact region where K-means identifies clusters
based on residual variations <em>unrelated to authenticity</em>. The
inverse relationship between K-means Silhouette and regularization
strength (Conservative: <span class="math inline">\(0.575\)</span> <span
class="math inline">\(&gt;\)</span> Moderate: <span
class="math inline">\(0.502\)</span> <span
class="math inline">\(&gt;\)</span> Aggressive: <span
class="math inline">\(0.425\)</span>) confirms that weaker
regularization produces tighter but less authenticity-aligned
clusters.</p>
<p>The paradox illuminates a fundamental challenge: unsupervised
clustering metrics can improve precisely because collapse eliminates the
variance that would otherwise enable authenticity discrimination.
Effective disentanglement requires maintaining sufficient geometric
spread for clustering algorithms to recover label structure, not merely
achieving tight clusters.</p>
<h4
id="regularization-strength-and-optimal-configuration">Regularization
Strength and Optimal Configuration</h4>
<p>The experiments reveal that optimal regularization strength is
<strong>embedding-dependent</strong>:</p>
<ul>
<li><p><strong>HuBERT</strong>: Aggressive (<span
class="math inline">\(\tau = 0.5\)</span>, <span
class="math inline">\(\lambda_{\text{reg}} = 5.0\)</span>) achieves best
ID and OOD performance</p></li>
<li><p><strong>OpenL3</strong>: Moderate (<span
class="math inline">\(\tau = 0.2\)</span>, <span
class="math inline">\(\lambda_{\text{reg}} = 2.0\)</span>) achieves best
ID performance; all schemes degrade OOD</p></li>
<li><p><strong>SENet</strong>: Aggressive achieves best label-alignment
but all schemes invert Separation Gap</p></li>
</ul>
<p>For HuBERT, the Aggressive scheme maintains <span
class="math inline">\(5\times\)</span> higher intra-group variance
(Real: <span class="math inline">\(0.212\)</span>, Fake: <span
class="math inline">\(0.299\)</span>) compared to Conservative (Real:
<span class="math inline">\(0.042\)</span>, Fake: <span
class="math inline">\(0.058\)</span>), suggesting that higher variance
floors successfully resist complete collapse. This preserved variance
correlates with superior label-alignment performance.</p>
<p>The Moderate scheme occupies an unstable region of the hyperparameter
space for HuBERT, failing to achieve positive Separation Gap (<span
class="math inline">\(-0.0023\)</span>) despite intermediate
regularization strength. This non-monotonic behavior suggests complex
interactions between the variance floor, prototype loss, and
orthogonality constraint that merit further investigation.</p>
<h4 id="ood-generalization-a-critical-divergence">OOD Generalization: A
Critical Divergence</h4>
<p>The out-of-distribution analysis on Sora2 content reveals the most
consequential finding: <strong>disentanglement training can destroy
baseline OOD generalization</strong>. Table <a href="#tab:ood-summary"
data-reference-type="ref" data-reference="tab:ood-summary">3.10</a>
summarizes the impact.</p>
<div id="tab:ood-summary">
<table>
<caption>OOD Generalization Impact. Positive values indicate correct
separation (fake samples farther from real centroid than real samples).
Values in <strong>bold</strong> indicate improvement over Original;
values in <span style="color: red">red</span> indicate
degradation.</caption>
<thead>
<tr>
<th style="text-align: left;"><strong>Embedding</strong></th>
<th style="text-align: center;"><strong>Original</strong></th>
<th style="text-align: center;"><strong>Conservative</strong></th>
<th style="text-align: center;"><strong>Moderate</strong></th>
<th style="text-align: center;"><strong>Aggressive</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">HuBERT</td>
<td style="text-align: center;"><span
class="math inline">\(-0.020\)</span></td>
<td style="text-align: center;"><strong><span
class="math inline">\(+0.0001\)</span></strong></td>
<td style="text-align: center;"><span style="color: red"><span
class="math inline">\(-0.0022\)</span></span></td>
<td style="text-align: center;"><strong><span
class="math inline">\(+0.00005\)</span></strong></td>
</tr>
<tr>
<td style="text-align: left;">OpenL3</td>
<td style="text-align: center;"><span
class="math inline">\(+0.0002\)</span></td>
<td style="text-align: center;"><span style="color: red"><span
class="math inline">\(-0.0055\)</span></span></td>
<td style="text-align: center;"><span style="color: red"><span
class="math inline">\(-0.0110\)</span></span></td>
<td style="text-align: center;"><span style="color: red"><span
class="math inline">\(-0.0273\)</span></span></td>
</tr>
<tr>
<td style="text-align: left;">SENet</td>
<td style="text-align: center;"><span
class="math inline">\(+0.074\)</span></td>
<td style="text-align: center;"><span style="color: red"><span
class="math inline">\(-0.0047\)</span></span></td>
<td style="text-align: center;"><span style="color: red"><span
class="math inline">\(-0.0067\)</span></span></td>
<td style="text-align: center;"><span style="color: red"><span
class="math inline">\(-0.0029\)</span></span></td>
</tr>
</tbody>
</table>
</div>
<p>Two distinct patterns emerge:</p>
<h5
id="pattern-1-baseline-deficient-embeddings-benefit-from-training-hubert.">Pattern
1: Baseline-deficient embeddings benefit from training (HuBERT).</h5>
<p>HuBERT’s negative baseline Separation Gap (<span
class="math inline">\(-0.020\)</span>) indicates that untrained
embeddings cannot distinguish Sora2 audio from authentic audio.
Disentanglement training creates positive separation structure that
transfers to OOD content, despite extreme collapse. The positive OOD
Separation Gaps, though small in magnitude, represent a qualitative
improvement from random to correct class orientation.</p>
<h5
id="pattern-2-baseline-rich-embeddings-suffer-from-training-openl3-senet.">Pattern
2: Baseline-rich embeddings suffer from training (OpenL3, SENet).</h5>
<p>Both OpenL3 and SENet possess positive baseline OOD Separation Gaps
that are systematically destroyed by disentanglement training. The
severity of degradation correlates with baseline OOD quality: SENet’s
Separation Gap drops from <span class="math inline">\(+0.074\)</span> to
approximately <span class="math inline">\(-0.005\)</span> (a sign flip
representing complete inversion), while OpenL3’s drops from <span
class="math inline">\(+0.0002\)</span> to as low as <span
class="math inline">\(-0.0273\)</span>.</p>
<p>This pattern suggests that embeddings with stronger baseline OOD
structure have “more to lose” when subjected to ID-focused optimization.
The disentanglement framework optimizes for ID separation at the cost of
OOD generalization, effectively overwriting the generalization-relevant
features encoded in pretrained representations.</p>
<h4 id="per-video-analysis-insights">Per-Video Analysis Insights</h4>
<p>The per-video visualizations (Figures <a href="#fig:hubert-pervideo"
data-reference-type="ref"
data-reference="fig:hubert-pervideo">[fig:hubert-pervideo]</a>–<a
href="#fig:senet-pervideo" data-reference-type="ref"
data-reference="fig:senet-pervideo">[fig:senet-pervideo]</a>) provide
granular insight into how regularization affects individual content
groups. Key observations include:</p>
<h5 id="distance-to-source-differentiation.">Distance-to-Source
Differentiation.</h5>
<p>The rightmost panels show cosine similarity between augmentations and
their source embedding. In the Original baselines, real augmentations
(green bars) consistently show higher similarity to the source than fake
augmentations (red bars), confirming that untrained embeddings naturally
separate real from fake within content groups. After training, this
differentiation diminishes as collapse compresses all embeddings—both
real and fake—closer together.</p>
<p>For HuBERT (Figure <a href="#fig:hubert-pervideo"
data-reference-type="ref"
data-reference="fig:hubert-pervideo">[fig:hubert-pervideo]</a>), the
Original baseline shows clear separation (Mean Real: <span
class="math inline">\(0.832\)</span>, Mean Fake: <span
class="math inline">\(0.181\)</span>), which compresses substantially
under all regularization schemes (Mean Real: <span
class="math inline">\(\sim 0.93\)</span>–<span
class="math inline">\(0.96\)</span>, Mean Fake: <span
class="math inline">\(\sim 0.83\)</span>–<span
class="math inline">\(0.84\)</span>). The gap narrows from <span
class="math inline">\(\Delta = 0.651\)</span> to <span
class="math inline">\(\Delta \approx 0.10\)</span>–<span
class="math inline">\(0.13\)</span>, reflecting the representation
collapse observed in global metrics.</p>
<h5 id="pca-structure-evolution.">PCA Structure Evolution.</h5>
<p>The left and center panels reveal how embedding space geometry
changes with regularization. Original embeddings show dispersed point
clouds with limited class structure. Progressive regularization
concentrates points into tighter clusters, but the color gradients
(authenticity scores) show that class separation within these clusters
varies by scheme. The Aggressive scheme typically maintains more spread
while achieving better alignment between cluster structure and
authenticity labels.</p>
<h5 id="cross-augmentation-consistency.">Cross-Augmentation
Consistency.</h5>
<p>A key validation of the disentanglement objective is whether
augmentations of the same source video cluster together in the identity
space (<span class="math inline">\(z^{\text{id}}\)</span>) regardless of
their authenticity label. The visualizations confirm that content groups
remain coherent across regularization schemes, suggesting the
prototypical contrastive loss successfully captures content invariance
even as the authenticity space collapses.</p>
<h3 id="diagnosis-of-failure-modes">Diagnosis of Failure Modes</h3>
<p>The comprehensive analysis identifies three primary failure modes
that limit the current framework’s effectiveness:</p>
<h4 id="failure-mode-1-variance-floor-insufficiency">Failure Mode 1:
Variance Floor Insufficiency</h4>
<p>Even the Aggressive regularization scheme (<span
class="math inline">\(\tau = 0.5\)</span>, <span
class="math inline">\(\lambda_{\text{reg}} = 5.0\)</span>) fails to
prevent <span class="math inline">\(&gt;99\%\)</span> Wasserstein
collapse. The variance floor regularization (Equation <a
href="#eq:var-loss" data-reference-type="ref"
data-reference="eq:var-loss">[eq:var-loss]</a>) is insufficient to
counteract the compressive forces of the prototype loss and
orthogonality constraint, which together encourage all embeddings to
concentrate in a small region.</p>
<p>The design of the variance floor targets the <em>real</em>
distribution only, penalizing collapse of authentic samples while
allowing fake samples to collapse freely. This asymmetric formulation
may be fundamentally flawed: if fake samples collapse toward the real
centroid (satisfying the variance objective while destroying
separation), the detector fails despite technically achieving its
training objective.</p>
<h4 id="failure-mode-2-objective-misalignment">Failure Mode 2: Objective
Misalignment</h4>
<p>The multi-objective optimization combines three losses with
potentially conflicting gradients:</p>
<ol>
<li><p><strong>Prototype loss</strong> (<span
class="math inline">\(\mathcal{L}_{\text{proto}}\)</span>): Pulls
content groups together, encouraging compression</p></li>
<li><p><strong>Variance loss</strong> (<span
class="math inline">\(\mathcal{L}_{\text{var}}\)</span>): Minimizes
spread of real samples, encouraging compression</p></li>
<li><p><strong>Orthogonality constraint</strong> (<span
class="math inline">\(\mathcal{L}_{\text{orth}}\)</span>): Decorrelates
identity and authenticity spaces</p></li>
</ol>
<p>The first two objectives both encourage embedding compression, with
only the variance floor providing countervailing pressure. The
optimization finds a degenerate solution that satisfies all objectives:
collapse all embeddings to a small region (satisfying <span
class="math inline">\(\mathcal{L}_{\text{proto}}\)</span> and <span
class="math inline">\(\mathcal{L}_{\text{var}}\)</span>) where the
identity and authenticity projections are trivially orthogonal
(satisfying <span
class="math inline">\(\mathcal{L}_{\text{orth}}\)</span>).</p>
<h4 id="failure-mode-3-ood-structure-overwriting">Failure Mode 3: OOD
Structure Overwriting</h4>
<p>For embeddings with strong baseline OOD generalization (OpenL3,
SENet), disentanglement training systematically destroys this structure.
The training objective contains no explicit regularization to preserve
baseline OOD performance, allowing the optimization to freely overwrite
generalization-relevant features with ID-specific discriminators.</p>
<p>This failure mode is particularly concerning because it suggests a
fundamental tension between ID optimization and OOD generalization in
the current framework. Achieving better ID separation may
<em>require</em> sacrificing the broad, generalizable features that
enable OOD detection.</p>
<h3 id="potential-remediation-strategies">Potential Remediation
Strategies</h3>
<p>Based on the identified failure modes, several architectural and
optimization modifications may address representation collapse while
preserving OOD generalization:</p>
<h4 id="contrastive-authenticity-loss">Contrastive Authenticity
Loss</h4>
<p>Replace variance minimization with a supervised contrastive
objective <span class="citation" data-cites="Khosla2020"></span> that
explicitly requires separation between real and fake samples: <span
class="math display">\[\begin{equation}
\mathcal{L}_{\text{auth}} = -\frac{1}{|P(i)|} \sum_{p \in P(i)} \log
\frac{\exp(z_i^{\text{auth}} \cdot z_p^{\text{auth}} / \tau)}{\sum_{a
\in A(i)} \exp(z_i^{\text{auth}} \cdot z_a^{\text{auth}} / \tau)}
\end{equation}\]</span> where <span class="math inline">\(P(i)\)</span>
is the set of samples with the same authenticity label as anchor <span
class="math inline">\(i\)</span>. This formulation directly optimizes
for class separation rather than relying on implicit separation through
variance differences.</p>
<h4 id="bidirectional-variance-regularization">Bidirectional Variance
Regularization</h4>
<p>Extend the variance floor to <em>both</em> real and fake samples:
<span class="math display">\[\begin{equation}
\mathcal{L}_{\text{var}}^{\text{bi}} = \lambda_{\text{reg}} \left[
\max(0, \tau - \sigma_{\text{real}}^2) + \max(0, \tau -
\sigma_{\text{fake}}^2) \right]
\end{equation}\]</span> This prevents the degenerate solution where fake
samples collapse toward the real centroid while satisfying the original
variance objective.</p>
<h4 id="ood-preservation-regularization">OOD Preservation
Regularization</h4>
<p>Introduce a knowledge distillation term that preserves baseline OOD
structure: <span class="math display">\[\begin{equation}
\mathcal{L}_{\text{preserve}} = \| z^{\text{auth}} -
\text{sg}(z^{\text{original}}) \|_2^2
\end{equation}\]</span> where <span
class="math inline">\(\text{sg}(\cdot)\)</span> denotes stop-gradient.
This regularization penalizes deviation from the original embedding
space, preserving generalization-relevant features while allowing
supervised refinement.</p>
<h4 id="margin-based-prototype-learning">Margin-Based Prototype
Learning</h4>
<p>Replace soft prototype assignment with hard margins that enforce
minimum distances between class centroids: <span
class="math display">\[\begin{equation}
\mathcal{L}_{\text{margin}} = \max(0, m - \| \mu_{\text{real}} -
\mu_{\text{fake}} \|_2)
\end{equation}\]</span> This prevents the centroid structure from
collapsing even as individual embeddings concentrate, maintaining the
geometric separation necessary for detection.</p>
<h4 id="staged-training">Staged Training</h4>
<p>Pretrain the authenticity head on a binary classification objective
to establish initial separation, then introduce disentanglement losses
with the separation structure as an anchor. This two-stage approach
prevents the optimization from finding degenerate collapsed solutions by
starting from a non-collapsed initialization.</p>
<h3 id="broader-implications">Broader Implications</h3>
<p>The systematic failure of variance-based disentanglement across
multiple embedding types suggests fundamental limitations of this
approach for deepfake detection. The core assumption—that authentic
content shares intrinsic properties distinguishable from synthetic
content through variance minimization—may be valid, but the optimization
dynamics prevent recovery of this structure.</p>
<p>More broadly, these results highlight the challenge of
multi-objective representation learning when objectives can be trivially
satisfied through degenerate solutions. The Silhouette Paradox
demonstrates that standard clustering metrics can misleadingly indicate
success even when the learned representations fail to capture
task-relevant structure.</p>
<p>Future work in disentangled deepfake detection should prioritize: (1)
loss formulations that explicitly prevent collapse while encouraging
separation, (2) regularization strategies that preserve baseline OOD
generalization, and (3) evaluation frameworks that distinguish between
genuine representation quality and degenerate clustering behavior.</p>
<h3 id="sec:future-work">Future Directions</h3>
<p>The results, while not achieving the intended disentanglement,
provide valuable diagnostic information about the challenges of
multi-objective representation learning for deepfake detection. Several
extensions remain for future work.</p>
<h4 id="multimodal-representation-fusion">Multimodal Representation
Fusion</h4>
<p>Audio and video modalities capture complementary manipulation
artifacts with moderate correlation (<span class="math inline">\(r
\approx 0.46\)</span> between HuBERT and SENet predictions). Several
fusion strategies merit investigation:</p>
<h5 id="early-fusion.">Early Fusion.</h5>
<p>Concatenating audio and video embeddings before disentanglement would
allow dual projection heads to learn joint authenticity representations
capturing cross-modal inconsistencies—for instance, lip movements not
matching audio content.</p>
<h5 id="late-fusion.">Late Fusion.</h5>
<p>Training separate disentanglement models for each modality, then
combining authenticity embeddings (<span
class="math inline">\(z^{\text{auth}}_{\text{audio}}\)</span> and <span
class="math inline">\(z^{\text{auth}}_{\text{video}}\)</span>) at
classification, preserves modality-specific representations while
enabling joint decision-making.</p>
<h5 id="attention-based-fusion.">Attention-Based Fusion.</h5>
<p>Cross-modal attention could dynamically weight modality contributions
based on per-sample reliability, emphasizing whichever modality exhibits
clearer artifacts.</p>
<h4 id="alternative-loss-formulations">Alternative Loss
Formulations</h4>
<p>The identified failure modes motivate systematic exploration of
alternative training objectives:</p>
<ul>
<li><p><strong>VICReg-style regularization</strong> <span
class="citation" data-cites="bardes2021vicreg"></span>: Combining
variance, invariance, and covariance terms may better balance the
competing objectives</p></li>
<li><p><strong>Hyperspherical uniformity</strong> <span class="citation"
data-cites="wang2020understanding"></span>: Distributing embeddings
uniformly on a hypersphere prevents collapse while maintaining
separation</p></li>
<li><p><strong>Spectral contrastive learning</strong> <span
class="citation" data-cites="haochen2021provable"></span>: Operating on
the eigenspace of the similarity matrix may provide more stable
gradients</p></li>
</ul>
<h4 id="extended-evaluation">Extended Evaluation</h4>
<p>While Sora2 provides a challenging OOD test case, evaluation on
additional unseen generators (Runway Gen-3, Pika, etc.) would further
validate generalization claims. Additionally, ablation studies on the
robustness to video compression, which can mask or mimic deepfake
artifacts, is essential for practical deployment.</p>
<h1 id="conclusion">Conclusion</h1>
<h2 id="summary-of-contributions">Summary of Contributions</h2>
<p>This thesis addressed two complementary challenges in visual
deception detection, developing specialized approaches for textual
homoglyph attacks and synthetic media identification.</p>
<p>For homoglyph detection, we introduced Visually-Aligned Text
Embeddings (VA-TE), a contrastive learning framework that leverages
vision-language model text encoders to capture visual properties of
Unicode characters without requiring image rendering. Through
curriculum-based hard negative mining and a lightweight projection
architecture, VA-TE achieves 0.95 ROC-AUC as a standalone system. When
combined with complementary string-similarity features in an ensemble,
VA-TE attains state-of-the-art performance (0.98 ROC-AUC) while offering
substantial advantages in scalability, memory efficiency, and deployment
simplicity compared to prior image-based approaches. Building on recent
advances in embedding refinement <span class="citation"
data-cites="jha2025harnessinguniversalgeometryembeddings"></span>, this
approach marks an important step toward multi-modal representations that
integrate the visual characteristics of text, reflecting how humans read
and interpret it.</p>
<p>For deepfake detection, we proposed a disentangled representation
learning framework with dual projection heads and orthogonality
constraints, designed to separate authenticity-relevant features from
identity and content information. Our systematic evaluation across three
embedding types (HuBERT, OpenL3, SENet) and three regularization schemes
revealed both the promise and limitations of variance-based
disentanglement. While all configurations exhibited representation
collapse (<span class="math inline">\(&gt;99\%\)</span> Wasserstein
distance reduction), the Aggressive regularization scheme on HuBERT
embeddings achieved improved label-alignment (AMI: <span
class="math inline">\(0.111 \rightarrow 0.120\)</span>, ARI: <span
class="math inline">\(0.066 \rightarrow 0.089\)</span>) and positive
out-of-distribution Separation Gaps on Sora2 content. However, for
embeddings with strong baseline generalization (OpenL3, SENet),
disentanglement training systematically degraded OOD performance,
highlighting a fundamental tension between in-distribution optimization
and generalization preservation. Additionally, we collected and
annotated a novel out-of-distribution evaluation dataset comprising 150
videos (11,000+ segments) generated by OpenAI’s Sora 2, providing a
challenging benchmark for assessing generalization to state-of-the-art
generation methods.</p>
<h2 id="methodological-insights">Methodological Insights</h2>
<p>Several methodological insights emerged from this work that may
inform future research in representation learning for security
applications.</p>
<p>First, the success of VA-TE demonstrates that vision-language models
encode visual characteristics of text beyond semantic meaning, and that
these latent features can be effectively surfaced through targeted
fine-tuning. The pairwise contrastive loss proved consistently superior
to triplet, InfoNCE, and supervised contrastive alternatives, likely due
to its alignment with the SigLIP encoder’s original training
objective.</p>
<p>Second, our deepfake detection experiments revealed what we term the
<em>Silhouette Paradox</em>: K-means clustering metrics can improve
substantially (e.g., <span class="math inline">\(0.274 \rightarrow
0.575\)</span> for HuBERT) even as label-alignment metrics degrade. This
divergence occurs because representation collapse creates well-formed
clusters based on residual variations unrelated to authenticity. This
finding underscores the importance of evaluating learned representations
with label-aware metrics rather than relying solely on unsupervised
clustering quality.</p>
<p>Third, the multi-objective optimization underlying our
disentanglement framework proved susceptible to degenerate solutions.
The variance minimization objective, computed only on real samples,
allows fake samples to collapse toward the real centroid while
technically satisfying all training objectives. Notably, the optimal
regularization strength proved embedding-dependent: Aggressive
regularization (<span class="math inline">\(\tau = 0.5\)</span>)
performed best for HuBERT, while Moderate regularization (<span
class="math inline">\(\tau = 0.2\)</span>) achieved superior
in-distribution results for OpenL3. This finding suggests that
representation learning frameworks require careful hyperparameter tuning
matched to the characteristics of their input embeddings.</p>
<p>Fourth, we observed a critical pattern in OOD generalization:
embeddings with weak baseline structure (HuBERT, with negative baseline
Separation Gap) benefited from disentanglement training, while
embeddings with strong baseline structure (SENet, with <span
class="math inline">\(+0.074\)</span> Separation Gap on Sora2) were
harmed. This suggests that ID-focused optimization can overwrite the
generalizable features encoded in pretrained representations, motivating
future work on regularization strategies that explicitly preserve OOD
structure.</p>
<p>Fifth, both projects benefited from combining learned representations
with complementary features. For VA-TE, fusing visual embeddings with
string-based metrics yielded a 3-point improvement in ROC-AUC. This
pattern suggests that hybrid approaches leveraging multiple signal
modalities remain valuable even as learned representations improve.</p>
<h2 id="broader-impact">Broader Impact</h2>
<p>Visual deception poses growing threats to financial systems, identity
verification platforms, and information ecosystems. Homoglyph attacks
enable fraud at scale by exploiting the gap between human perception and
computational string matching, while synthetic media undermines the
trustworthiness of audio-visual content for authentication and
evidence.</p>
<p>The methods developed in this thesis contribute to defending against
these threats. VA-TE provides financial institutions with a scalable,
deployable solution for detecting fraudulent account names and domain
spoofing. The disentanglement framework, while requiring refinement to
address representation collapse, establishes diagnostic tools and
evaluation metrics that clarify the challenges of multi-objective
representation learning for deepfake detection. The identification of
the collapse-separation trade-off as the primary failure mode offers a
concrete target for future architectural improvements. More broadly, by
focusing on learning stable properties of authentic content rather than
chasing evolving adversarial signatures, both approaches embody a
defensive paradigm better suited to the rapid pace of generative AI
advancement.</p>
<p>We note that detection systems can also be misused—for instance, to
identify which synthetic content evades detection, thereby improving
adversarial attacks. Responsible deployment requires ongoing monitoring,
regular model updates, and integration with broader security protocols
rather than reliance on any single detection method.</p>
<h2 id="future-directions">Future Directions</h2>
<p>Several promising directions extend this work. For VA-TE, applying
the framework to non-Latin scripts (Chinese, Arabic, Cyrillic) and
integrating with OCR pipelines for document verification represent
natural next steps. The perceptual decoding extension proposed in
Section <a href="#sec:vate-future" data-reference-type="ref"
data-reference="sec:vate-future">2.6.3</a> offers a path toward
grounding text embeddings in low-level visual features.</p>
<p>For deepfake detection, the immediate priority is addressing
representation collapse through alternative loss formulations.
Supervised contrastive objectives that explicitly require separation
between real and fake samples, bidirectional variance regularization
that penalizes collapse of both distributions, and margin-based
prototype learning that enforces minimum centroid distances all merit
systematic evaluation. The strong baseline performance of SENet video
embeddings (0.914 AUROC, <span class="math inline">\(+0.074\)</span> OOD
Separation Gap) suggests that preserving, rather than overwriting,
pretrained generalization structure may be more effective than
aggressive representation learning.</p>
<p>Beyond addressing collapse, multimodal fusion strategies that combine
audio and video authenticity signals offer substantial potential. The
moderate correlation between HuBERT and SENet predictions (<span
class="math inline">\(r \approx 0.46\)</span>) indicates partially
independent information that joint models could exploit. Attention-based
fusion mechanisms that dynamically weight modality contributions based
on per-sample reliability represent a promising direction.</p>
<p>Evaluation on additional unseen generators (Runway Gen-3, Pika,
Kling) and across video compression levels will be essential for
validating generalization claims before deployment. The Sora2 dataset
introduced in this work provides one such benchmark, but broader
coverage of the rapidly evolving generative model landscape is
needed.</p>
<p>Finally, exploring unified architectures that address both textual
and media-level visual deception within a single framework represents a
longer-term opportunity, potentially leveraging shared principles of
authenticity representation learning across modalities.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Fuzzy Matching library, including Token Set Ratio, can
be found here:
<span>https://rapidfuzz.github.io/RapidFuzz/Usage/fuzz.html</span><a
href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>The python-Levenshtein and RapidFuzz’s TokenSetRatio
libraries are used; distances are computed on raw strings.<a
href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Documenation found at https://optuna.org/<a
href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
