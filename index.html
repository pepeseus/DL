<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>DL Final Project</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }

    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }

    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }

      h1 {
        font-size: 1.8em;
      }
    }

    @media print {
      html {
        background-color: white;
      }

      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }

      p,
      h2,
      h3 {
        orphans: 3;
        widows: 3;
      }

      h2,
      h3,
      h4 {
        page-break-after: avoid;
      }
    }

    p {
      margin: 1em 0;
    }

    a {
      color: #1a1a1a;
    }

    a:visited {
      color: #1a1a1a;
    }

    img {
      max-width: 100%;
    }

    svg {
      height: auto;
      max-width: 100%;
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
      margin-top: 1.4em;
    }

    h5,
    h6 {
      font-size: 1em;
      font-style: italic;
    }

    h6 {
      font-weight: normal;
    }

    ol,
    ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }

    li>ol,
    li>ul {
      margin-top: 0;
    }

    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }

    div.abstract {
      margin: 2em 2em 2em 2em;
      text-align: left;
      font-size: 85%;
    }

    div.abstract-title {
      font-weight: bold;
      text-align: center;
      padding: 0;
      margin-bottom: 0.5em;
    }

    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }

    pre {
      margin: 1em 0;
      overflow: auto;
    }

    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }

    .sourceCode {
      background-color: transparent;
      overflow: visible;
    }

    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }

    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }

    table caption {
      margin-bottom: 0.75em;
    }

    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }

    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }

    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }

    header {
      margin-bottom: 4em;
      text-align: center;
    }

    #TOC li {
      list-style: none;
    }

    #TOC ul {
      padding-left: 1.3em;
    }

    #TOC>ul {
      padding-left: 0;
    }

    #TOC a:not(:hover) {
      text-decoration: none;
    }

    code {
      white-space: pre-wrap;
    }

    span.smallcaps {
      font-variant: small-caps;
    }

    div.columns {
      display: flex;
      gap: min(4vw, 1.5em);
    }

    div.column {
      flex: auto;
      overflow-x: auto;
    }

    div.hanging-indent {
      margin-left: 1.5em;
      text-indent: -1.5em;
    }

    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class] {
      list-style: none;
    }

    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
</head>
<h1 id="learning-what-is-real-intrinsic-authenticity-detection-for-generalization-across-deepfake-methods">Learning
  What Is Real: Intrinsic Authenticity Detection for Generalization Across
  Deepfake Methods</h1>
<h2 id="introduction-2">Introduction</h2>
<p>Deepfake generation technology has been advancing at an unprecedented
  pace, posing significant threats to systems dependent on identity
  verification and public trust. Early deepfakes relied on simple
  face-swapping techniques using Generative Adversarial Networks (GANs),
  but modern diffusion models are now able to produce synthetic media with
  photorealistic quality that is increasingly difficult to distinguish
  from authentic content. This rapid evolution in generation capabilities
  and methods has exposed critical vulnerabilities in existing detection
  approaches.</p>
<p>Current deepfake detection methods face two fundamental limitations.
  First, many approaches train exclusively on synthetic data generated by
  a single model, learning to identify artifacts specific to that
  generation method [cite]. While this strategy achieves reasonable
  performance on data from within the training distribution, it
  drastically underperforms when confronted with novel generation
  techniques. The accelerating pace of advancement in this field poses a
  critical weakness for identifying authentic media. Second, alternative
  approaches attempt to leverage embeddings from various modalities,
  namely audio and video features, to distinguish real from synthetic
  media (cite). However, these learned representations predominantly
  encode content-related information rather than the subtle artifacts and
  inconsistencies that reliably indicate synthetic generation.</p>
<p>These limitations motivate the approach: disentangling
  authenticity-related features from those encoding identity and content
  within pretrained representations, isolating the signals most relevant
  for deepfake detection. By explicitly separating these feature types,
  the goal is to reduce the influence of extraneous information, which may
  be similar across both real and fake media, while amplifying
  detection-relevant signals that generalize across generation methods.
  The central hypothesis is that disentanglement will enable learned
  representations to cluster authentic and synthetic media into distinct
  regions of a latent space, achieving robust generalization to previously
  unseen generation techniques. This approach addresses both the
  overfitting problem of technique-specific detectors and the
  content-encoding limitation of existing representation-based methods,
  offering a more principled path toward reliable, generalizable deepfake
  detection.</p>
<h2 id="related-works">Related Works</h2>
<h3 id="deepfake-generation-so-far">Deepfake Generation So Far</h3>
<p>Deepfake generation has progressed rapidly from early face-swapping
  pipelines toward highly expressive multimodal synthesis systems. Initial
  methods relied on autoencoders and Generative Adversarial
  Network(GAN)-based architectures <span class="citation" data-cites="goodfellow2020a"></span>, which learned identity
  mappings
  through reconstruction losses and produced artifacts that were often
  detectable using pixel-level or frequency-based heuristics. More recent
  advances leverage diffusion models and large-scale text-to-video
  architectures, such as those powering modern systems like Sora, which
  model long-range temporal coherence and fine-grained photorealism<span class="citation"
    data-cites="babaei2025a"></span>. These approaches
  significantly reduce the low-level inconsistencies that earlier
  detectors relied on, making generation artifacts less stable and less
  predictable. As generative frameworks diverge in architecture, training
  data, and inference strategies, the space of possible deepfake
  signatures becomes more variable. This shift has made artifact-based
  detection fundamentally brittle: cues that signal synthesis for one
  model are often fundamentally different from those from another model.
  Understanding how the diversity of generative techniques influences
  downstream embedding structure is therefore essential for developing
  detection systems that generalize beyond the specific artifacts of a
  single generator.</p>
<h3 id="deepfake-detection-approaches">Deepfake Detection
  Approaches</h3>
<p>Existing deepfake detection research spans supervised,
  self-supervised, and multimodal strategies, yet most approaches share a
  central assumption: the fake distribution is sufficiently stable that
  learned discriminative boundaries will transfer to new content.
  Artifact-based methods seek inconsistencies in eye blinking, head
  motion, frequency spectra, or compression patterns, and while effective
  within their training distribution, they generalize poorly to newer or
  higher-fidelity generators <span class="citation" data-cites="matern2019a frank2020a li2018a"></span>
  Embedding-based
  approaches attempt to bypass brittle pixel-level cues by leveraging
  audio, visual, or joint audio-visual representations. Video models
  capture identity and motion patterns, while audio models encode speaker
  characteristics, prosody, and phonetic structure. However, these
  embeddings are optimized primarily for recognition tasks—such as speech
  transcription, speaker identification, or scene classification—rather
  than for modeling authenticity. As a result, they tend to encode content
  and identity far more strongly than generative irregularities, and so
  simple ensembles have been proved ineffective for this task <span class="citation"
    data-cites="khalid2021evaluation"></span>. Because
  deepfake models increasingly preserve high-level content while altering
  low-level generative processes, relying on embeddings not explicitly
  disentangled for authenticity introduces confounding factors that may
  obscure the subtle and cross-model-consistent cues necessary for
  generalization. This limitation motivates approaches that explicitly
  separate authenticity-related signals from identity and semantic
  content.</p>
<h3 id="disentangled-representation-learning">Disentangled
  Representation Learning</h3>
<p>REVISED</p>
<p>Disentangled representation learning seeks to separate latent factors
  of variation into orthogonal or minimally correlated subspaces, enabling
  models to isolate specific causal or semantic dimensions. Classical
  approaches include <span class="math inline">\(\beta\)</span>-VAE  <span class="citation"
    data-cites="higgins2017beta"></span> frameworks,
  contrastive learning variants[NEED CITATION], and supervised
  disentanglement [NEED CITATION] using attribute labels. More recently,
  multi-task learning and orthogonality-constrained projection methods
  have been used to prevent one objective from dominating representation
  space, as demonstrated in gradient-balancing strategies such as GradNorm
   <span class="citation" data-cites="chen2018gradnorm"></span>. For
  deepfake detection, disentanglement is appealing because authenticity
  should represent a low-variance, shared property across all real
  samples, while identity and content vary widely across individuals and
  contexts  <span class="citation" data-cites="liang2025a"></span>.
  Separating these components could allow detectors to operate on features
  that generalize across generative models without collapsing under
  distributional shifts. However, little prior work has explored
  disentanglement explicitly for deepfake detection, and existing
  embedding refinement techniques primarily target robustness or semantic
  alignment [NEED CITATION HERE]. This gap highlights the need for
  systematic evaluation of whether disentangling identity from
  authenticity can meaningfully improve generalization, particularly for
  unseen generators where artifact-based cues are unreliable.</p>
<h2 id="methods">Methods</h2>
<p>First, we define the problem of deepfake detection as a binary
  classification task, where the goal is to distinguish authentic from
  synthetic media. We use a dataset of authentic and synthetic audio–visual
  content, collected from a variety of sources, to train a deepfake
  detector. The core hypothesis of this work is that authentic audio–visual
  content lies on a relatively stable, low-variance manifold, whereas
  deepfakes, regardless of the generative model used, deviate from this
  structure. Conventional embeddings emphasize identity and
  content-relevant features because they are trained for tasks like speech
  recognition, speaker identification, or image classification. As a
  result, these embeddings often conflate identity information with
  authenticity information. To address this, the proposed framework learns
  two complementary subspaces:</p>
<p><span class="math inline">\(\textbf{Authenticity Subspace}\)</span>:
  Captures stable characteristics that real samples share and fake samples
  disrupt.</p>
<p><span class="math inline">\(\textbf{Identity/Content
    Subspace}\)</span>: Preserves information that varies between speakers,
  scenes, or utterances and should not influence authenticity
  decisions.</p>
<p>Given an embedding from a pretrained model <span class="math inline">\(z\)</span> [REFER TO CORRESPONDING SECTION],
  the
  framework maps this representation into to projected vectors</p>
<p><span class="math display">\[z^{\text{auth}} =
    f_{\text{auth}}(z)\]</span></p>
<p><span class="math display">\[z^{\text{id}} =
    f_{\text{id}}(z)\]</span></p>
<p>where <span class="math inline">\(f_{auth}\)</span> and <span class="math inline">\(f_{id}\)</span> are
  lightweight, two-layer
  projection heads. These heads are trained jointly to enforce structural
  separation between authenticity and identity components while
  maintaining useful representational geometry. This is motivated by
  recent work showing separation of complementary factor aids
  generalization in media forensics  <span class="citation" data-cites="liang2025a"></span>.</p>
<p>The learning framework is built around three objectives: (1) an
  authenticity-consistency objective, (2) an identity discrimination
  objective, and (3) an orthogonality constraint between subspaces. A
  loss-balancing mechanism regulates their interaction to prevent collapse
  and ensure stability across embedding types.</p>
<h3 id="sec:representation-learning">Training Objectives</h3>
<h5 id="orthogonality-constraint">Orthogonality Constraint</h5>
<p>Even with complementary objectives, the two projection heads may
  drift toward encoding correlated information. To enforce explicit
  disentanglement, the framework imposes an orthogonality regularizer:</p>
<p><span class="math display">\[\begin{equation}
    \mathcal{L}_{\text{orth}} = \frac{1}{N^2} \sum_{i=1}^{N}
    \sum_{j=1}^{N}
    \left| \text{sim}\left(z_i^{\text{id}}, z_j^{\text{auth}}\right)
    \right|
    \label{eq:orth}
    \end{equation}\]</span></p>
<p>where <span class="math inline">\(\text{sim}(\cdot, \cdot)\)</span>
  denotes cosine similarity. Penalizing cross-subspace dot products
  encourages the two projections to lie in approximately orthogonal
  directions within representation space. This decorrelation is critical
  for generalization: a model that disentangles these factors should not
  memorize identity-specific deepfake patterns (e.g., “person A is always
  fake in the training set”) but instead learn identity-agnostic
  authenticity cues.</p>
<h5 id="identity-learning-via-prototypical-contrastive-loss">Identity
  Learning via Prototypical Contrastive Loss</h5>
<p><br />
  To prevent the authenticity projection from inadvertently absorbing
  identity-related information, a complementary objective ensures that
  identity remains encoded separately. This mirrors the intuition that two
  real samples of different individuals should not appear artificially
  similar in authenticity space simply because the variance penalty
  compresses them. [PREVIEW]For a "content group" (augmentations of the
  same source video or frames from the same video), we compute the
  prototype as the mean embedding.</p>
<p><span class="math display">\[c_k = \frac{1}{|G_k|}\sum_{i \in
    G_k}z_i^{\text{id}}\]</span></p>
<p>where <span class="math inline">\(G_k\)</span> is the set of samples
  belonging to a content group <span class="math inline">\(k\)</span>.
  Then, the identity projection head is trained with a prototypical
  contrastive objective:</p>
<p><span class="math display">\[\begin{equation}
    \label{eq:proto_loss}
    \mathcal{L}_{\text{proto}} = -\frac{1}{N} \sum_{i=1}^{N} \log \left[
    \frac{\exp\left(-d(z^{\text{id}}_i, c_k) / \tau\right)}{\sum_{j}
    \exp\left(-d(z^{\text{id}}_i, c_j) / \tau\right)} \right]
    \end{equation}\]</span> where <span class="math inline">\(d(a,b)\)</span> is Euclidean distance, <span
    class="math inline">\(\tau\)</span> is a temperature hyperparameter, and
  the sum is over all prototypes in the batch. In addition, for each</p>
<p>where positive pairs share speaker or identity labels and negatives
  differ. This ensures that <span class="math inline">\(z_i\)</span>
  preserves rich, discriminative variation and prevents the authenticity
  head from learning content-based shortcuts that do not generalize.
  Together, the authenticity and identity objectives form a disentangled
  representation in which each subspace captures distinct, minimally
  interacting factors.</p>
<h5 id="authenticity-learning-via-variance-minimization">Authenticity
  Learning via Variance Minimization</h5>
<p>The authenticity head <span class="math inline">\(f_{\text{auth}}\)</span> is trained using an
  anomaly-detection principle: only real videos are used during
  optimization, encouraging their embeddings to form a compact cluster in
  the <span class="math inline">\(z^{\text{auth}}\)</span> space. Real
  samples share consistent authenticity cues (e.g., sensor noise and
  photometric coherence), whereas fake videos exhibit heterogeneous
  artifacts that depend on the generation method. Minimizing variance
  among real embeddings is therefore intended to form a compact "real
  manifold" that fake samples naturally deviate from.</p>
<p>[NEED CITATION]However, naïve variance minimization introduces a
  critical failure mode: the objective cannot distinguish a meaningful
  compact cluster from a degenerate collapsed solution in which all
  embeddings map to a single point. When combined with the prototypical
  identity loss, this collapse becomes even more likely, as both
  objectives can be jointly minimized by destroying class
  separability.</p>
<p>To prevent this, a <em>variance floor regularizer</em> is added:
  <span class="math display">\[\begin{equation}
    \label{eq:variance_loss_reg}
    \mathcal{L}_{\text{var}} =
    \frac{1}{N_{\text{real}}} \sum_{i : y_i = 1}
    \left\| z^{\text{auth}}_i - \mu_{\text{real}} \right\|^2
    + \lambda_{\text{reg}}
    \left[
    \left( \max(0, \tau - \sigma^2) \right)^2
    + 5 \cdot \max(0, \tau - \sigma^2)
    \right].
    \end{equation}\]</span>
</p>
<p>Here, <span class="math inline">\(\tau\)</span> sets the minimum
  allowable variance. The quadratic term stabilizes gradients near the
  threshold, while the linear term provides stronger penalties when
  variance collapses. This ensures real embeddings remain tightly
  clustered yet sufficiently spread to preserve structure and allow fake
  samples to diverge.</p>
<h5 id="joint-optimization">Joint Optimization</h5>
<p>The total loss for the representation learning stage combines three
  objectives: <span class="math display">\[\begin{equation}
    \label{eq:total_loss}
    \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{proto}} +
    \mathcal{L}_{\text{var}} + \lambda_{\text{orth}}
    \mathcal{L}_{\text{orth}}
    \end{equation}\]</span> where <span class="math inline">\(\lambda_{\text{orth}}\)</span> weights the
  orthogonality constraint. Each loss is normalized by its initial value
  to ensure similar magnitudes across objectives, then equal unit weights
  are applied (1.0 for prototype, 1.0 for variance, <span class="math inline">\(\lambda_{\text{orth}} = 0.1\)</span>
  for
  orthogonality). This normalization strategy proved more stable than
  gradient-based balancing methods <span class="citation" data-cites="chen2018gradnorm"></span> in preliminary
  experiments, as it
  avoids the interaction between adaptive weight scheduling and variance
  floor regularization.</p>
<p>[METHODS SECTION: 1244 WORDS AT THE MOMENT]</p>
<h2 id="experiments">Experiments</h2>
<h3 id="sec:datasets">Datasets</h3>
<p>The method is trained and evaluated on three complementary
  datasets—AVDeepfake-1M++ <span class="citation" data-cites="cai2025av"></span>, ShareVeo3  <span class="citation"
    data-cites="wang2024vidprom"></span>, and Sora2 <span class="citation" data-cites="sora2_2025"></span>—chosen to
  cover a broad spectrum of
  deepfake generation techniques. AVDeepfake-1M++ contains large-scale
  face-swap and voice-cloning manipulations applied to real source videos,
  while ShareVeo3 and OpenAI’s Sora2 represent fully synthetic,
  text-to-video diffusion models that generate entire videos from scratch.
  ShareVeo3 contributes additional diversity during training, and Sora2 is
  reserved exclusively for out-of-distribution evaluation, enabling
  assessment of generalization to a state-of-the-art generator unseen
  during training.</p>
<p>All videos are segmented into 0.15-second clips, from which
  pretrained encoders extract audio and video embeddings that are stored
  in a serverless PostgreSQL database. This granularity balances temporal
  resolution with computational efficiency. From AVDeepfake-1M++, the
  training subset includes 241 source videos and their augmented variants,
  producing 231,889 embeddings. The dataset provides two structural
  advantages: multiple perturbation-based augmentations per source,
  supporting identity learning, and partial fake injections with
  segment-level authenticity labels. After preprocessing, the subset
  contains 225,843 real, 2,394 fully fake, and 3,652 partial fake
  embeddings. Balanced batching and content-group definitions (all
  augmentations of the same source) support prototypical contrastive
  learning. ShareVeo3 contributes 63,184 fully fake embeddings from 1,460
  diffusion-generated videos. Although lacking augmentations, temporal
  continuity within each video forms content groups for identity modeling.
  Sora2 provides 11,317 fully synthetic embeddings for strict
  out-of-distribution evaluation. Its diverse human and non-human content
  enables testing whether the model captures intrinsic properties of
  authentic video rather than artifacts specific to a manipulation
  technique.</p>
<h3 id="sec:embedding-selection">Embedding Selection</h3>
<p>Before training the disentanglement model, a systematic evaluation is
  conducted to identify which pretrained audio and video embeddings
  provide the strongest baseline separability between real and fake
  samples. Candidate embeddings are evaluated using 5-fold
  cross-validation on a balanced subset of AVDeepfake1M++, training simple
  logistic regression classifiers on each embedding type independently.
  This analysis informs the choice of input representations for the full
  pipeline.</p>
<h4 id="audio-embeddings">Audio Embeddings</h4>
<p>Four audio embedding approaches are evaluated: OpenL3 <span class="citation" data-cites="cramer2019openl3"></span>,
  HuBERT <span class="citation" data-cites="hsu2021hubert"></span>, Wav2Vec2 <span class="citation"
    data-cites="baevski2020wav2vec"></span>, and
  Mel-Frequency Cepstral Coefficients (MFCC). Table <a href="#tab:audio-embed" data-reference-type="ref"
    data-reference="tab:audio-embed">3.1</a> reports classification
  performance for each embedding type.</p>
<div id="tab:audio-embed">
  <table>
    <caption>Audio embedding comparison for deepfake detection. Results
      averaged over 5-fold cross-validation.</caption>
    <thead>
      <tr>
        <th style="text-align: left;"><strong>Embedding</strong></th>
        <th style="text-align: center;"><strong>AUROC</strong></th>
        <th style="text-align: center;"><strong>Accuracy</strong></th>
        <th style="text-align: center;"><strong>Recall</strong></th>
        <th style="text-align: center;"><strong>FPR</strong></th>
        <th style="text-align: center;"><strong>F1</strong></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align: left;">OpenL3</td>
        <td style="text-align: center;"><strong>0.976</strong></td>
        <td style="text-align: center;"><strong>93.84%</strong></td>
        <td style="text-align: center;"><strong>93.78%</strong></td>
        <td style="text-align: center;"><strong>6.12%</strong></td>
        <td style="text-align: center;"><strong>0.931</strong></td>
      </tr>
      <tr>
        <td style="text-align: left;">HuBERT</td>
        <td style="text-align: center;">0.958</td>
        <td style="text-align: center;">90.75%</td>
        <td style="text-align: center;">87.08%</td>
        <td style="text-align: center;">6.31%</td>
        <td style="text-align: center;">0.893</td>
      </tr>
      <tr>
        <td style="text-align: left;">Wav2Vec2</td>
        <td style="text-align: center;">0.776</td>
        <td style="text-align: center;">74.92%</td>
        <td style="text-align: center;">65.31%</td>
        <td style="text-align: center;">17.40%</td>
        <td style="text-align: center;">0.698</td>
      </tr>
      <tr>
        <td style="text-align: left;">MFCC</td>
        <td style="text-align: center;">0.699</td>
        <td style="text-align: center;">66.10%</td>
        <td style="text-align: center;">54.31%</td>
        <td style="text-align: center;">24.47%</td>
        <td style="text-align: center;">0.587</td>
      </tr>
    </tbody>
  </table>
</div>
<p>OpenL3 achieves the strongest performance across all metrics, with an
  AUROC of 0.976 and F1 score of 0.931. However, HuBERT performs
  competitively, achieving 0.958 AUROC with a comparable false positive
  rate (6.31% vs 6.12%).</p>
<p>To further characterize the discriminative power of each embedding,
  distributional divergence metrics between real and fake samples are
  computed. Table <a href="#tab:audio-divergence" data-reference-type="ref"
    data-reference="tab:audio-divergence">3.2</a>
  reports KL divergence for each embedding type.</p>
<div id="tab:audio-divergence">
  <table>
    <caption>Distribution divergence between real and fake samples for audio
      embeddings. Higher values indicate greater separability.</caption>
    <thead>
      <tr>
        <th style="text-align: left;"><strong>Embedding</strong></th>
        <th style="text-align: center;"><strong>KL Divergence</strong></th>
        <th style="text-align: center;"></th>
        <th style="text-align: center;"></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align: left;">OpenL3</td>
        <td style="text-align: center;"><strong>7.57</strong></td>
        <td style="text-align: center;"></td>
        <td style="text-align: center;"></td>
      </tr>
      <tr>
        <td style="text-align: left;">HuBERT</td>
        <td style="text-align: center;">6.34</td>
        <td style="text-align: center;"></td>
        <td style="text-align: center;"></td>
      </tr>
      <tr>
        <td style="text-align: left;">Wav2Vec2</td>
        <td style="text-align: center;">1.28</td>
        <td style="text-align: center;"></td>
        <td style="text-align: center;"></td>
      </tr>
      <tr>
        <td style="text-align: left;">MFCC</td>
        <td style="text-align: center;">1.21</td>
        <td style="text-align: center;"></td>
        <td style="text-align: center;"></td>
      </tr>
    </tbody>
  </table>
</div>
<p>The divergence metric corroborats the classification results: OpenL3
  and HuBERT exhibit substantially higher distributional separation
  between classes. This suggests that OpenL3 and HuBERT encode features
  that naturally distinguish authentic from synthetic audio, making them
  suitable candidates for downstream representation learning.</p>
<p>Based on these results, both OpenL3 and HuBERT are selected as audio
  embeddings. While OpenL3 achieves marginally higher performance, HuBERT
  captures complementary information (Pearson correlation <span class="math inline">\(r = 0.82\)</span> between their
  prediction
  scores), motivating their combined use.</p>
<h4 id="video-embeddings">Video Embeddings</h4>
<p>Five video embedding approaches are evaluated spanning face
  recognition models and video understanding architectures: SENet <span class="citation"
    data-cites="hu2018senet"></span>, Marlin <span class="citation" data-cites="cai2022marlin"></span>, ArcFace <span
    class="citation" data-cites="deng2019arcface"></span>, FaceNet <span class="citation"
    data-cites="schroff2015facenet"></span>, and
  MagFace <span class="citation" data-cites="meng2021magface"></span>.
  Table <a href="#tab:video-embed" data-reference-type="ref" data-reference="tab:video-embed">3.3</a> reports
  classification
  performance.</p>
<div id="tab:video-embed">
  <table>
    <caption>Video embedding comparison for deepfake detection. Results
      averaged over 5-fold cross-validation.</caption>
    <thead>
      <tr>
        <th style="text-align: left;"><strong>Embedding</strong></th>
        <th style="text-align: center;"><strong>AUROC</strong></th>
        <th style="text-align: center;"><strong>Accuracy</strong></th>
        <th style="text-align: center;"><strong>Recall</strong></th>
        <th style="text-align: center;"><strong>FPR</strong></th>
        <th style="text-align: center;"><strong>F1</strong></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align: left;">SENet</td>
        <td style="text-align: center;"><strong>0.914</strong></td>
        <td style="text-align: center;"><strong>87.03%</strong></td>
        <td style="text-align: center;"><strong>88.04%</strong></td>
        <td style="text-align: center;"><strong>13.77%</strong></td>
        <td style="text-align: center;"><strong>0.858</strong></td>
      </tr>
      <tr>
        <td style="text-align: left;">Marlin</td>
        <td style="text-align: center;">0.802</td>
        <td style="text-align: center;">74.28%</td>
        <td style="text-align: center;">66.27%</td>
        <td style="text-align: center;">19.31%</td>
        <td style="text-align: center;">0.696</td>
      </tr>
      <tr>
        <td style="text-align: left;">ArcFace</td>
        <td style="text-align: center;">0.720</td>
        <td style="text-align: center;">67.16%</td>
        <td style="text-align: center;">78.47%</td>
        <td style="text-align: center;">41.87%</td>
        <td style="text-align: center;">0.680</td>
      </tr>
      <tr>
        <td style="text-align: left;">FaceNet</td>
        <td style="text-align: center;">0.711</td>
        <td style="text-align: center;">66.42%</td>
        <td style="text-align: center;">69.86%</td>
        <td style="text-align: center;">36.33%</td>
        <td style="text-align: center;">0.649</td>
      </tr>
      <tr>
        <td style="text-align: left;">MagFace</td>
        <td style="text-align: center;">0.561</td>
        <td style="text-align: center;">52.50%</td>
        <td style="text-align: center;">79.43%</td>
        <td style="text-align: center;">69.02%</td>
        <td style="text-align: center;">0.598</td>
      </tr>
    </tbody>
  </table>
</div>
<p>SENet substantially outperforms all other video embeddings, achieving
  an AUROC of 0.914. Notably, the face recognition embeddings (ArcFace,
  FaceNet, MagFace) perform poorly for deepfake detection, which is
  expected as these models are optimized for identity discrimination
  rather than authenticity detection, and they may encode
  identity-specific features that are preserved across real and fake
  versions of the same subject.</p>
<p>Table <a href="#tab:video-divergence" data-reference-type="ref" data-reference="tab:video-divergence">3.4</a>
  reports distributional
  divergence metrics for video embeddings.</p>
<div id="tab:video-divergence">
  <table>
    <caption>Distribution divergence between real and fake samples for video
      embeddings. Higher values indicate greater separability.</caption>
    <thead>
      <tr>
        <th style="text-align: left;"><strong>Embedding</strong></th>
        <th style="text-align: center;"><strong>KL Divergence</strong></th>
        <th style="text-align: center;"></th>
        <th style="text-align: center;"></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align: left;">SENet</td>
        <td style="text-align: center;">2.69</td>
        <td style="text-align: center;"></td>
        <td style="text-align: center;"></td>
      </tr>
      <tr>
        <td style="text-align: left;">Marlin</td>
        <td style="text-align: center;"><strong>3.03</strong></td>
        <td style="text-align: center;"></td>
        <td style="text-align: center;"></td>
      </tr>
      <tr>
        <td style="text-align: left;">FaceNet</td>
        <td style="text-align: center;">2.03</td>
        <td style="text-align: center;"></td>
        <td style="text-align: center;"></td>
      </tr>
      <tr>
        <td style="text-align: left;">ArcFace</td>
        <td style="text-align: center;">1.09</td>
        <td style="text-align: center;"></td>
        <td style="text-align: center;"></td>
      </tr>
      <tr>
        <td style="text-align: left;">MagFace</td>
        <td style="text-align: center;">0.14</td>
        <td style="text-align: center;"></td>
        <td style="text-align: center;"></td>
      </tr>
    </tbody>
  </table>
</div>
<p>While Marlin achieves the highest KL divergence (3.03), SENet has
  substantially higher classification performance, so it is selected as
  the video embedding.</p>
<h3 id="sec:repr-quality">Representation Quality Analysis</h3>
<p>To evaluate the quality of the learned authenticity representations
  without relying on a downstream classifier, we employ a suite of
  unsupervised metrics that assess the geometric structure and intrinsic
  separability of the embedding space.</p>
<h4 id="clustering-metric-adjusted-mutual-information-ami">Clustering
  Metric: Adjusted Mutual Information (AMI)</h4>
<p>K-means clustering is applied to the authenticity embeddings <span class="math inline">\(z^{\text{auth}}\)</span>,
  and AMI is computed to
  measure agreement between cluster assignments and real/fake labels,
  corrected for chance. High AMI values indicate that real and fake
  samples naturally separate into distinct clusters, suggesting that
  authenticity is encoded as an intrinsic property of the embedding
  space.</p>
<h4 id="distribution-metric-kl-divergence">Distribution Metric: KL
  Divergence</h4>
<p>Real and fake embeddings are projected onto the
  distance-to-real-centroid statistic, and KL divergence between the
  resulting distributions is computed. Higher KL divergence indicates
  greater distributional separation and reflects the effectiveness of the
  variance minimization objective in creating a distinguishable real
  manifold.</p>
<p><span class="math display">\[\begin{equation}
    D_{\text{KL}}(P \| Q) = \int p(x) \log \frac{p(x)}{q(x)} \, dx
    \end{equation}\]</span></p>
<h4 id="separation-metric-mean-cosine-similarity">Separation Metric:
  Mean Cosine Similarity</h4>
<p>We compute cosine similarity between each embedding and both the real
  and fake centroids. Key statistics include real-to-real, fake-to-real,
  real-to-fake, and fake-to-fake similarities. The separation gap—defined
  as the difference between real-to-real and fake-to-real
  similarity—quantifies how well real samples form a compact cluster from
  which fake samples deviate. For each sample <span class="math inline">\(i\)</span> with embedding <span
    class="math inline">\(z_i^{\text{auth}}\)</span> and centroid <span class="math inline">\(\mu_c\)</span>, the
  cosine similarity is: <span class="math display">\[\begin{equation}
    \text{sim}(z_i^{\text{auth}}, \mu_c) = \frac{z_i^{\text{auth}} \cdot
    \mu_c}{\|z_i^{\text{auth}}\| \|\mu_c\|}
    \end{equation}\]</span></p>
<h4 id="local-content-group-metrics">Local Content-Group Metrics</h4>
<p>To assess disentanglement consistency within individual videos
  (content groups), we evaluate two metrics: (i) intra-group cosine
  similarity in <span class="math inline">\(z^{\text{id}}\)</span>,
  measuring whether identity features remain invariant across
  augmentations, and (ii) intra-group variance in <span class="math inline">\(z^{\text{auth}}\)</span> for real and
  fake
  augmentations, with the variance ratio capturing the relative
  consistency of authenticity cues.</p>
<h4 id="regularization-strategy-comparison">Regularization Strategy
  Comparison</h4>
<p>Three variance-floor regularization regimes (conservative, moderate,
  aggressive) are compared using this complete metric suite to evaluate
  collapse prevention, real–fake separability, and training stability.</p>
<h3 id="sec:regularization-strategies">Regularization Strategy
  Comparison</h3>
<p>To systematically evaluate the effectiveness of variance floor
  regularization and determine optimal hyperparameter settings, three
  training paradigms are designed that span the spectrum from weak to
  strong regularization enforcement. These paradigms test whether the
  regularization mechanism successfully prevents the representation
  collapse observed in preliminary experiments (Section <a href="#sec:interpretation" data-reference-type="ref"
    data-reference="sec:interpretation">3.6.1</a>), and which hyperparameter
  configuration achieves the best balance between preventing collapse and
  maintaining real/fake separability.</p>
<h4 id="training-paradigms">Training Paradigms</h4>
<p>Three regularization strategies are defined by varying the variance
  floor threshold <span class="math inline">\(\tau\)</span> and
  regularization weight <span class="math inline">\(\lambda_{\text{reg}}\)</span> in Equation <a
    href="#eq:variance_loss_reg" data-reference-type="ref"
    data-reference="eq:variance_loss_reg">[eq:variance_loss_reg]</a>:</p>
<h5 id="conservative-regularization">Conservative Regularization</h5>
<p>(<span class="math inline">\(\tau = 0.1\)</span>, <span class="math inline">\(\lambda_{\text{reg}} = 1.0\)</span>)
  allows
  aggressive compression of real embeddings with minimal resistance
  against collapse. This weak enforcement strategy tests whether light
  regularization suffices to prevent the degenerate behavior where all
  embeddings collapse to a single point despite satisfying the variance
  objective. This approach is hypothesized to still exhibit collapse, as
  the low variance floor and weak penalty provide insufficient constraint
  on the optimization.</p>
<h5 id="moderate-regularization">Moderate Regularization</h5>
<p>(<span class="math inline">\(\tau = 0.2\)</span>, <span class="math inline">\(\lambda_{\text{reg}} = 2.0\)</span>)
  enforces a
  moderate variance floor with moderate penalty strength. The higher
  threshold (<span class="math inline">\(\tau = 0.2\)</span> vs. 0.1)
  requires real embeddings to maintain more spread, while the doubled
  regularization weight (<span class="math inline">\(\lambda_{\text{reg}}
    = 2.0\)</span>) increases the cost of violating this constraint. This
  balanced approach is hypothesized to prevent collapse while allowing
  sufficient compression for effective anomaly detection, striking an
  optimal trade-off between the competing objectives.</p>
<h5 id="aggressive-regularization">Aggressive Regularization</h5>
<p>(<span class="math inline">\(\tau = 0.5\)</span>, <span class="math inline">\(\lambda_{\text{reg}} = 5.0\)</span>)
  imposes a
  high variance floor with severe penalties for violations. While this
  should reliably prevent collapse, it risks overregularization—forcing
  real embeddings to remain spread even when tighter clustering would
  improve separation from fake samples. The concern is that aggressive
  regularization could artificially inflate the real manifold, reducing
  the margin between real and fake distributions and potentially harming
  out-of-distribution generalization.</p>
<h4 id="evaluation-protocol">Evaluation Protocol</h4>
<p>Each paradigm is trained for 50 epochs on identical data splits
  (AVDeepfake-1M++ and ShareVeo3 as described in Section <a href="#sec:datasets" data-reference-type="ref"
    data-reference="sec:datasets">3.4.1</a>), using the loss formulation in
  Equation <a href="#eq:total_loss" data-reference-type="ref" data-reference="eq:total_loss">[eq:total_loss]</a> with
  paradigm-specific <span class="math inline">\(\tau\)</span> and <span
    class="math inline">\(\lambda_{\text{reg}}\)</span> values. All other
  hyperparameters remain fixed across paradigms: batch size of 256,
  learning rate of <span class="math inline">\(1 \times 10^{-4}\)</span>
  with cosine annealing, and balanced batching to ensure equal
  representation of real and fake samples.</p>
<p>Evaluation occurs at each epoch, where the full suite of unsupervised
  representation quality metrics described in Section <a href="#sec:repr-quality" data-reference-type="ref"
    data-reference="sec:repr-quality">3.4.3</a> is computed on 2 evaluation
  sets: (1) Mixed AVDeepfake-1M++ and  ShareVeo3 validation split
  (in-distribution real and fake) and (2) Sora2 (out-of-distribution
  synthetic).</p>
<p>Paradigms are compared on three criteria:</p>
<ol>
  <li>
    <p><strong>Collapse Prevention:</strong> Does the approach prevent
      the representation collapse observed in preliminary experiments? This is
      measured through Wasserstein distance between real and fake
      distributions (should remain <span class="math inline">\(&gt;
        0.1\)</span>), variance trajectory of real embeddings over training
      (should stabilize above <span class="math inline">\(\tau\)</span>), and
      mean distance to real centroid (should not approach zero for both
      classes).</p>
  </li>
  <li>
    <p><strong>Class Separability:</strong> Does the approach maintain
      meaningful separation between real and fake samples? This is assessed
      through AMI and ARI scores (should increase or remain stable),
      silhouette coefficient with ground truth labels (should be positive),
      and KL/JS divergence between real and fake distributions (should
      increase).</p>
  </li>
  <li>
    <p><strong>Training Stability:</strong> What are the training
      dynamics and gradient flow characteristics? Loss curves for <span
        class="math inline">\(\mathcal{L}_{\text{proto}}\)</span>, <span
        class="math inline">\(\mathcal{L}_{\text{var}}\)</span>, and <span
        class="math inline">\(\mathcal{L}_{\text{orth}}\)</span> are tracked to
      verify all objectives decrease without plateauing or oscillating, and
      whether the variance loss remains active (non-zero gradient) throughout
      training is monitored.</p>
  </li>
</ol>
<p>The central research questions addressed are: (1) Does variance floor
  regularization successfully prevent collapse while maintaining
  separability? (2) Which regularization strength
  (conservative/moderate/aggressive) achieves optimal performance? (3)
  What insights does this provide for multi-objective optimization in
  representation learning, particularly for anomaly detection frameworks
  that rely on variance minimization?</p>
<h2 id="sec:results">Results</h2>
<p>The analysis is structured per embedding model (HuBERT, OpenL3,
  SENet) and explores how varying the regularization strength
  (Conservative, Moderate, Aggressive) impacts the learned
  representations. For each model, the <strong>In-Distribution (ID)
    Analysis</strong> on the training data is first presented, followed by
  the <strong>Out-of-Distribution (OOD) Analysis</strong> on
  Sora2-generated content to evaluate generalization.</p>
<h3 id="sec:hubert-repr-results">HuBERT Audio Embeddings</h3>
<h5 id="in-distribution-analysis">In-Distribution Analysis</h5>
<p>Table <a href="#tab:repr-id-comparison" data-reference-type="ref"
    data-reference="tab:repr-id-comparison">[tab:repr-id-comparison]</a>
  presents geometric and clustering metrics on the validation set.</p>
<p>All projected schemes exhibit <strong>representation
    collapse</strong>, with Wasserstein distance dropping <span class="math inline">\(&gt;99\%\)</span> (from <span
    class="math inline">\(0.533\)</span> to <span class="math inline">\(\approx0.004\)</span>). However, the schemes
  differ in their preservation of authenticity structure.</p>
<p>The <strong>Aggressive</strong> scheme (<span class="math inline">\(\tau=0.5\)</span>) outperforms the baseline,
  improving AMI by <span class="math inline">\(8\%\)</span> (<span class="math inline">\(0.120\)</span>) and ARI by
  <span class="math inline">\(35\%\)</span> (<span class="math inline">\(0.089\)</span>). Unlike the baseline, it
  achieves
  a positive Separation Gap (<span class="math inline">\(+0.0010\)</span>), confirming correct centroid
  alignment despite collapse. Conversely, <strong>Conservative</strong>
  and <strong>Moderate</strong> schemes degrade label alignment. Notably,
  Moderate fails to achieve a positive Separation Gap (<span class="math inline">\(-0.0023\)</span>), indicating
  incorrect centroid
  orientation.
</p>
<p>Results confirm the <strong>Silhouette Paradox</strong>: K-means
  Silhouette peaks with the most collapsed representations (Conservative:
  <span class="math inline">\(0.575\)</span>), inversely to label
  alignment. This indicates that collapse creates tight clusters unrelated
  to authenticity. Crucially, the Aggressive scheme maintains <span class="math inline">\(5\times\)</span> higher
  intra-group variance
  (Real: <span class="math inline">\(0.212\)</span>) than Conservative,
  suggesting the higher variance floor successfully resists complete
  collapse.
</p>
<figure id="fig:global-hubert-comparison" data-latex-placement="t!">
  <div class="minipage">
    <img
      src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/senet/baseline/global_tsne_all_datasets_senet_baseline.png" />
  </div>
  <div class="minipage">
    <img
      src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/hubert/conservative/global_tsne_all_datasets_hubert_conservative.png" />
  </div>
  <div class="minipage">
    <img
      src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/hubert/moderate/global_tsne_all_datasets_hubert_moderate.png" />
  </div>
  <div class="minipage">
    <img
      src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/hubert/aggressive/global_tsne_all_datasets_hubert_aggressive.png" />
  </div>
  <figcaption>Global t-SNE comparison. The Original baseline (a) shows
    broad dispersion. The Aggressive scheme (d) maintains the most spread
    among projected schemes while achieving better authenticity
    alignment.</figcaption>
</figure>
<figure id="fig:per-video-hubert-comparison" data-latex-placement="t!">
  <div class="minipage">
    <img
      src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/hubert/baseline/exp1_hubert_baseline_gdg4mUSwuhI_00002_seg14.png"
      style="width:90.0%" />
  </div>
  <div class="minipage">
    <img
      src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/hubert/conservative/exp1_hubert_conservative_gdg4mUSwuhI_00002_seg14.png"
      style="width:90.0%" />
  </div>
  <div class="minipage">
    <img
      src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/hubert/moderate/exp1_hubert_moderate_gdg4mUSwuhI_00002_seg14.png"
      style="width:90.0%" />
  </div>
  <div class="minipage">
    <img
      src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/hubert/aggressive/exp1_hubert_aggressive_gdg4mUSwuhI_00002_seg14.png"
      style="width:90.0%" />
  </div>
  <figcaption>Per-video analysis: (left) PC1/PC2 scatter, (center)
    authenticity score scatter, (right) cosine similarity to source. The
    distance-to-source metric (right) demonstrates improved differentiation
    between real (green) and fake (red) augmentations under stronger
    regularization.</figcaption>
</figure>
<h5 id="out-of-distribution-analysis">Out-of-Distribution Analysis</h5>
<p>Table <a href="#tab:repr-ood-comparison" data-reference-type="ref" data-reference="tab:repr-ood-comparison">3.5</a>
  evaluates
  generalization by comparing embeddings of in-distribution real samples
  (AVDeepfake1M++) against out-of-distribution synthetic samples from
  Sora2.</p>
<div id="tab:repr-ood-comparison">
  <table>
    <caption>Out-of-Distribution Representation Metrics Comparison (HuBERT).
      Metrics compare ID Real samples against OOD Sora2 samples. <span class="math inline">\(\uparrow\)</span>
      indicates higher is better for
      generalization.</caption>
    <thead>
      <tr>
        <th style="text-align: left;"><strong>Metric</strong></th>
        <th style="text-align: center;"><strong>Original</strong></th>
        <th style="text-align: center;"><strong>Conservative</strong></th>
        <th style="text-align: center;"><strong>Moderate</strong></th>
        <th style="text-align: center;"><strong>Aggressive</strong></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td colspan="5" style="text-align: left;"><em>Clustering
            Metrics</em></td>
      </tr>
      <tr>
        <td style="text-align: left;">AMI <span class="math inline">\(\uparrow\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(0.016\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(0.017\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(0.017\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(\mathbf{0.018}\)</span></td>
      </tr>
      <tr>
        <td style="text-align: left;">ARI <span class="math inline">\(\uparrow\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(0.004\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(-0.001\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(-0.001\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(\mathbf{0.007}\)</span></td>
      </tr>
      <tr>
        <td style="text-align: left;">Silhouette (GT) <span class="math inline">\(\uparrow\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(-0.025\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(-0.002\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(-0.009\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(\mathbf{+0.001}\)</span></td>
      </tr>
      <tr>
        <td colspan="5" style="text-align: left;"><em>Distribution and
            Separation Metrics</em></td>
      </tr>
      <tr>
        <td style="text-align: left;">Separation Gap <span class="math inline">\(\uparrow\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(-0.020\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(\mathbf{+0.0001}\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(-0.0022\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(+0.00005\)</span></td>
      </tr>
      <tr>
        <td style="text-align: left;">Wasserstein Distance <span class="math inline">\(\uparrow\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(\mathbf{0.219}\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(0.001\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(0.004\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(0.002\)</span></td>
      </tr>
    </tbody>
  </table>
</div>
<p>The OOD analysis reveals that the <strong>Aggressive</strong> scheme
  achieves the best generalization to unseen Sora2 content across all
  clustering metrics. It is the only scheme to achieve positive values for
  both ARI (<span class="math inline">\(+0.007\)</span> vs. <span class="math inline">\(+0.004\)</span> for Original)
  and ground-truth
  Silhouette (<span class="math inline">\(+0.001\)</span> vs. <span class="math inline">\(-0.025\)</span> for
  Original). This represents a
  qualitative shift: while the Original embeddings and
  Conservative/Moderate schemes produce negative Silhouette scores
  (indicating that Sora2 samples are geometrically closer to real samples
  than to each other), the Aggressive scheme produces positive Silhouette,
  suggesting meaningful geometric separation between ID real and OOD
  synthetic content.</p>
<p>The Separation Gap results reveal an important nuance. Both
  Conservative (<span class="math inline">\(+0.0001\)</span>) and
  Aggressive (<span class="math inline">\(+0.00005\)</span>) achieve
  positive gaps, indicating correct centroid orientation where Sora2
  content is positioned farther from the real centroid than real samples.
  However, the Moderate scheme fails to generalize, producing a negative
  Separation Gap (<span class="math inline">\(-0.0022\)</span>) that
  mirrors its in-distribution failure. This suggests that the Moderate
  configuration (<span class="math inline">\(\tau=0.2\)</span>, <span
    class="math inline">\(\lambda_{\text{reg}}=2.0\)</span>) occupies an
  unstable region of the hyperparameter space where neither sufficient
  collapse prevention nor effective centroid alignment is achieved.</p>
<p>Despite these improvements in geometric structure, all schemes suffer
  from severe Wasserstein collapse in the OOD setting (<span class="math inline">\(0.219 \rightarrow
    0.001\)</span>–<span class="math inline">\(0.004\)</span>), indicating that the
  distributional spread necessary for robust anomaly detection is not
  preserved. The positive Separation Gaps and improved Silhouette scores
  suggest that the <em>direction</em> of OOD separation is learned
  correctly, but the <em>magnitude</em> of separation remains insufficient
  for high-confidence detection of novel generation methods.</p>
<p>The Aggressive scheme’s superior OOD performance, combined with its
  best-in-class ID metrics, suggests that higher variance floors are
  essential for both preserving authenticity structure and enabling
  generalization. However, the persistent Wasserstein collapse across all
  schemes indicates that variance floor regularization alone is
  insufficient to fully prevent the degenerate optimization behavior,
  motivating the alternative loss formulations discussed in Section <a href="#sec:interpretation"
    data-reference-type="ref" data-reference="sec:interpretation">3.6.1</a>.</p>
<h3 id="sec:openl3-repr-results">OpenL3 Audio Embeddings</h3>
<h5 id="in-distribution-analysis-1">In-Distribution Analysis</h5>
<p>OpenL3 embeddings exhibit markedly different baseline characteristics
  compared to HuBERT. The Original OpenL3 embeddings achieve substantially
  higher ground-truth Silhouette (<span class="math inline">\(0.321\)</span> vs. HuBERT’s <span
    class="math inline">\(0.033\)</span>) and a positive baseline Separation
  Gap (<span class="math inline">\(+0.0096\)</span> vs. HuBERT’s <span class="math inline">\(-0.013\)</span>),
  indicating that OpenL3 already
  encodes authenticity-relevant structure prior to disentanglement
  training. The extremely high K-means Silhouette (<span class="math inline">\(0.937\)</span>) suggests that OpenL3
  embeddings
  naturally form tight clusters, though the low AMI/ARI indicate these
  clusters do not align with authenticity labels.</p>
<p>Despite these favorable baseline properties, all three regularization
  schemes induce severe <strong>representation collapse</strong>. The
  Wasserstein distance drops from <span class="math inline">\(3.413\)</span> to <span
    class="math inline">\(0.010\)</span>–<span class="math inline">\(0.018\)</span> (<span
    class="math inline">\(\downarrow 99.5\%\)</span>), and the ground-truth
  Silhouette decreases from <span class="math inline">\(0.321\)</span> to
  <span class="math inline">\(0.029\)</span>–<span class="math inline">\(0.059\)</span> (<span
    class="math inline">\(\downarrow 82\)</span>–<span class="math inline">\(91\%\)</span>). This collapse is more
  dramatic
  than observed with HuBERT, likely because OpenL3’s higher initial
  variance provides more room for compression.
</p>
<p>The <strong>Moderate</strong> scheme (<span class="math inline">\(\tau=0.2\)</span>, <span
    class="math inline">\(\lambda_{\text{reg}}=2.0\)</span>) achieves the
  best label-alignment metrics, with AMI of <span class="math inline">\(0.081\)</span> and ARI of <span
    class="math inline">\(0.088\)</span>—representing <span class="math inline">\(238\%\)</span> and <span
    class="math inline">\(120\%\)</span> improvements over the Original
  baseline respectively. Moderate also achieves the highest positive
  Separation Gap (<span class="math inline">\(+0.0102\)</span>), slightly
  exceeding even the Original (<span class="math inline">\(+0.0096\)</span>). This contrasts with HuBERT,
  where the Aggressive scheme performed best, suggesting that optimal
  regularization strength is embedding-dependent.</p>
<p>The <strong>Aggressive</strong> scheme (<span class="math inline">\(\tau=0.5\)</span>, <span
    class="math inline">\(\lambda_{\text{reg}}=5.0\)</span>) shows degraded
  performance relative to Moderate, with AMI dropping to <span class="math inline">\(0.042\)</span> and ARI to <span
    class="math inline">\(0.031\)</span>—both below even the Conservative
  scheme. However, Aggressive maintains the highest preserved variance
  (Real: <span class="math inline">\(0.215\)</span>, Fake: <span class="math inline">\(0.315\)</span>) and K-means
  Silhouette (<span class="math inline">\(0.535\)</span>), suggesting that excessive
  variance regularization prevents the model from learning discriminative
  structure. The <strong>Conservative</strong> scheme fails to achieve a
  positive Separation Gap (<span class="math inline">\(-0.0001\)</span>),
  indicating incorrect centroid orientation despite acceptable
  label-alignment metrics.</p>
<figure id="fig:openl3-global-comparison" data-latex-placement="h">
  <div class="minipage">
    <img
      src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/openl3/baseline/global_tsne_all_datasets_openl3_baseline.png" />
  </div>
  <div class="minipage">
    <img
      src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/openl3/conservative/global_tsne_all_datasets_openl3_conservative.png" />
  </div>
  <div class="minipage">
    <img
      src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/openl3/moderate/global_tsne_all_datasets_openl3_moderate.png" />
  </div>
  <div class="minipage">
    <img
      src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/openl3/aggressive/global_tsne_all_datasets_openl3_aggressive.png" />
  </div>
  <figcaption>Visualization of the global embedding space for OpenL3 using
    t-SNE. The Original baseline (a) exhibits tighter clustering than HuBERT
    due to OpenL3’s higher intrinsic similarity structure (K-means
    Silhouette <span class="math inline">\(0.937\)</span>). The projected
    schemes (b–d) are expected to show progressive collapse, with Moderate
    (c) achieving the best balance between compression and authenticity
    separation.</figcaption>
</figure>
<figure id="fig:openl3-per-video-comparison" data-latex-placement="h">
  <div class="minipage">
    <img
      src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/openl3/baseline/exp1_openl3_baseline_gdg4mUSwuhI_00002_seg14.png"
      style="width:90.0%" />
  </div>
  <div class="minipage">
    <img
      src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/openl3/conservative/exp1_openl3_conservative_gdg4mUSwuhI_00002_seg14.png"
      style="width:90.0%" />
  </div>
  <div class="minipage">
    <img
      src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/openl3/moderate/exp1_openl3_moderate_gdg4mUSwuhI_00002_seg14.png"
      style="width:90.0%" />
  </div>
  <div class="minipage">
    <img
      src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/openl3/aggressive/exp1_openl3_aggressive_gdg4mUSwuhI_00002_seg14.png"
      style="width:90.0%" />
  </div>
  <figcaption>Per-video analysis for the projected OpenL3 embeddings
    across regularization schemes. Given OpenL3’s superior baseline
    separation, the per-video plots should show clearer differentiation
    between real and fake augmentations in the Original baseline compared to
    HuBERT. The Moderate scheme (c) is expected to maintain this
    differentiation while the Conservative (b) and Aggressive (d) schemes
    may show degraded structure.</figcaption>
</figure>
<h5 id="out-of-distribution-analysis-1">Out-of-Distribution
  Analysis</h5>
<div id="tab:repr-ood-comparison-openl3">
  <table>
    <caption>Out-of-Distribution Representation Metrics Comparison (OpenL3).
      Metrics compare ID Real samples against OOD Sora2 samples. <span class="math inline">\(\uparrow\)</span>
      indicates higher is better for
      generalization.</caption>
    <thead>
      <tr>
        <th style="text-align: left;"><strong>Metric</strong></th>
        <th style="text-align: center;"><strong>Original</strong></th>
        <th style="text-align: center;"><strong>Conservative</strong></th>
        <th style="text-align: center;"><strong>Moderate</strong></th>
        <th style="text-align: center;"><strong>Aggressive</strong></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td colspan="5" style="text-align: left;"><em>Clustering
            Metrics</em></td>
      </tr>
      <tr>
        <td style="text-align: left;">AMI <span class="math inline">\(\uparrow\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(0.005\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(\mathbf{0.019}\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(\mathbf{0.020}\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(0.010\)</span></td>
      </tr>
      <tr>
        <td style="text-align: left;">ARI <span class="math inline">\(\uparrow\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(-0.020\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(0.015\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(\mathbf{0.021}\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(0.006\)</span></td>
      </tr>
      <tr>
        <td style="text-align: left;">Silhouette (GT) <span class="math inline">\(\uparrow\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(\mathbf{+0.029}\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(-0.048\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(-0.040\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(-0.036\)</span></td>
      </tr>
      <tr>
        <td colspan="5" style="text-align: left;"><em>Distribution and
            Separation Metrics</em></td>
      </tr>
      <tr>
        <td style="text-align: left;">Separation Gap <span class="math inline">\(\uparrow\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(\mathbf{+0.0002}\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(-0.0055\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(-0.0110\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(-0.0273\)</span></td>
      </tr>
      <tr>
        <td style="text-align: left;">Wasserstein Distance <span class="math inline">\(\uparrow\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(\mathbf{0.808}\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(0.017\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(0.023\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(0.028\)</span></td>
      </tr>
    </tbody>
  </table>
</div>
<p>The OOD analysis reveals a critical limitation of the disentanglement
  framework when applied to OpenL3 embeddings: <strong>all three
    regularization schemes degrade OOD generalization</strong> relative to
  the Original baseline. The Original OpenL3 embeddings achieve a positive
  Separation Gap (<span class="math inline">\(+0.0002\)</span>) and
  positive ground-truth Silhouette (<span class="math inline">\(+0.029\)</span>) on Sora2 content, indicating that
  untrained OpenL3 embeddings already possess some capacity to distinguish
  novel synthetic audio from authentic content.</p>
<p>After disentanglement training, all schemes produce <strong>negative
    Separation Gaps</strong> (Conservative: <span class="math inline">\(-0.0055\)</span>, Moderate: <span
    class="math inline">\(-0.0110\)</span>, Aggressive: <span class="math inline">\(-0.0273\)</span>), indicating that
  Sora2 content
  is now positioned <em>closer</em> to the real centroid than real
  samples—the opposite of the intended behavior. This inversion worsens
  monotonically with regularization strength, suggesting that stronger
  variance floors exacerbate the OOD failure mode.</p>
<p>The ground-truth Silhouette scores similarly degrade from <span class="math inline">\(+0.029\)</span> (Original) to
  <span class="math inline">\(-0.036\)</span> to <span class="math inline">\(-0.048\)</span> across the projected
  schemes.
  Notably, while the label-alignment metrics (AMI, ARI) improve for the
  projected schemes relative to Original, this improvement reflects better
  clustering of ID content rather than improved OOD detection. The
  Moderate scheme achieves the best AMI (<span class="math inline">\(0.020\)</span>) and ARI (<span
    class="math inline">\(0.021\)</span>), but its negative Separation Gap
  indicates this clustering does not generalize to unseen generation
  methods.
</p>
<p>This pattern contrasts sharply with HuBERT, where the Conservative
  and Aggressive schemes achieved positive OOD Separation Gaps despite
  similar collapse patterns. The difference may stem from OpenL3’s
  stronger baseline OOD performance: because OpenL3 already encodes some
  generalization-relevant structure, the disentanglement training
  <em>overwrites</em> this information with ID-specific features that fail
  to transfer. HuBERT’s weaker baseline may paradoxically benefit from
  disentanglement training by providing a “blank slate” that can learn
  more generalizable representations.
</p>
<p>These results suggest that disentanglement training on OpenL3
  embeddings optimizes for ID separation at the cost of OOD
  generalization. Future work should explore regularization strategies
  that explicitly preserve baseline OOD structure while improving ID
  discrimination, such as knowledge distillation from the Original
  embeddings or OOD-aware training objectives.</p>
<h3 id="sec:senet-repr-results">SENet Visual Embeddings</h3>
<h5 id="in-distribution-analysis-2">In-Distribution Analysis</h5>
<p>Table <a href="#tab:repr-id-comparison-senet" data-reference-type="ref"
    data-reference="tab:repr-id-comparison-senet">[tab:repr-id-comparison-senet]</a>
  reports the core geometric and clustering metrics for SENet visual
  embeddings on the combined AVDeepfake1M++ and ShareVeo3 validation
  set.</p>
<p>SENet visual embeddings exhibit fundamentally different baseline
  characteristics compared to both audio embedding types. The Original
  SENet embeddings possess a substantially higher Wasserstein distance
  (<span class="math inline">\(6.759\)</span> vs. HuBERT’s <span class="math inline">\(0.533\)</span> and OpenL3’s
  <span class="math inline">\(3.413\)</span>) and a strongly positive Separation
  Gap (<span class="math inline">\(+0.081\)</span>), indicating that
  untrained SENet embeddings already encode significant
  authenticity-relevant structure with correctly-oriented class centroids.
  The high intra-group variances (Real: <span class="math inline">\(3659.8\)</span>, Fake: <span
    class="math inline">\(5010.7\)</span>) reflect the greater
  dimensionality and heterogeneity of visual features compared to
  audio.
</p>
<p>All three regularization schemes induce severe <strong>representation
    collapse</strong>, with Wasserstein distance dropping from <span class="math inline">\(6.759\)</span> to <span
    class="math inline">\(0.003\)</span>–<span class="math inline">\(0.015\)</span> (<span
    class="math inline">\(\downarrow 99.8\%\)</span>). More critically, all
  schemes <strong>destroy the baseline’s positive Separation Gap</strong>,
  producing negative values (Conservative: <span class="math inline">\(-0.0048\)</span>, Moderate: <span
    class="math inline">\(-0.0062\)</span>, Aggressive: <span class="math inline">\(-0.0015\)</span>). This represents
  a qualitative
  failure: the disentanglement training <em>inverts</em> the centroid
  structure, positioning fake samples closer to the real centroid than
  real samples—the opposite of the intended behavior and worse than the
  untrained baseline.</p>
<p>Despite this centroid inversion, the <strong>Aggressive</strong>
  scheme (<span class="math inline">\(\tau=0.5\)</span>, <span class="math inline">\(\lambda_{\text{reg}}=5.0\)</span>)
  achieves
  remarkable improvements in label-alignment metrics. AMI increases from
  <span class="math inline">\(0.141\)</span> to <span class="math inline">\(0.146\)</span> (<span
    class="math inline">\(\uparrow 4\%\)</span>) and ARI improves
  dramatically from <span class="math inline">\(0.053\)</span> to <span class="math inline">\(0.158\)</span> (<span
    class="math inline">\(\uparrow 198\%\)</span>). This paradoxical
  result—improved clustering despite inverted centroids—suggests that the
  Aggressive scheme learns discriminative structure that is orthogonal to
  the prototype-based separation objective. The projected embeddings may
  capture authenticity-relevant features that enable clustering without
  conforming to the intended centroid geometry.
</p>
<p>The Conservative and Moderate schemes show degraded AMI (<span class="math inline">\(0.088\)</span> and <span
    class="math inline">\(0.073\)</span> respectively) compared to both the
  Original baseline and the Aggressive scheme, while achieving
  intermediate ARI values (<span class="math inline">\(0.089\)</span> and
  <span class="math inline">\(0.085\)</span>). The inverse relationship
  between regularization strength and K-means Silhouette (Conservative:
  <span class="math inline">\(0.516\)</span>, Moderate: <span class="math inline">\(0.452\)</span>, Aggressive: <span
    class="math inline">\(0.400\)</span>) mirrors the pattern observed in
  HuBERT and OpenL3, confirming that weaker regularization produces
  tighter but less authenticity-aligned clusters.
</p>
<figure id="fig:senet-global-comparison" data-latex-placement="h">
  <div class="minipage">
    <img
      src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/senet/baseline/global_tsne_all_datasets_senet_baseline.png" />
  </div>
  <div class="minipage">
    <img
      src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/senet/conservative/global_tsne_all_datasets_senet_conservative.png" />
  </div>
  <div class="minipage">
    <img
      src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/senet/moderate/global_tsne_all_datasets_senet_moderate.png" />
  </div>
  <div class="minipage">
    <img
      src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/senet/aggressive/global_tsne_all_datasets_senet_aggressive.png" />
  </div>
  <figcaption>Visualization of the global embedding space for SENet using
    t-SNE. The Original baseline (a) exhibits the highest spread among all
    embedding types due to SENet’s high-dimensional visual features. The
    projected schemes (b–d) are expected to show dramatic collapse, with
    Aggressive (d) maintaining slightly more spread while achieving the best
    label-alignment despite inverted centroid structure.</figcaption>
</figure>
<figure id="fig:senet-per-video-comparison" data-latex-placement="h">
  <div class="minipage">
    <img
      src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/senet/baseline/exp1_senet_baseline_gdg4mUSwuhI_00002_seg14.png"
      style="width:90.0%" />
  </div>
  <div class="minipage">
    <img
      src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/senet/conservative/exp1_senet_conservative_gdg4mUSwuhI_00002_seg14.png"
      style="width:90.0%" />
  </div>
  <div class="minipage">
    <img
      src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/senet/moderate/exp1_senet_moderate_gdg4mUSwuhI_00002_seg14.png"
      style="width:90.0%" />
  </div>
  <div class="minipage">
    <img
      src="Machine_Learning_for_Visual_Fraud_Detection___Homoglyph_Spoofing_and_Deepfake_Identification/deepfake/senet/aggressive/exp1_senet_aggressive_gdg4mUSwuhI_00002_seg14.png"
      style="width:90.0%" />
  </div>
  <figcaption>Per-video analysis for the projected SENet embeddings across
    regularization schemes. Given SENet’s strong baseline separation (<span class="math inline">\(+0.081\)</span>
    Separation Gap), the Original
    baseline (a) should show clear differentiation between real and fake
    augmentations in the distance-to-source bar chart. The projected schemes
    (b–d) may show reduced or inverted differentiation, reflecting the
    negative Separation Gaps observed in the quantitative
    metrics.</figcaption>
</figure>
<h5 id="out-of-distribution-analysis-2">Out-of-Distribution
  Analysis</h5>
<div id="tab:repr-ood-comparison-senet">
  <table>
    <caption>Out-of-Distribution Representation Metrics Comparison (SENet).
      Metrics compare ID Real samples against OOD Sora2 samples. <span class="math inline">\(\uparrow\)</span>
      indicates higher is better for
      generalization.</caption>
    <thead>
      <tr>
        <th style="text-align: left;"><strong>Metric</strong></th>
        <th style="text-align: center;"><strong>Original</strong></th>
        <th style="text-align: center;"><strong>Conservative</strong></th>
        <th style="text-align: center;"><strong>Moderate</strong></th>
        <th style="text-align: center;"><strong>Aggressive</strong></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td colspan="5" style="text-align: left;"><em>Clustering
            Metrics</em></td>
      </tr>
      <tr>
        <td style="text-align: left;">AMI <span class="math inline">\(\uparrow\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(\mathbf{0.039}\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(0.023\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(0.015\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(0.029\)</span></td>
      </tr>
      <tr>
        <td style="text-align: left;">ARI <span class="math inline">\(\uparrow\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(-0.010\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(0.023\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(0.020\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(\mathbf{0.043}\)</span></td>
      </tr>
      <tr>
        <td style="text-align: left;">Silhouette (GT) <span class="math inline">\(\uparrow\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(\mathbf{+0.115}\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(-0.050\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(-0.030\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(-0.015\)</span></td>
      </tr>
      <tr>
        <td colspan="5" style="text-align: left;"><em>Distribution and
            Separation Metrics</em></td>
      </tr>
      <tr>
        <td style="text-align: left;">Separation Gap <span class="math inline">\(\uparrow\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(\mathbf{+0.074}\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(-0.0047\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(-0.0067\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(-0.0029\)</span></td>
      </tr>
      <tr>
        <td style="text-align: left;">Wasserstein Distance <span class="math inline">\(\uparrow\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(\mathbf{7.012}\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(0.015\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(0.014\)</span></td>
        <td style="text-align: center;"><span class="math inline">\(0.004\)</span></td>
      </tr>
    </tbody>
  </table>
</div>
<p>The OOD analysis reveals that SENet exhibits the strongest baseline
  generalization to Sora2 content among all embedding types, but this
  generalization is <strong>completely destroyed</strong> by
  disentanglement training. The Original SENet embeddings achieve a
  Separation Gap of <span class="math inline">\(+0.074\)</span> and
  ground-truth Silhouette of <span class="math inline">\(+0.115\)</span>
  on Sora2 content—substantially higher than both HuBERT (Gap: <span class="math inline">\(-0.020\)</span>,
  Silhouette: <span class="math inline">\(-0.025\)</span>) and OpenL3 (Gap: <span
    class="math inline">\(+0.0002\)</span>, Silhouette: <span class="math inline">\(+0.029\)</span>). This suggests
  that visual
  features naturally encode artifacts that distinguish Sora2-generated
  video from authentic content, consistent with Sora2’s primary function
  as a video generation model.</p>
<p>All three regularization schemes invert the OOD Separation Gap to
  negative values (Conservative: <span class="math inline">\(-0.0047\)</span>, Moderate: <span
    class="math inline">\(-0.0067\)</span>, Aggressive: <span class="math inline">\(-0.0029\)</span>), indicating that
  Sora2 content
  is now positioned <em>closer</em> to the real centroid than real
  samples. The ground-truth Silhouette scores similarly degrade from <span class="math inline">\(+0.115\)</span> to
  negative values ranging from
  <span class="math inline">\(-0.015\)</span> to <span class="math inline">\(-0.050\)</span>. This pattern mirrors the
  ID
  results, confirming that the centroid inversion observed in-distribution
  extends to OOD content.
</p>
<p>The Aggressive scheme achieves the best OOD ARI (<span class="math inline">\(0.043\)</span>) among projected
  schemes—and
  notably exceeds even the Original baseline (<span class="math inline">\(-0.010\)</span>)—suggesting that despite
  incorrect
  centroid orientation, the learned representations contain
  OOD-discriminative structure recoverable by clustering. However, the
  negative Separation Gap indicates this structure does not conform to the
  intended anomaly detection framework where novel synthetic content
  should be positioned farther from the real prototype.</p>
<p>The severity of OOD degradation for SENet (<span class="math inline">\(+0.074 \rightarrow -0.0029\)</span> to <span
    class="math inline">\(-0.0067\)</span>) exceeds that observed for OpenL3
  (<span class="math inline">\(+0.0002 \rightarrow -0.0055\)</span> to
  <span class="math inline">\(-0.0273\)</span>), suggesting that visual
  embeddings are particularly susceptible to losing generalization
  capacity during disentanglement training. This may reflect the higher
  initial quality of SENet’s OOD structure: embeddings with stronger
  baseline generalization have more to lose when subjected to ID-focused
  optimization. These results motivate future work on regularization
  strategies that explicitly preserve OOD separation during training, such
  as incorporating Sora2 or other held-out synthetic content into the
  training objective as negative examples.
</p>
<h2 id="discussion">Discussion</h2>
<h3 id="sec:interpretation">Interpretation of Results</h3>
<p>The experiments across HuBERT, OpenL3, and SENet embeddings reveal
  systematic patterns in how variance-based disentanglement interacts with
  different pretrained representations. While all three embedding types
  exhibit representation collapse under the disentanglement framework, the
  nature of this collapse and its impact on detection performance varies
  substantially across modalities and regularization strengths.</p>
<h4 id="cross-embedding-comparison">Cross-Embedding Comparison</h4>
<p>The three embedding types exhibit fundamentally different baseline
  characteristics that shape their response to disentanglement
  training:</p>
<h5 id="hubert-audio-embeddings.">HuBERT Audio Embeddings.</h5>
<p>HuBERT begins with the weakest baseline structure: negative
  Separation Gap (<span class="math inline">\(-0.013\)</span>), low
  ground-truth Silhouette (<span class="math inline">\(0.033\)</span>),
  and moderate distributional spread (Wasserstein <span class="math inline">\(0.533\)</span>). This “blank slate”
  property
  proves advantageous—the Aggressive regularization scheme achieves the
  best overall results, improving AMI from <span class="math inline">\(0.111\)</span> to <span
    class="math inline">\(0.120\)</span> (<span class="math inline">\(\uparrow 8\%\)</span>) and ARI from <span
    class="math inline">\(0.066\)</span> to <span class="math inline">\(0.089\)</span> (<span
    class="math inline">\(\uparrow 35\%\)</span>). Critically, both
  Conservative and Aggressive schemes achieve positive Separation Gaps on
  OOD Sora2 content (<span class="math inline">\(+0.0001\)</span> and
  <span class="math inline">\(+0.00005\)</span> respectively),
  demonstrating transfer of learned authenticity structure to unseen
  generation methods.
</p>
<h5 id="openl3-audio-embeddings.">OpenL3 Audio Embeddings.</h5>
<p>OpenL3 exhibits strong baseline structure: positive Separation Gap
  (<span class="math inline">\(+0.0096\)</span>), high ground-truth
  Silhouette (<span class="math inline">\(0.321\)</span>), and substantial
  distributional spread (Wasserstein <span class="math inline">\(3.413\)</span>). The Moderate scheme achieves best
  in-distribution performance (AMI <span class="math inline">\(0.081\)</span>, ARI <span
    class="math inline">\(0.088\)</span>—improvements of <span class="math inline">\(238\%\)</span> and <span
    class="math inline">\(120\%\)</span> over baseline). However, all three
  regularization schemes <em>degrade</em> OOD generalization, with
  Separation Gaps inverting from <span class="math inline">\(+0.0002\)</span> (Original) to negative values
  ranging from <span class="math inline">\(-0.0055\)</span> to <span class="math inline">\(-0.0273\)</span>. This
  suggests that OpenL3’s
  baseline OOD structure is overwritten by ID-specific features during
  training.</p>
<h5 id="senet-visual-embeddings.">SENet Visual Embeddings.</h5>
<p>SENet demonstrates the strongest baseline generalization: Separation
  Gap of <span class="math inline">\(+0.081\)</span> and ground-truth
  Silhouette of <span class="math inline">\(+0.115\)</span> on Sora2
  content—substantially exceeding both audio embeddings. The Aggressive
  scheme achieves remarkable label-alignment improvements (ARI: <span class="math inline">\(0.053 \rightarrow
    0.158\)</span>, <span class="math inline">\(\uparrow 198\%\)</span>), yet all schemes invert
  the Separation Gap to negative values. This represents a qualitative
  failure: the disentanglement training destroys SENet’s inherent capacity
  to distinguish novel synthetic content.</p>
<h4 id="the-collapse-separation-trade-off">The Collapse-Separation
  Trade-off</h4>
<p>All embedding types exhibit severe representation collapse, with
  Wasserstein distances decreasing by <span class="math inline">\(99\%\)</span> or more across all regularization
  schemes (Table <a href="#tab:collapse-summary" data-reference-type="ref"
    data-reference="tab:collapse-summary">3.8</a>). However, the
  relationship between collapse and separation varies systematically with
  regularization strength.</p>
<div id="tab:collapse-summary">
  <table>
    <caption>Wasserstein distance collapse across embedding types and
      regularization schemes. All schemes show <span class="math inline">\(&gt;99\%\)</span> reduction from
      baseline.</caption>
    <thead>
      <tr>
        <th style="text-align: left;"><strong>Embedding</strong></th>
        <th style="text-align: center;"><strong>Original</strong></th>
        <th style="text-align: center;"><strong>Conservative</strong></th>
        <th style="text-align: center;"><strong>Moderate</strong></th>
        <th style="text-align: center;"><strong>Aggressive</strong></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align: left;">HuBERT</td>
        <td style="text-align: center;">0.533</td>
        <td style="text-align: center;">0.003</td>
        <td style="text-align: center;">0.005</td>
        <td style="text-align: center;">0.004</td>
      </tr>
      <tr>
        <td style="text-align: left;">OpenL3</td>
        <td style="text-align: center;">3.413</td>
        <td style="text-align: center;">0.010</td>
        <td style="text-align: center;">0.018</td>
        <td style="text-align: center;">0.012</td>
      </tr>
      <tr>
        <td style="text-align: left;">SENet</td>
        <td style="text-align: center;">6.759</td>
        <td style="text-align: center;">0.015</td>
        <td style="text-align: center;">0.013</td>
        <td style="text-align: center;">0.003</td>
      </tr>
    </tbody>
  </table>
</div>
<p>The key finding is that collapse and separation are partially
  decoupled: collapsed representations can still maintain
  correctly-oriented centroid structure. For HuBERT, the transition from
  negative to positive Separation Gap (Conservative: <span class="math inline">\(+0.0008\)</span>, Aggressive: <span
    class="math inline">\(+0.0010\)</span>) occurs despite <span class="math inline">\(99.5\%\)</span> Wasserstein
  collapse. The variance
  minimization objective successfully learns the <em>direction</em> of
  authenticity separation but fails to preserve the <em>magnitude</em> of
  distributional spread necessary for robust discrimination.</p>
<h4 id="the-silhouette-paradox">The Silhouette Paradox</h4>
<p>A consistent pattern emerges across all embedding types: K-means
  Silhouette scores <em>increase</em> with collapse while label-alignment
  metrics often <em>decrease</em>. Table <a href="#tab:silhouette-paradox" data-reference-type="ref"
    data-reference="tab:silhouette-paradox">3.9</a> illustrates this
  divergence.</p>
<div id="tab:silhouette-paradox">
  <table>
    <caption>The Silhouette Paradox: K-means Silhouette increases while
      label-alignment metrics (AMI/ARI) show variable response. HuBERT data
      shown; similar patterns observed for OpenL3 and SENet.</caption>
    <thead>
      <tr>
        <th style="text-align: left;"><strong>Metric</strong></th>
        <th style="text-align: center;"><strong>Original</strong></th>
        <th style="text-align: center;"><strong>Conservative</strong></th>
        <th style="text-align: center;"><strong>Moderate</strong></th>
        <th style="text-align: center;"><strong>Aggressive</strong></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align: left;">K-means Silhouette <span class="math inline">\(\uparrow\)</span></td>
        <td style="text-align: center;">0.274</td>
        <td style="text-align: center;">0.575</td>
        <td style="text-align: center;">0.502</td>
        <td style="text-align: center;">0.425</td>
      </tr>
      <tr>
        <td style="text-align: left;">Ground-truth Silhouette</td>
        <td style="text-align: center;">0.033</td>
        <td style="text-align: center;">0.066</td>
        <td style="text-align: center;">0.054</td>
        <td style="text-align: center;">0.062</td>
      </tr>
      <tr>
        <td style="text-align: left;">AMI</td>
        <td style="text-align: center;">0.111</td>
        <td style="text-align: center;">0.104</td>
        <td style="text-align: center;">0.103</td>
        <td style="text-align: center;"><strong>0.120</strong></td>
      </tr>
      <tr>
        <td style="text-align: left;">ARI</td>
        <td style="text-align: center;">0.066</td>
        <td style="text-align: center;">0.033</td>
        <td style="text-align: center;">0.032</td>
        <td style="text-align: center;"><strong>0.089</strong></td>
      </tr>
    </tbody>
  </table>
</div>
<p>This divergence occurs because representation collapse concentrates
  all embeddings into a compact region where K-means identifies clusters
  based on residual variations <em>unrelated to authenticity</em>. The
  inverse relationship between K-means Silhouette and regularization
  strength (Conservative: <span class="math inline">\(0.575\)</span> <span class="math inline">\(&gt;\)</span>
  Moderate: <span class="math inline">\(0.502\)</span> <span class="math inline">\(&gt;\)</span> Aggressive: <span
    class="math inline">\(0.425\)</span>) confirms that weaker
  regularization produces tighter but less authenticity-aligned
  clusters.</p>
<p>The paradox illuminates a fundamental challenge: unsupervised
  clustering metrics can improve precisely because collapse eliminates the
  variance that would otherwise enable authenticity discrimination.
  Effective disentanglement requires maintaining sufficient geometric
  spread for clustering algorithms to recover label structure, not merely
  achieving tight clusters.</p>
<h4 id="regularization-strength-and-optimal-configuration">Regularization
  Strength and Optimal Configuration</h4>
<p>The experiments reveal that optimal regularization strength is
  <strong>embedding-dependent</strong>:
</p>
<ul>
  <li>
    <p><strong>HuBERT</strong>: Aggressive (<span class="math inline">\(\tau = 0.5\)</span>, <span
        class="math inline">\(\lambda_{\text{reg}} = 5.0\)</span>) achieves best
      ID and OOD performance</p>
  </li>
  <li>
    <p><strong>OpenL3</strong>: Moderate (<span class="math inline">\(\tau = 0.2\)</span>, <span
        class="math inline">\(\lambda_{\text{reg}} = 2.0\)</span>) achieves best
      ID performance; all schemes degrade OOD</p>
  </li>
  <li>
    <p><strong>SENet</strong>: Aggressive achieves best label-alignment
      but all schemes invert Separation Gap</p>
  </li>
</ul>
<p>For HuBERT, the Aggressive scheme maintains <span class="math inline">\(5\times\)</span> higher intra-group
  variance
  (Real: <span class="math inline">\(0.212\)</span>, Fake: <span class="math inline">\(0.299\)</span>) compared to
  Conservative (Real:
  <span class="math inline">\(0.042\)</span>, Fake: <span class="math inline">\(0.058\)</span>), suggesting that
  higher variance
  floors successfully resist complete collapse. This preserved variance
  correlates with superior label-alignment performance.
</p>
<p>The Moderate scheme occupies an unstable region of the hyperparameter
  space for HuBERT, failing to achieve positive Separation Gap (<span class="math inline">\(-0.0023\)</span>) despite
  intermediate
  regularization strength. This non-monotonic behavior suggests complex
  interactions between the variance floor, prototype loss, and
  orthogonality constraint that merit further investigation.</p>
<h4 id="ood-generalization-a-critical-divergence">OOD Generalization: A
  Critical Divergence</h4>
<p>The out-of-distribution analysis on Sora2 content reveals the most
  consequential finding: <strong>disentanglement training can destroy
    baseline OOD generalization</strong>. Table <a href="#tab:ood-summary" data-reference-type="ref"
    data-reference="tab:ood-summary">3.10</a>
  summarizes the impact.</p>
<div id="tab:ood-summary">
  <table>
    <caption>OOD Generalization Impact. Positive values indicate correct
      separation (fake samples farther from real centroid than real samples).
      Values in <strong>bold</strong> indicate improvement over Original;
      values in <span style="color: red">red</span> indicate
      degradation.</caption>
    <thead>
      <tr>
        <th style="text-align: left;"><strong>Embedding</strong></th>
        <th style="text-align: center;"><strong>Original</strong></th>
        <th style="text-align: center;"><strong>Conservative</strong></th>
        <th style="text-align: center;"><strong>Moderate</strong></th>
        <th style="text-align: center;"><strong>Aggressive</strong></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align: left;">HuBERT</td>
        <td style="text-align: center;"><span class="math inline">\(-0.020\)</span></td>
        <td style="text-align: center;"><strong><span class="math inline">\(+0.0001\)</span></strong></td>
        <td style="text-align: center;"><span style="color: red"><span class="math inline">\(-0.0022\)</span></span>
        </td>
        <td style="text-align: center;"><strong><span class="math inline">\(+0.00005\)</span></strong></td>
      </tr>
      <tr>
        <td style="text-align: left;">OpenL3</td>
        <td style="text-align: center;"><span class="math inline">\(+0.0002\)</span></td>
        <td style="text-align: center;"><span style="color: red"><span class="math inline">\(-0.0055\)</span></span>
        </td>
        <td style="text-align: center;"><span style="color: red"><span class="math inline">\(-0.0110\)</span></span>
        </td>
        <td style="text-align: center;"><span style="color: red"><span class="math inline">\(-0.0273\)</span></span>
        </td>
      </tr>
      <tr>
        <td style="text-align: left;">SENet</td>
        <td style="text-align: center;"><span class="math inline">\(+0.074\)</span></td>
        <td style="text-align: center;"><span style="color: red"><span class="math inline">\(-0.0047\)</span></span>
        </td>
        <td style="text-align: center;"><span style="color: red"><span class="math inline">\(-0.0067\)</span></span>
        </td>
        <td style="text-align: center;"><span style="color: red"><span class="math inline">\(-0.0029\)</span></span>
        </td>
      </tr>
    </tbody>
  </table>
</div>
<p>Two distinct patterns emerge:</p>
<h5 id="pattern-1-baseline-deficient-embeddings-benefit-from-training-hubert.">Pattern
  1: Baseline-deficient embeddings benefit from training (HuBERT).</h5>
<p>HuBERT’s negative baseline Separation Gap (<span class="math inline">\(-0.020\)</span>) indicates that untrained
  embeddings cannot distinguish Sora2 audio from authentic audio.
  Disentanglement training creates positive separation structure that
  transfers to OOD content, despite extreme collapse. The positive OOD
  Separation Gaps, though small in magnitude, represent a qualitative
  improvement from random to correct class orientation.</p>
<h5 id="pattern-2-baseline-rich-embeddings-suffer-from-training-openl3-senet.">Pattern
  2: Baseline-rich embeddings suffer from training (OpenL3, SENet).</h5>
<p>Both OpenL3 and SENet possess positive baseline OOD Separation Gaps
  that are systematically destroyed by disentanglement training. The
  severity of degradation correlates with baseline OOD quality: SENet’s
  Separation Gap drops from <span class="math inline">\(+0.074\)</span> to
  approximately <span class="math inline">\(-0.005\)</span> (a sign flip
  representing complete inversion), while OpenL3’s drops from <span class="math inline">\(+0.0002\)</span> to as low
  as <span class="math inline">\(-0.0273\)</span>.</p>
<p>This pattern suggests that embeddings with stronger baseline OOD
  structure have “more to lose” when subjected to ID-focused optimization.
  The disentanglement framework optimizes for ID separation at the cost of
  OOD generalization, effectively overwriting the generalization-relevant
  features encoded in pretrained representations.</p>
<h4 id="per-video-analysis-insights">Per-Video Analysis Insights</h4>
<p>The per-video visualizations (Figures <a href="#fig:hubert-pervideo" data-reference-type="ref"
    data-reference="fig:hubert-pervideo">[fig:hubert-pervideo]</a>–<a href="#fig:senet-pervideo"
    data-reference-type="ref" data-reference="fig:senet-pervideo">[fig:senet-pervideo]</a>) provide
  granular insight into how regularization affects individual content
  groups. Key observations include:</p>
<h5 id="distance-to-source-differentiation.">Distance-to-Source
  Differentiation.</h5>
<p>The rightmost panels show cosine similarity between augmentations and
  their source embedding. In the Original baselines, real augmentations
  (green bars) consistently show higher similarity to the source than fake
  augmentations (red bars), confirming that untrained embeddings naturally
  separate real from fake within content groups. After training, this
  differentiation diminishes as collapse compresses all embeddings—both
  real and fake—closer together.</p>
<p>For HuBERT (Figure <a href="#fig:hubert-pervideo" data-reference-type="ref"
    data-reference="fig:hubert-pervideo">[fig:hubert-pervideo]</a>), the
  Original baseline shows clear separation (Mean Real: <span class="math inline">\(0.832\)</span>, Mean Fake: <span
    class="math inline">\(0.181\)</span>), which compresses substantially
  under all regularization schemes (Mean Real: <span class="math inline">\(\sim 0.93\)</span>–<span
    class="math inline">\(0.96\)</span>, Mean Fake: <span class="math inline">\(\sim 0.83\)</span>–<span
    class="math inline">\(0.84\)</span>). The gap narrows from <span class="math inline">\(\Delta = 0.651\)</span> to
  <span class="math inline">\(\Delta \approx 0.10\)</span>–<span class="math inline">\(0.13\)</span>, reflecting the
  representation
  collapse observed in global metrics.
</p>
<h5 id="pca-structure-evolution.">PCA Structure Evolution.</h5>
<p>The left and center panels reveal how embedding space geometry
  changes with regularization. Original embeddings show dispersed point
  clouds with limited class structure. Progressive regularization
  concentrates points into tighter clusters, but the color gradients
  (authenticity scores) show that class separation within these clusters
  varies by scheme. The Aggressive scheme typically maintains more spread
  while achieving better alignment between cluster structure and
  authenticity labels.</p>
<h5 id="cross-augmentation-consistency.">Cross-Augmentation
  Consistency.</h5>
<p>A key validation of the disentanglement objective is whether
  augmentations of the same source video cluster together in the identity
  space (<span class="math inline">\(z^{\text{id}}\)</span>) regardless of
  their authenticity label. The visualizations confirm that content groups
  remain coherent across regularization schemes, suggesting the
  prototypical contrastive loss successfully captures content invariance
  even as the authenticity space collapses.</p>
<h3 id="diagnosis-of-failure-modes">Diagnosis of Failure Modes</h3>
<p>The comprehensive analysis identifies three primary failure modes
  that limit the current framework’s effectiveness:</p>
<h4 id="failure-mode-1-variance-floor-insufficiency">Failure Mode 1:
  Variance Floor Insufficiency</h4>
<p>Even the Aggressive regularization scheme (<span class="math inline">\(\tau = 0.5\)</span>, <span
    class="math inline">\(\lambda_{\text{reg}} = 5.0\)</span>) fails to
  prevent <span class="math inline">\(&gt;99\%\)</span> Wasserstein
  collapse. The variance floor regularization (Equation <a href="#eq:var-loss" data-reference-type="ref"
    data-reference="eq:var-loss">[eq:var-loss]</a>) is insufficient to
  counteract the compressive forces of the prototype loss and
  orthogonality constraint, which together encourage all embeddings to
  concentrate in a small region.</p>
<p>The design of the variance floor targets the <em>real</em>
  distribution only, penalizing collapse of authentic samples while
  allowing fake samples to collapse freely. This asymmetric formulation
  may be fundamentally flawed: if fake samples collapse toward the real
  centroid (satisfying the variance objective while destroying
  separation), the detector fails despite technically achieving its
  training objective.</p>
<h4 id="failure-mode-2-objective-misalignment">Failure Mode 2: Objective
  Misalignment</h4>
<p>The multi-objective optimization combines three losses with
  potentially conflicting gradients:</p>
<ol>
  <li>
    <p><strong>Prototype loss</strong> (<span class="math inline">\(\mathcal{L}_{\text{proto}}\)</span>): Pulls
      content groups together, encouraging compression</p>
  </li>
  <li>
    <p><strong>Variance loss</strong> (<span class="math inline">\(\mathcal{L}_{\text{var}}\)</span>): Minimizes
      spread of real samples, encouraging compression</p>
  </li>
  <li>
    <p><strong>Orthogonality constraint</strong> (<span class="math inline">\(\mathcal{L}_{\text{orth}}\)</span>):
      Decorrelates
      identity and authenticity spaces</p>
  </li>
</ol>
<p>The first two objectives both encourage embedding compression, with
  only the variance floor providing countervailing pressure. The
  optimization finds a degenerate solution that satisfies all objectives:
  collapse all embeddings to a small region (satisfying <span class="math inline">\(\mathcal{L}_{\text{proto}}\)</span>
  and <span class="math inline">\(\mathcal{L}_{\text{var}}\)</span>) where the
  identity and authenticity projections are trivially orthogonal
  (satisfying <span class="math inline">\(\mathcal{L}_{\text{orth}}\)</span>).</p>
<h4 id="failure-mode-3-ood-structure-overwriting">Failure Mode 3: OOD
  Structure Overwriting</h4>
<p>For embeddings with strong baseline OOD generalization (OpenL3,
  SENet), disentanglement training systematically destroys this structure.
  The training objective contains no explicit regularization to preserve
  baseline OOD performance, allowing the optimization to freely overwrite
  generalization-relevant features with ID-specific discriminators.</p>
<p>This failure mode is particularly concerning because it suggests a
  fundamental tension between ID optimization and OOD generalization in
  the current framework. Achieving better ID separation may
  <em>require</em> sacrificing the broad, generalizable features that
  enable OOD detection.
</p>
<h3 id="potential-remediation-strategies">Potential Remediation
  Strategies</h3>
<p>Based on the identified failure modes, several architectural and
  optimization modifications may address representation collapse while
  preserving OOD generalization:</p>
<h4 id="contrastive-authenticity-loss">Contrastive Authenticity
  Loss</h4>
<p>Replace variance minimization with a supervised contrastive
  objective <span class="citation" data-cites="Khosla2020"></span> that
  explicitly requires separation between real and fake samples: <span class="math display">\[\begin{equation}
    \mathcal{L}_{\text{auth}} = -\frac{1}{|P(i)|} \sum_{p \in P(i)} \log
    \frac{\exp(z_i^{\text{auth}} \cdot z_p^{\text{auth}} / \tau)}{\sum_{a
    \in A(i)} \exp(z_i^{\text{auth}} \cdot z_a^{\text{auth}} / \tau)}
    \end{equation}\]</span> where <span class="math inline">\(P(i)\)</span>
  is the set of samples with the same authenticity label as anchor <span class="math inline">\(i\)</span>. This
  formulation directly optimizes
  for class separation rather than relying on implicit separation through
  variance differences.</p>
<h4 id="bidirectional-variance-regularization">Bidirectional Variance
  Regularization</h4>
<p>Extend the variance floor to <em>both</em> real and fake samples:
  <span class="math display">\[\begin{equation}
    \mathcal{L}_{\text{var}}^{\text{bi}} = \lambda_{\text{reg}} \left[
    \max(0, \tau - \sigma_{\text{real}}^2) + \max(0, \tau -
    \sigma_{\text{fake}}^2) \right]
    \end{equation}\]</span> This prevents the degenerate solution where fake
  samples collapse toward the real centroid while satisfying the original
  variance objective.
</p>
<h4 id="ood-preservation-regularization">OOD Preservation
  Regularization</h4>
<p>Introduce a knowledge distillation term that preserves baseline OOD
  structure: <span class="math display">\[\begin{equation}
    \mathcal{L}_{\text{preserve}} = \| z^{\text{auth}} -
    \text{sg}(z^{\text{original}}) \|_2^2
    \end{equation}\]</span> where <span class="math inline">\(\text{sg}(\cdot)\)</span> denotes stop-gradient.
  This regularization penalizes deviation from the original embedding
  space, preserving generalization-relevant features while allowing
  supervised refinement.</p>
<h4 id="margin-based-prototype-learning">Margin-Based Prototype
  Learning</h4>
<p>Replace soft prototype assignment with hard margins that enforce
  minimum distances between class centroids: <span class="math display">\[\begin{equation}
    \mathcal{L}_{\text{margin}} = \max(0, m - \| \mu_{\text{real}} -
    \mu_{\text{fake}} \|_2)
    \end{equation}\]</span> This prevents the centroid structure from
  collapsing even as individual embeddings concentrate, maintaining the
  geometric separation necessary for detection.</p>
<h4 id="staged-training">Staged Training</h4>
<p>Pretrain the authenticity head on a binary classification objective
  to establish initial separation, then introduce disentanglement losses
  with the separation structure as an anchor. This two-stage approach
  prevents the optimization from finding degenerate collapsed solutions by
  starting from a non-collapsed initialization.</p>
<h3 id="broader-implications">Broader Implications</h3>
<p>The systematic failure of variance-based disentanglement across
  multiple embedding types suggests fundamental limitations of this
  approach for deepfake detection. The core assumption—that authentic
  content shares intrinsic properties distinguishable from synthetic
  content through variance minimization—may be valid, but the optimization
  dynamics prevent recovery of this structure.</p>
<p>More broadly, these results highlight the challenge of
  multi-objective representation learning when objectives can be trivially
  satisfied through degenerate solutions. The Silhouette Paradox
  demonstrates that standard clustering metrics can misleadingly indicate
  success even when the learned representations fail to capture
  task-relevant structure.</p>
<p>Future work in disentangled deepfake detection should prioritize: (1)
  loss formulations that explicitly prevent collapse while encouraging
  separation, (2) regularization strategies that preserve baseline OOD
  generalization, and (3) evaluation frameworks that distinguish between
  genuine representation quality and degenerate clustering behavior.</p>
<h3 id="sec:future-work">Future Directions</h3>
<p>The results, while not achieving the intended disentanglement,
  provide valuable diagnostic information about the challenges of
  multi-objective representation learning for deepfake detection. Several
  extensions remain for future work.</p>
<h4 id="multimodal-representation-fusion">Multimodal Representation
  Fusion</h4>
<p>Audio and video modalities capture complementary manipulation
  artifacts with moderate correlation (<span class="math inline">\(r
    \approx 0.46\)</span> between HuBERT and SENet predictions). Several
  fusion strategies merit investigation:</p>
<h5 id="early-fusion.">Early Fusion.</h5>
<p>Concatenating audio and video embeddings before disentanglement would
  allow dual projection heads to learn joint authenticity representations
  capturing cross-modal inconsistencies—for instance, lip movements not
  matching audio content.</p>
<h5 id="late-fusion.">Late Fusion.</h5>
<p>Training separate disentanglement models for each modality, then
  combining authenticity embeddings (<span class="math inline">\(z^{\text{auth}}_{\text{audio}}\)</span> and <span
    class="math inline">\(z^{\text{auth}}_{\text{video}}\)</span>) at
  classification, preserves modality-specific representations while
  enabling joint decision-making.</p>
<h5 id="attention-based-fusion.">Attention-Based Fusion.</h5>
<p>Cross-modal attention could dynamically weight modality contributions
  based on per-sample reliability, emphasizing whichever modality exhibits
  clearer artifacts.</p>
<h4 id="alternative-loss-formulations">Alternative Loss
  Formulations</h4>
<p>The identified failure modes motivate systematic exploration of
  alternative training objectives:</p>
<ul>
  <li>
    <p><strong>VICReg-style regularization</strong> <span class="citation" data-cites="bardes2021vicreg"></span>:
      Combining
      variance, invariance, and covariance terms may better balance the
      competing objectives</p>
  </li>
  <li>
    <p><strong>Hyperspherical uniformity</strong> <span class="citation" data-cites="wang2020understanding"></span>:
      Distributing embeddings
      uniformly on a hypersphere prevents collapse while maintaining
      separation</p>
  </li>
  <li>
    <p><strong>Spectral contrastive learning</strong> <span class="citation" data-cites="haochen2021provable"></span>:
      Operating on
      the eigenspace of the similarity matrix may provide more stable
      gradients</p>
  </li>
</ul>
<h4 id="extended-evaluation">Extended Evaluation</h4>
<p>While Sora2 provides a challenging OOD test case, evaluation on
  additional unseen generators (Runway Gen-3, Pika, etc.) would further
  validate generalization claims. Additionally, ablation studies on the
  robustness to video compression, which can mask or mimic deepfake
  artifacts, is essential for practical deployment.</p>
<h1 id="conclusion">Conclusion</h1>
<h2 id="summary-of-contributions">Summary of Contributions</h2>
<p>This thesis addressed two complementary challenges in visual
  deception detection, developing specialized approaches for textual
  homoglyph attacks and synthetic media identification.</p>
<p>For homoglyph detection, we introduced Visually-Aligned Text
  Embeddings (VA-TE), a contrastive learning framework that leverages
  vision-language model text encoders to capture visual properties of
  Unicode characters without requiring image rendering. Through
  curriculum-based hard negative mining and a lightweight projection
  architecture, VA-TE achieves 0.95 ROC-AUC as a standalone system. When
  combined with complementary string-similarity features in an ensemble,
  VA-TE attains state-of-the-art performance (0.98 ROC-AUC) while offering
  substantial advantages in scalability, memory efficiency, and deployment
  simplicity compared to prior image-based approaches. Building on recent
  advances in embedding refinement <span class="citation"
    data-cites="jha2025harnessinguniversalgeometryembeddings"></span>, this
  approach marks an important step toward multi-modal representations that
  integrate the visual characteristics of text, reflecting how humans read
  and interpret it.</p>
<p>For deepfake detection, we proposed a disentangled representation
  learning framework with dual projection heads and orthogonality
  constraints, designed to separate authenticity-relevant features from
  identity and content information. Our systematic evaluation across three
  embedding types (HuBERT, OpenL3, SENet) and three regularization schemes
  revealed both the promise and limitations of variance-based
  disentanglement. While all configurations exhibited representation
  collapse (<span class="math inline">\(&gt;99\%\)</span> Wasserstein
  distance reduction), the Aggressive regularization scheme on HuBERT
  embeddings achieved improved label-alignment (AMI: <span class="math inline">\(0.111 \rightarrow 0.120\)</span>,
  ARI: <span class="math inline">\(0.066 \rightarrow 0.089\)</span>) and positive
  out-of-distribution Separation Gaps on Sora2 content. However, for
  embeddings with strong baseline generalization (OpenL3, SENet),
  disentanglement training systematically degraded OOD performance,
  highlighting a fundamental tension between in-distribution optimization
  and generalization preservation. Additionally, we collected and
  annotated a novel out-of-distribution evaluation dataset comprising 150
  videos (11,000+ segments) generated by OpenAI’s Sora 2, providing a
  challenging benchmark for assessing generalization to state-of-the-art
  generation methods.</p>
<h2 id="methodological-insights">Methodological Insights</h2>
<p>Several methodological insights emerged from this work that may
  inform future research in representation learning for security
  applications.</p>
<p>First, the success of VA-TE demonstrates that vision-language models
  encode visual characteristics of text beyond semantic meaning, and that
  these latent features can be effectively surfaced through targeted
  fine-tuning. The pairwise contrastive loss proved consistently superior
  to triplet, InfoNCE, and supervised contrastive alternatives, likely due
  to its alignment with the SigLIP encoder’s original training
  objective.</p>
<p>Second, our deepfake detection experiments revealed what we term the
  <em>Silhouette Paradox</em>: K-means clustering metrics can improve
  substantially (e.g., <span class="math inline">\(0.274 \rightarrow
    0.575\)</span> for HuBERT) even as label-alignment metrics degrade. This
  divergence occurs because representation collapse creates well-formed
  clusters based on residual variations unrelated to authenticity. This
  finding underscores the importance of evaluating learned representations
  with label-aware metrics rather than relying solely on unsupervised
  clustering quality.
</p>
<p>Third, the multi-objective optimization underlying our
  disentanglement framework proved susceptible to degenerate solutions.
  The variance minimization objective, computed only on real samples,
  allows fake samples to collapse toward the real centroid while
  technically satisfying all training objectives. Notably, the optimal
  regularization strength proved embedding-dependent: Aggressive
  regularization (<span class="math inline">\(\tau = 0.5\)</span>)
  performed best for HuBERT, while Moderate regularization (<span class="math inline">\(\tau = 0.2\)</span>) achieved
  superior
  in-distribution results for OpenL3. This finding suggests that
  representation learning frameworks require careful hyperparameter tuning
  matched to the characteristics of their input embeddings.</p>
<p>Fourth, we observed a critical pattern in OOD generalization:
  embeddings with weak baseline structure (HuBERT, with negative baseline
  Separation Gap) benefited from disentanglement training, while
  embeddings with strong baseline structure (SENet, with <span class="math inline">\(+0.074\)</span> Separation Gap on
  Sora2) were
  harmed. This suggests that ID-focused optimization can overwrite the
  generalizable features encoded in pretrained representations, motivating
  future work on regularization strategies that explicitly preserve OOD
  structure.</p>
<p>Fifth, both projects benefited from combining learned representations
  with complementary features. For VA-TE, fusing visual embeddings with
  string-based metrics yielded a 3-point improvement in ROC-AUC. This
  pattern suggests that hybrid approaches leveraging multiple signal
  modalities remain valuable even as learned representations improve.</p>
<h2 id="broader-impact">Broader Impact</h2>
<p>Visual deception poses growing threats to financial systems, identity
  verification platforms, and information ecosystems. Homoglyph attacks
  enable fraud at scale by exploiting the gap between human perception and
  computational string matching, while synthetic media undermines the
  trustworthiness of audio-visual content for authentication and
  evidence.</p>
<p>The methods developed in this thesis contribute to defending against
  these threats. VA-TE provides financial institutions with a scalable,
  deployable solution for detecting fraudulent account names and domain
  spoofing. The disentanglement framework, while requiring refinement to
  address representation collapse, establishes diagnostic tools and
  evaluation metrics that clarify the challenges of multi-objective
  representation learning for deepfake detection. The identification of
  the collapse-separation trade-off as the primary failure mode offers a
  concrete target for future architectural improvements. More broadly, by
  focusing on learning stable properties of authentic content rather than
  chasing evolving adversarial signatures, both approaches embody a
  defensive paradigm better suited to the rapid pace of generative AI
  advancement.</p>
<p>We note that detection systems can also be misused—for instance, to
  identify which synthetic content evades detection, thereby improving
  adversarial attacks. Responsible deployment requires ongoing monitoring,
  regular model updates, and integration with broader security protocols
  rather than reliance on any single detection method.</p>
<h2 id="future-directions">Future Directions</h2>
<p>Several promising directions extend this work. For VA-TE, applying
  the framework to non-Latin scripts (Chinese, Arabic, Cyrillic) and
  integrating with OCR pipelines for document verification represent
  natural next steps. The perceptual decoding extension proposed in
  Section <a href="#sec:vate-future" data-reference-type="ref" data-reference="sec:vate-future">2.6.3</a> offers a
  path toward
  grounding text embeddings in low-level visual features.</p>
<p>For deepfake detection, the immediate priority is addressing
  representation collapse through alternative loss formulations.
  Supervised contrastive objectives that explicitly require separation
  between real and fake samples, bidirectional variance regularization
  that penalizes collapse of both distributions, and margin-based
  prototype learning that enforces minimum centroid distances all merit
  systematic evaluation. The strong baseline performance of SENet video
  embeddings (0.914 AUROC, <span class="math inline">\(+0.074\)</span> OOD
  Separation Gap) suggests that preserving, rather than overwriting,
  pretrained generalization structure may be more effective than
  aggressive representation learning.</p>
<p>Beyond addressing collapse, multimodal fusion strategies that combine
  audio and video authenticity signals offer substantial potential. The
  moderate correlation between HuBERT and SENet predictions (<span class="math inline">\(r \approx 0.46\)</span>)
  indicates partially
  independent information that joint models could exploit. Attention-based
  fusion mechanisms that dynamically weight modality contributions based
  on per-sample reliability represent a promising direction.</p>
<p>Evaluation on additional unseen generators (Runway Gen-3, Pika,
  Kling) and across video compression levels will be essential for
  validating generalization claims before deployment. The Sora2 dataset
  introduced in this work provides one such benchmark, but broader
  coverage of the rapidly evolving generative model landscape is
  needed.</p>
<p>Finally, exploring unified architectures that address both textual
  and media-level visual deception within a single framework represents a
  longer-term opportunity, potentially leveraging shared principles of
  authenticity representation learning across modalities.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
  <hr />
  <ol>
    <li id="fn1">
      <p>Fuzzy Matching library, including Token Set Ratio, can
        be found here:
        <span>https://rapidfuzz.github.io/RapidFuzz/Usage/fuzz.html</span><a href="#fnref1" class="footnote-back"
          role="doc-backlink">↩︎</a>
      </p>
    </li>
    <li id="fn2">
      <p>The python-Levenshtein and RapidFuzz’s TokenSetRatio
        libraries are used; distances are computed on raw strings.<a href="#fnref2" class="footnote-back"
          role="doc-backlink">↩︎</a></p>
    </li>
    <li id="fn3">
      <p>Documenation found at https://optuna.org/<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a>
      </p>
    </li>
  </ol>
</section>
</body>

</html>